Unsupervised Visual Representation Learning by Context Prediction

Carl Doersch1,2
1 School of Computer Science Carnegie Mellon University

Abhinav Gupta1 Alexei A. Efros2
2 Dept. of Electrical Engineering and Computer Science University of California, Berkeley

Abstract
This work explores the use of spatial context as a source of free and plentiful supervisory signal for training a rich visual representation. Given only a large, unlabeled image collection, we extract random pairs of patches from each image and train a convolutional neural net to predict the position of the second patch relative to the ﬁrst. We argue that doing well on this task requires the model to learn to recognize objects and their parts. We demonstrate that the feature representation learned using this within-image context indeed captures visual similarity across images. For example, this representation allows us to perform unsupervised visual discovery of objects like cats, people, and even birds from the Pascal VOC 2011 detection dataset. Furthermore, we show that the learned ConvNet can be used in the RCNN framework [19] and provides a signiﬁcant boost over a randomly-initialized ConvNet, resulting in state-of-theart performance among algorithms which use only Pascalprovided training set annotations.
1. Introduction
Recently, new computer vision methods have leveraged large datasets of millions of labeled examples to learn rich, high-performance visual representations [29]. Yet efforts to scale these methods to truly Internet-scale datasets (i.e. hundreds of billions of images) are hampered by the sheer expense of the human annotation required. A natural way to address this difﬁculty would be to employ unsupervised learning, which aims to use data without any annotation. Unfortunately, despite several decades of sustained effort, unsupervised methods have not yet been shown to extract useful information from large collections of full-sized, real images. After all, without labels, it is not even clear what should be represented. How can one write an objective function to encourage a representation to capture, for example, objects, if none of the objects are labeled?
Interestingly, in the text domain, context has proven to be a powerful source of automatic supervisory signal for learning representations [3, 38, 9, 37]. Given a large text corpus, the idea is to train a model that maps each word to a feature vector, such that it is easy to predict the words

Example:

Question 1:

Question 2:

_ ?

_ ?

Figure 1. Our task for learning patch representations involves randomly sampling a patch (blue) and then one of eight possible neighbors (red). Can you guess the spatial conﬁguration for the two pairs of patches? Note that the task is much easier once you have recognized the object!

Answer key: Q1: Bottom right Q2: Top center

in the context (i.e., a few words before and/or after) given the vector. This converts an apparently unsupervised problem (ﬁnding a good similarity metric between words) into a “self-supervised” one: learning a function from a given word to the words surrounding it. Here the context prediction task is just a “pretext” to force the model to learn a good word embedding, which, in turn, has been shown to be useful in a number of real tasks, such as semantic word similarity [37].
Our paper aims to provide a similar “self-supervised” formulation for image data: a supervised task involving predicting the context for a patch. Our task is illustrated in Figures 1 and 2. We sample random pairs of patches in one of eight spatial conﬁgurations, and present each pair to a machine learner, providing no information about the patches’ original position within the image. The algorithm must then guess the position of one patch relative to the other. Our underlying hypothesis is that doing well on this task requires understanding scenes and objects, i.e. a good visual representation for this task will need to extract objects and their parts in order to reason about their relative spatial location. “Objects,” after all, consist of multiple parts that can be detected independently of one another, and which

11422

occur in a speciﬁc spatial conﬁguration (if there is no speciﬁc conﬁguration of the parts, then it is “stuff” [1]). We present a ConvNet-based approach to learn a visual representation from this task. We demonstrate that the resulting visual representation is good for both object detection, providing a signiﬁcant boost on PASCAL VOC 2007 compared to learning from scratch, as well as for unsupervised object discovery / visual data mining. This means, surprisingly, that our representation generalizes across images, despite being trained using an objective function that operates on a single image at a time. That is, instance-level supervision appears to improve performance on category-level tasks.
2. Related Work
One way to think of a good image representation is as the latent variables of an appropriate generative model. An ideal generative model of natural images would both generate images according to their natural distribution, and be concise in the sense that it would seek common causes for different images and share information between them. However, inferring the latent structure given an image is intractable for even relatively simple models. To deal with these computational issues, a number of works, such as the wake-sleep algorithm [23], contrastive divergence [22], deep Boltzmann machines [45], and variational Bayesian methods [28, 43] use sampling to perform approximate inference. Generative models have shown promising performance on smaller datasets such as handwritten digits [23, 22, 45, 28, 43], but none have proven effective for high-resolution natural images.
Unsupervised representation learning can also be formulated as learning an embedding (i.e. a feature vector for each image) where images that are semantically similar are close, while semantically different ones are far apart. One way to build such a representation is to create a supervised “pretext” task such that an embedding which solves the task will also be useful for other real-world tasks. For example, denoising autoencoders [53, 4] use reconstruction from noisy data as a pretext task: the algorithm must connect images to other images with similar objects to tell the difference between noise and signal. Sparse autoencoders also use reconstruction as a pretext task, along with a sparsity penalty [39], and such autoencoders may be stacked to form a deep representation [32, 31]. (however, only [31] was successfully applied to full-sized images, requiring a million CPU hours to discover just three objects). We believe that current reconstruction-based algorithms struggle with lowlevel phenomena, like stochastic textures, making it hard to even measure whether a model is generating well.
Another pretext task is “context prediction.” A strong tradition for this kind of task already exists in the text domain, where “skip-gram” [37] models have been shown to generate useful word representations. The idea is to train a

1 23

4

5

6 7 8

X = ( , ); Y = 3
Figure 2. The algorithm receives two patches in one of these eight possible spatial arrangements, without any context, and must then classify which conﬁguration was sampled.
model (e.g. a deep network) to predict, from a single word, the n preceding and n succeeding words. In principle, similar reasoning could be applied in the image domain, a kind of visual “ﬁll in the blank” task, but, again, one runs into the problem of determining whether the predictions themselves are correct [12], unless one cares about predicting only very low-level features [14, 30, 50]. To address this, [36] predicts the appearance of an image region by consensus voting of the transitive nearest neighbors of its surrounding regions. Our previous work [12] explicitly formulates a statistical test to determine whether the data is better explained by a prediction or by a low-level null hypothesis model.
The key problem that these approaches must address is that predicting pixels is much harder than predicting words, due to the huge variety of pixels that can arise from the same semantic object. In the text domain, one interesting idea is to switch from a pure prediction task to a discrimination task [38, 9]. In this case, the pretext task is to discriminate true snippets of text from the same snippets where a word has been replaced at random. A direct extension of this to 2D might be to discriminate between real images vs. images where one patch has been replaced by a random patch from elsewhere in the dataset. However, such a task would be trivial, since discriminating low-level color statistics and lighting would be enough. To make the task harder and more high-level, in this paper, we instead classify between multiple possible conﬁgurations of patches sampled from the same image, which means they will share lighting and color statistics, as shown on Figure 2.
Another line of work in unsupervised learning from images aims to discover object categories using hand-crafted features and various forms of clustering (e.g. [48, 44] learned a generative model over bags of visual words). Such representations lose shape information, and will readily dis-

1423

cover clusters of, say, foliage. A few subsequent works have attempted to use representations more closely tied to shape [33, 40], but relied on contour extraction, which is difﬁcult in complex images. Many other approaches [20, 27, 16] focus on deﬁning similarity metrics which can be used in more standard clustering algorithms; [42], for instance, re-casts the problem as frequent itemset mining. Geometry may also be used to for verifying links between images [41, 6, 21], although this can fail for deformable objects.
Video can provide another cue for representation learning. For most scenes, the identity of objects remains unchanged even as appearance changes with time. This kind of temporal coherence has a long history in visual learning literature [18, 56], and contemporaneous work shows strong improvements on modern detection datasets [54].
Finally, our work is related to a line of research on discriminative patch mining [13, 47, 26, 34, 49, 11], which has emphasized weak supervision as a means of object discovery. Like the current work, they emphasize the utility of learning representations of patches (i.e. object parts) before learning full objects and scenes, and argue that scene-level labels can serve as a pretext task. For example, [13] trains detectors to be sensitive to different geographic locales, but the actual goal is to discover speciﬁc elements of architectural style.
3. Learning Visual Context Prediction
We aim to learn an image representation for our pretext task, i.e., predicting the relative position of patches within an image. We employ Convolutional Neural Networks (ConvNets), which are well known to learn complex image representations with minimal human feature design. Building a ConvNet that can predict a relative offset for a pair of patches is, in principle, straightforward: the network must feed the two input patches through several convolution layers, and produce an output that assigns a probability to each of the eight spatial conﬁgurations (Figure 2) that might have been sampled (i.e. a softmax output). Note, however, that we ultimately wish to learn a feature embedding for individual patches, such that patches which are visually similar (across different images) would be close in the embedding space.
To achieve this, we use a late-fusion architecture shown in Figure 3: a pair of AlexNet-style architectures [29] that process each patch separately, until a depth analogous to fc6 in AlexNet, after which point the representations are fused. For the layers that process only one of the patches, weights are tied between both sides of the network, such that the same fc6-level embedding function is computed for both patches. Because there is limited capacity for joint reasoning—i.e., only two layers receive input from both patches—we expect the network to perform the bulk of the

fc9 (8) fc8 (4096)

fc7 (4096)

fc6 (4096) pool5 (3x3,256,2) conv5 (3x3,256,1) conv4 (3x3,384,1) conv3 (3x3,384,1)
LRN2 pool2 (3x3,384,2) conv2 (5x5,384,2)
LRN1 pool1 (3x3,96,2) conv1 (11x11,96,4)

fc6 (4096) pool5 (3x3,256,2) conv5 (3x3,256,1) conv4 (3x3,384,1) conv3 (3x3,384,1)
LRN2 pool2 (3x3,384,2) conv2 (5x5,384,2)
LRN1 pool1 (3x3,96,2) conv1 (11x11,96,4)

Patch 1

Patch 2

Figure 3. Our architecture for pair classiﬁcation. Dotted lines indicate shared weights. ‘conv’ stands for a convolution layer, ‘fc’ stands for a fully-connected one, ‘pool’ is a max-pooling layer, and ‘LRN’ is a local response normalization layer. Numbers in parentheses are kernel size, number of outputs, and stride (fc layers have only a number of outputs). The LRN parameters follow [29]. All conv and fc layers are followed by ReLU nonlinearities, except fc9 which feeds into a softmax classiﬁer.
semantic reasoning for each patch separately. When designing the network, we followed AlexNet where possible.
To obtain training examples given an image, we sample the ﬁrst patch uniformly, without any reference to image content. Given the position of the ﬁrst patch, we sample the second patch randomly from the eight possible neighboring locations as in Figure 2.

3.1. Avoiding “trivial” solutions
When designing a pretext task, care must be taken to ensure that the task forces the network to extract the desired information (high-level semantics, in our case), without taking “trivial” shortcuts. In our case, low-level cues like boundary patterns or textures continuing between patches could potentially serve as such a shortcut. Hence, for the relative prediction task, it was important to include a gap between patches (in our case, approximately half the patch width). Even with the gap, it is possible that long lines spanning neighboring patches could could give away the correct answer. Therefore, we also randomly jitter each patch location by up to 7 pixels (see Figure 2).
However, even these precautions are not enough: we were surprised to ﬁnd that, for some images, another trivial solution exists. We traced the problem to an unexpected culprit: chromatic aberration. Chromatic aberration arises from differences in the way the lens focuses light at different wavelengths. In some cameras, one color channel (commonly green) is shrunk toward the image center relative to the others [5, p. 76]. A ConvNet, it turns out, can learn to localize a patch relative to the lens itself (see Section 4.2) sim-

1424

Input Random Initialization

ImageNet AlexNet

Ours

Figure 4. Examples of patch clusters obtained by nearest neighbors. The query patch is shown on the far left. Matches are for three different features: fc6 features from a random initialization of our architecture, AlexNet fc7 after training on labeled ImageNet, and the fc6 features learned from our method. Queries were chosen from 1000 randomly-sampled patches. The top group is examples where our algorithm performs well; for the middle AlexNet outperforms our approach; and for the bottom all three features work well.

ply by detecting the separation between green and magenta (red + blue). Once the network learns the absolute location on the lens, solving the relative location task becomes trivial. To deal with this problem, we experimented with two types of pre-processing. One is to shift green and magenta toward gray (‘projection’). Speciﬁcally, let a = [−1, 2, −1] (the ’green-magenta color axis’ in RGB space). We then deﬁne B = I − aT a/(aaT ), which is a matrix that subtracts the projection of a color onto the green-magenta color axis. We multiply every pixel value by B. An alternative approach is to randomly drop 2 of the 3 color channels from each patch (‘color dropping’), replacing the dropped colors with Gaussian noise (standard deviation ∼ 1/100 the standard deviation of the remaining channel). For qualitative results, we show the ‘color-dropping’ approach, but found both performed similarly; for the object detection results,

we show both results.
Implementation Details: We use Caffe [25], and train on the ImageNet [10] 2012 training set ( 1.3M images), using only the images and discarding the labels. First, we resize each image to between 150K and 450K total pixels, preserving the aspect-ratio. From these images, we sample patches at resolution 96-by-96. For computational efﬁciency, we only sample the patches from a grid like pattern, such that each sampled patch can participate in as many as 8 separate pairings. We allow a gap of 48 pixels between the sampled patches in the grid, but also jitter the location of each patch in the grid by −7 to 7 pixels in each direction. We preprocess patches by (1) mean subtraction (2) projecting or dropping colors (see above), and (3) randomly downsampling some patches to as little as 100 total pixels, and then upsampling it, to build robustness to pixelation. When ap-

1425

Image layout Initial layout, with sampled patches in red is discarded We can recover image layout automatically Cannot recover layout with color removed Figure 5. We trained a network to predict the absolute (x, y) coordinates of randomly sampled patches. Far left: input image. Center left: extracted patches. Center right: the location the trained network predicts for each patch shown on the left. Far right: the same result after our color projection scheme. Note that the far right patches are shown after color projection; the operation’s effect is almost unnoticeable.

plying simple SGD to train the network, we found that the network predictions would degenerate to a uniform prediction over the 8 categories, with all activations for fc6 and fc7 collapsing to 0. This meant that the optimization became permanently stuck in a saddle point where it ignored the input from the lower layers (which helped minimize the variance of the ﬁnal output), and therefore that the net could not tune the lower-level features and escape the saddle point. Hence, Our ﬁnal implementation employs batch normalization [24], which forces the network activations to vary across examples. We also ﬁnd that high momentum values (e.g. .999) accelerated learning. For experiments, we use a ConvNet trained on a K40 GPU for approximately four weeks.
4. Experiments
We ﬁrst demonstrate the network has learned to associate semantically similar patches, using simple nearest-neighbor matching. We then apply the trained network in two domains. First, we use the model as “pre-training” for a standard vision task with only limited training data: speciﬁcally, we use the VOC 2007 object detection. Second, we evaluate visual data mining, where the goal is to start with an unlabeled image collection and discover object classes. Finally, we analyze the performance on the layout prediction “pretext task” to see how much is left to learn from this supervisory signal.
4.1. Nearest Neighbors
Recall our intuition that training should assign similar representations to semantically similar patches. In this section, our goal is to understand which patches our network considers similar. We begin by sampling random 96x96 patches, which we represent using fc6 features (i.e. we remove fc7 and higher shown in Figure 3, and use only one of the two stacks). We ﬁnd nearest neighbors using normalized correlation of these features. Results for some patches (selected out of 1000 random queries) are shown in Figure 4. For comparison, we repeated the experiment using fc7 features from AlexNet trained on ImageNet (obtained by upsampling the patches), and using fc6 features from our architecture but without any training (random weights ini-

…

fc8 (21) fc7 (4096) pool6 (3x3,1024,2) conv6b (1x1,1024,1) conv6 (3x3,4096,1)
pool5
Image (227x227)

Figure 6. Our architecture for Pascal VOC detection. Layers from conv1 through pool5 are copied from our patch-based network (Figure 3). The new ’conv6’ layer is created by converting the fc6 layer into a convolution layer. Kernel sizes, output units, and stride are given in parentheses, as in Figure 3.

tialization). As shown in Figure 4, the matches returned by our feature often capture the semantic information that we are after, matching AlexNet in terms of semantic content (in some cases, e.g. the car wheel, our matches capture pose better). Interestingly, in a few cases, random (untrained) ConvNet also does reasonably well.

4.2. Aside: Learnability of Chromatic Aberration
We noticed in early nearest-neighbor experiments that some patches retrieved match patches from the same absolute location in the image, regardless of content, because those patches displayed similar aberration. To further demonstrate this phenomenon, we trained a network to predict the absolute (x, y) coordinates of patches sampled from ImageNet. While the overall accuracy of this regressor is not very high, it does surprisingly well for some images: for the top 10% of images, the average (root-mean-square) error is .255, while chance performance (always predicting the image center) yields a RMSE of .371. Figure 5 shows one such result. Applying the proposed “projection” scheme increases the error on the top 10% of images to .321.

4.3. Object Detection
Previous work on the Pascal VOC challenge [15] has shown that pre-training on ImageNet (i.e., training a ConvNet to solve the ImageNet challenge) and then “ﬁnetuning” the network (i.e. re-training the ImageNet model for PASCAL data) provides a substantial boost over training on the Pascal training set alone [19, 2]. However, as far as we are aware, no works have shown that unsupervised pretraining on images can provide such a performance boost, no matter how much data is used.
Since we are already using a ConvNet, we adopt the cur-

1426

VOC-2007 Test aero bike bird boat bottle bus car cat chair cow table dog horse mbike person plant sheep sofa train tv mAP

DPM-v5[17]

33.2 60.3 10.2 16.1 27.3 54.3 58.2 23.0 20.0 24.1 26.7 12.7 58.1 48.2

[8] w/o context 52.6 52.6 19.2 25.4 18.7 47.3 56.9 42.1 16.6 41.4 41.9 27.7 47.9 51.5

Regionlets[55] 54.2 52.0 20.3 24.0 20.1 55.5 68.7 42.6 19.2 44.2 49.1 26.6 57.0 54.5

Scratch-R-CNN[2] 49.9 60.6 24.7 23.7 20.3 52.5 64.8 32.9 20.4 43.5 34.2 29.9 49.0 60.4

Scratch-Ours

52.6 60.5 23.8 24.3 18.1 50.6 65.9 29.2 19.5 43.5 35.2 27.6 46.5 59.4

Ours-projection 58.4 62.8 33.5 27.7 24.4 58.5 68.5 41.2 26.3 49.5 42.6 37.3 55.7 62.5

Ours-color-dropping 60.5 66.5 29.6 28.5 26.3 56.1 70.4 44.8 24.6 45.5 45.4 35.1 52.2 60.2

Ours-Yahoo100m 56.2 63.9 29.8 27.8 23.9 57.4 69.8 35.6 23.7 47.4 43.0 29.5 52.9 62.0

43.2 12.0 21.1 36.1 46.0 43.5 33.7 29.9 20.0 41.1 36.4 48.6 53.2 38.5 43.4 16.4 36.6 37.7 59.4 52.3 41.7 47.5 28.0 42.3 28.6 51.2 50.0 40.7 46.5 25.6 42.4 23.5 50.0 50.6 39.8 49.4 29.0 47.5 28.4 54.7 56.8 45.7 50.0 28.1 46.7 42.6 54.8 58.6 46.3 48.7 28.4 45.1 33.6 49.0 55.5 44.2

Ours-VGG

63.6 64.4 42.0 42.9 18.9 67.9 69.5 65.9 28.2 48.1 58.4 58.5 66.2 64.9 54.1 26.1 43.9 55.9 69.8 50.9 53.0

ImageNet-R-CNN[19] 64.2 69.7 50 41.9 32.0 62.6 71.0 60.7 32.7 58.5 46.5 56.1 60.6 66.8 54.2 31.5 52.8 48.9 57.9 64.7 54.2

Table 1. Results on VOC-2007. R-CNN performance with our unsupervised pre-training is 5% MAP better than training from scratch, but

still 8% below pre-training with ImageNet label supervision.

rent state-of-the-art R-CNN pipeline [19]. R-CNN works on object proposals that have been resized to 227x227. Our algorithm, however, is aimed at 96x96 patches. We ﬁnd that downsampling the proposals to 96x96 loses too much detail. Instead, we adopt the architecture shown in Figure 6. As above, we use only one stack from Figure 3. Second, we resize the convolution layers to operate on inputs of 227x227. This results in a pool5 that is 7x7 spatially, so we must convert the previous fc6 layer into a convolution layer (which we call conv6) following [35]. Note our conv6 layer has 4096 channels, where each unit connects to a 3x3 region of pool5. A conv layer with 4096 channels would be quite expensive to connect directly to a 4096-dimensional fullyconnected layer. Hence, we add another layer after conv6 (called conv6b), using a 1x1 kernel, which reduces the dimensionality to 1024 channels (and adds a nonlinearity). Finally, we feed the outputs through a pooling layer to a fully connected layer (fc7) which in turn connects to a ﬁnal fc8 layer which feeds into the softmax. We ﬁne-tune this network according to the procedure described in [19] (conv6b, fc7, and fc8 start with random weights), and use fc7 as the ﬁnal representation. We do not use boundingbox regression, and take the appropriate results from [19] and [2].
Table 1 shows our results. Our architecture trained from scratch (random initialization) performs slightly worse than AlexNet trained from scratch. However, our pre-training makes up for this, boosting the from-scratch number by 6% MAP, and outperforms an AlexNet-style model trained from scratch on Pascal by over 5%. This puts us about 8% behind the performance of R-CNN pre-trained with ImageNet labels [19]. This is the best result we are aware of on VOC 2007 without using labels outside the dataset. We ran additional baselines initialized with batch normalization, but found they performed worse than the ones shown.
To understand the effect of various dataset biases [52], we also performed a preliminary experiment pre-training on a randomly-selected 2M subset of the Yahoo/Flickr 100million Dataset [51], which was collected entirely automatically. The performance after ﬁne-tuning is slightly worse than Imagenet, but there is still a considerable boost over the from-scratch model. We also performed a preliminary ex-

periment with a VGG-style [46] (16-layer) network, shown as “Ours-VGG” in Table 1.
4.4. Visual Data Mining
Visual data mining [41, 13, 47, 42], or unsupervised object discovery [48, 44, 20], aims to use a large image collection to discover image fragments which happen to depict the same semantic objects. Applications include dataset visualization, content-based retrieval, and tasks that require relating visual data to other unstructured information (e.g. GPS coordinates [13]). For automatic data mining, our approach from section 4.1 is inadequate: although object patches match to similar objects, textures match just as readily to similar textures. Suppose, however, that we sampled two non-overlapping patches from the same object. Not only would the nearest neighbor lists for both patches share many images, but within those images, the nearest neighbors would be in roughly the same spatial conﬁguration. For texture regions, on the other hand, the spatial conﬁgurations of the neighbors would be random, because the texture has no global layout.
To implement this, we ﬁrst sample a constellation of four adjacent patches from an image (we use four to reduce the likelihood of a matching spatial arrangement happening by chance). We ﬁnd the top 100 images which have the strongest matches for all four patches, ignoring spatial layout. We then use a type of geometric veriﬁcation [7] to ﬁlter away the images where the four matches are not geometrically consistent. Because our features are more semantically-tuned, we can use a much weaker type of geometric veriﬁcation than [7]. Finally, we rank the different constellations by counting the number of times the top 100 matches geometrically verify. Implementation Details: To compute whether a set of four matched patches geometrically veriﬁes, we ﬁrst compute the best-ﬁtting square S to the patch centers (via leastsquares), while constraining that side of S be between 2/3 and 4/3 of the average side of the patches. We then compute the squared error of the patch centers relative to S (normalized by dividing the sum-of-squared-errors by the square of the side of S). The patch is geometrically veriﬁed if this normalized squared error is less than 1. When sampling

1427

1

88

4

121

7

131

12

142

25

179

29

187

30

229

229

35

232

232

46

240

70

256

71

351

73

464

Figure 7. Object clusters discovered by our algorithm. The number beside each cluster indicates its ranking, determined by the fraction of the top matches that geometrically veriﬁed. For all clusters, we show the raw top 7 matches that veriﬁed geometrically. The full ranking is available on our project webpage.

patches do not use any of the data augmentation preprocessing steps (e.g. downsampling). We use the color-dropping version of our network.
We applied the described mining algorithm to Pascal VOC 2011, with no pre-ﬁltering of images and no additional labels. We show some of the resulting patch clusters in Figure 7. The results are visually comparable to our previous work [12], although we discover a few objects that were not found in [12], such as monitors, birds, torsos, and plates of food. The discovery of birds and torsos—which are notoriously deformable—provides further evidence for the invariances our algorithm has learned. We believe we have covered all objects discovered in [12], with the exception of (1) trusses and (2) railroad tracks without trains (though we do discover them with trains). For some objects like dogs, we discover more variety and rank the best ones higher. Furthermore, many of the clusters shown in [12] depict gratings (14 out of the top 100), whereas none of ours do (though two of our top hundred depict diffuse gradients).

As in [12], we often re-discover the same object multiple times with different viewpoints, which accounts for most of the gaps between ranks in Figure 7. The main disadvantages of our algorithm relative to [12] are 1) some loss of purity, and 2) that we cannot currently determine an object mask automatically (although one could imagine dynamically adding more sub-patches to each proposed object).
To ensure that our algorithm has not simply learned an object-centric representation due to the various biases [52] in ImageNet, we also applied our algorithm to 15,000 Street View images from Paris (following [13]). The results in Figure 8 show that our representation captures scene layout and architectural elements. For this experiment, to rank clusters, we use the de-duplication procedure originally proposed in [13].
4.4.1 Quantitative Results
As part of the qualitative evaluation, we applied our algorithm to the subset of Pascal VOC 2007 selected in [47]: speciﬁcally, those containing at least one instance of bus,

1428

1:
4:
5:
6:
13:
18:
42:
53:
Figure 8. Clusters discovered and automatically ranked via our algorithm (§ 4.4) from the Paris Street View dataset. dining table, motorbike, horse, sofa, or train, and evaluate via a purity coverage curve following [12]. We select 1000 sets of 10 images each for evaluation. The evaluation then sorts the sets by purity: the fraction of images in the cluster containing the same category. We generate the curve by walking down the ranking. For each point on the curve, we plot average purity of all sets up to a given point in the ranking against coverage: the fraction of images in the dataset that are contained in at least one of the sets up to that point. As shown in Figure 9, we have gained substantially in terms of coverage, suggesting increased invariance for our learned feature. However, we have also lost some highly-pure clusters compared to [12]—which is not very surprising considering that our validation procedure is considerably simpler. Implementation Details: We initialize 16,384 clusters by sampling patches, mining nearest neighbors, and geometric veriﬁcation ranking as described above. The resulting clusters are highly redundant. The cluster selection procedure of [12] relies on a likelihood ratio score that is calibrated across clusters, which is not available to us. To select clusters, we ﬁrst select the top 10 geometrically-veriﬁed neighbors for each cluster. Then we iteratively select the highest-ranked cluster that contributes at least one image to our coverage score. When we run out of images that aren’t included in the coverage score, we choose clusters to cover each image at least twice, and then three times, and so on.
4.5. Accuracy on the Relative Prediction Task Task
Can we improve the representation by further training on our relative prediction pretext task? To ﬁnd out, we

Purity-Coverage for Proposed Objects
1.0

0.9

0.8

0.7

0.6

Purity

0.5

0.4

Visual Words .63 (.37)

0.3 Russel et al. .66 (.38)

0.2

HOG Kmeans .70 (.40) Singh et al. .83 (.47)

0.1

Doersch et al. .83 (.48) Our Approach .87 (.48)

0.00.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0

Coverage

Figure 9. Purity vs coverage for objects discovered on a subset of

Pascal VOC 2007. The numbers in the legend indicate area under

the curve (AUC). In parentheses is the AUC up to a coverage of .5.

brieﬂy analyze classiﬁcation performance on pretext task itself. We sampled 500 random images from Pascal VOC 2007, sampled 256 pairs of patches from each, and classiﬁed them into the eight relative-position categories from Figure 2. This gave an accuracy of 38.4%, where chance performance is 12.5%, suggesting that the pretext task is quite hard (indeed, human performance on the task is similar). To measure possible overﬁtting, we also ran the same experiment on ImageNet, which is the dataset we used for training. The network was 39.5% accurate on the training set, and 40.3% accurate on the validation set (which the network never saw during training), suggesting that little overﬁtting has occurred.
One possible reason why the pretext task is so difﬁcult is because, for a large fraction of patches within each image, the task is almost impossible. Might the task be easiest for image regions corresponding to objects? To test this hypothesis, we repeated our experiment using only patches sampled from within Pascal object ground-truth bounding boxes. We select only those boxes that are at least 240 pixels on each side, and which are not labeled as truncated, occluded, or difﬁcult. Surprisingly, this gave essentially the same accuracy of 39.2%, and a similar experiment only on cars yielded 45.6% accuracy. So, while our algorithm is sensitive to objects, it is almost as sensitive to the layout of the rest of the image.
Acknowledgements We thank Xiaolong Wang and Pulkit Agrawal for help with baselines, Berkeley and CMU vision group members for many fruitful discussions, and Jitendra Malik for putting gelato on the line. This work was partially supported by Google Graduate Fellowship to CD, ONR MURI N000141010934, Intel research grant, an NVidia hardware grant, and an Amazon Web Services grant.

References
[1] E. H. Adelson. On seeing stuff: the perception of materials by humans and machines. In Photonics West 2001-Electronic Imaging, 2001. 2
[2] P. Agrawal, R. Girshick, and J. Malik. Analyzing the performance of multilayer neural networks for object recognition. In ECCV. 2014.

1429

5, 6 [3] R. K. Ando and T. Zhang. A framework for learning predictive struc-
tures from multiple tasks and unlabeled data. JMLR, 2005. 1 [4] Y. Bengio, E. Thibodeau-Laufer, G. Alain, and J. Yosinski. Deep
generative stochastic networks trainable by backprop. ICML, 2014. 2 [5] D. Brewster and A. D. Bache. Treatise on optics. Blanchard and Lea, 1854. 3 [6] O. Chum, M. Perdoch, and J. Matas. Geometric min-hashing: Finding a (thick) needle in a haystack. In CVPR, 2009. 3 [7] O. Chum, J. Philbin, J. Sivic, M. Isard, and A. Zisserman. Total recall: Automatic query expansion with a generative feature model for object retrieval. In ICCV, 2007. 6 [8] R. G. Cinbis, J. Verbeek, and C. Schmid. Segmentation driven object detection with Fisher vectors. In ICCV, 2013. 6 [9] R. Collobert and J. Weston. A uniﬁed architecture for natural language processing: Deep neural networks with multitask learning. In ICML, 2008. 1, 2 [10] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei. Imagenet: A large-scale hierarchical image database. In CVPR, 2009. 4 [11] C. Doersch, A. Gupta, and A. A. Efros. Mid-level visual element discovery as discriminative mode seeking. In NIPS, 2013. 3 [12] C. Doersch, A. Gupta, and A. A. Efros. Context as supervisory signal: Discovering objects with predictable context. In ECCV. 2014. 2, 7, 8 [13] C. Doersch, S. Singh, A. Gupta, J. Sivic, and A. A. Efros. What makes Paris look like Paris? SIGGRAPH, 2012. 3, 6, 7 [14] J. Domke, A. Karapurkar, and Y. Aloimonos. Who killed the directed model? In CVPR, 2008. 2 [15] M. Everingham, L. Van Gool, C. K. Williams, J. Winn, and A. Zisserman. The pascal visual object classes (voc) challenge. IJCV, 2010. 5 [16] A. Faktor and M. Irani. clustering by composition–unsupervised discovery of image categories. In ECCV. 2012. 3 [17] P. Felzenszwalb, R. Girshick, D. McAllester, and D. Ramanan. Object detection with discriminatively trained part-based models. PAMI, 2010. 6 [18] P. Fo¨ldia´k. Learning invariance from transformation sequences. Neural Computation, 1991. 3 [19] R. Girshick, J. Donahue, T. Darrell, and J. Malik. Rich feature hierarchies for accurate object detection and semantic segmentation. In CVPR, 2014. 1, 5, 6 [20] K. Grauman and T. Darrell. Unsupervised learning of categories from sets of partially matching image features. In CVPR, 2006. 3, 6 [21] K. Heath, N. Gelfand, M. Ovsjanikov, M. Aanjaneya, and L. J. Guibas. Image webs: Computing and exploiting connectivity in image collections. In CVPR, 2010. 3 [22] G. Hinton, S. Osindero, and Y.-W. Teh. A fast learning algorithm for deep belief nets. Neural computation, 2006. 2 [23] G. E. Hinton, P. Dayan, B. J. Frey, and R. M. Neal. The “wakesleep” algorithm for unsupervised neural networks. Proceedings. IEEE, 1995. 2 [24] S. Ioffe and C. Szegedy. Batch normalization: Accelerating deep network training by reducing internal covariate shift. arXiv preprint arXiv:1502.03167, 2015. 5 [25] Y. Jia, E. Shelhamer, J. Donahue, S. Karayev, J. Long, R. Girshick, S. Guadarrama, and T. Darrell. Caffe: Convolutional architecture for fast feature embedding. In ACM-MM, 2014. 4 [26] M. Juneja, A. Vedaldi, C. V. Jawahar, and A. Zisserman. Blocks that shout: Distinctive parts for scene classiﬁcation. In CVPR, 2013. 3 [27] G. Kim, C. Faloutsos, and M. Hebert. Unsupervised modeling of object categories using link analysis techniques. In CVPR, 2008. 3 [28] D. P. Kingma and M. Welling. Auto-encoding variational bayes. 2014. 2

[29] A. Krizhevsky, I. Sutskever, and G. Hinton. Imagenet classiﬁcation with deep convolutional neural networks. In NIPS, 2012. 1, 3
[30] H. Larochelle and I. Murray. The neural autoregressive distribution estimator. In AISTATS, 2011. 2
[31] Q. V. Le. Building high-level features using large scale unsupervised learning. In ICASSP, 2013. 2
[32] H. Lee, A. Battle, R. Raina, and A. Y. Ng. Efﬁcient sparse coding algorithms. In NIPS, 2006. 2
[33] Y. J. Lee and K. Grauman. Foreground focus: Unsupervised learning from partially matching images. IJCV, 2009. 3
[34] Q. Li, J. Wu, and Z. Tu. Harvesting mid-level visual concepts from large-scale internet images. In CVPR, 2013. 3
[35] J. Long, E. Shelhamer, and T. Darrell. Fully convolutional networks for semantic segmentation. arXiv preprint arXiv:1411.4038, 2014. 6
[36] T. Malisiewicz and A. Efros. Beyond categories: The visual memex model for reasoning about object relationships. In NIPS, 2009. 2
[37] T. Mikolov, I. Sutskever, K. Chen, G. S. Corrado, and J. Dean. Distributed representations of words and phrases and their compositionality. In NIPS, 2013. 1, 2
[38] D. Okanohara and J. Tsujii. A discriminative language model with pseudo-negative samples. In ACL, 2007. 1, 2
[39] B. A. Olshausen and D. J. Field. Emergence of simple-cell receptive ﬁeld properties by learning a sparse code for natural images. Nature, 1996. 2
[40] N. Payet and S. Todorovic. From a set of shapes to object discovery. In ECCV. 2010. 3
[41] T. Quack, B. Leibe, and L. Van Gool. World-scale mining of objects and events from community photo collections. In CIVR, 2008. 3, 6
[42] K. Rematas, B. Fernando, F. Dellaert, and T. Tuytelaars. Dataset ﬁngerprints: Exploring image collections through data mining. In CVPR, 2015. 3, 6
[43] D. J. Rezende, S. Mohamed, and D. Wierstra. Stochastic backpropagation and approximate inference in deep generative models. ICML, 2014. 2
[44] B. C. Russell, W. T. Freeman, A. A. Efros, J. Sivic, and A. Zisserman. Using multiple segmentations to discover objects and their extent in image collections. In CVPR, 2006. 2, 6
[45] R. Salakhutdinov and G. E. Hinton. Deep boltzmann machines. In ICAIS, 2009. 2
[46] K. Simonyan and A. Zisserman. Very deep convolutional networks for large-scale image recognition. CoRR, 2014. 6
[47] S. Singh, A. Gupta, and A. A. Efros. Unsupervised discovery of mid-level discriminative patches. In ECCV, 2012. 3, 6, 7
[48] J. Sivic, B. C. Russell, A. A. Efros, A. Zisserman, and W. T. Freeman. Discovering objects and their location in images. In ICCV, 2005. 2, 6
[49] J. Sun and J. Ponce. Learning discriminative part detectors for image classiﬁcation and cosegmentation. In ICCV, 2013. 3
[50] L. Theis and M. Bethge. Generative image modeling using spatial lstms. In NIPS, 2015. 2
[51] B. Thomee, D. A. Shamma, G. Friedland, B. Elizalde, K. Ni, D. Poland, D. Borth, and L.-J. Li. The new data and new challenges in multimedia research. arXiv preprint arXiv:1503.01817, 2015. 6
[52] A. Torralba and A. A. Efros. Unbiased look at dataset bias. In CVPR, 2011. 6, 7
[53] P. Vincent, H. Larochelle, Y. Bengio, and P.-A. Manzagol. Extracting and composing robust features with denoising autoencoders. In ICML, 2008. 2
[54] X. Wang and A. Gupta. Unsupervised learning of visual representations using videos. In ICCV, 2015. 3
[55] X. Wang, M. Yang, S. Zhu, and Y. Lin. Regionlets for generic object detection. In ICCV, 2013. 6
[56] L. Wiskott and T. J. Sejnowski. Slow feature analysis:unsupervised learning of invariances. Neural Computation, 2002. 3

1430

