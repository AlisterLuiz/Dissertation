arXiv:2003.10769v1 [eess.IV] 22 Mar 2020

Estimating Uncertainty and Interpretability in Deep Learning for Coronavirus (COVID-19) Detection
Biraja Ghoshal1 and Allan Tucker1
Brunel University London, Uxbridge, UB8 3PH, United Kingdom biraja.ghoshal@brunel.ac.uk
https://www.brunel.ac.uk/computer-science
Abstract. Deep Learning has achieved state of the art performance in medical imaging. However, these methods for disease detection focus exclusively on improving the accuracy of classiﬁcation or predictions without quantifying uncertainty in a decision. Knowing how much conﬁdence there is in a computer-based medical diagnosis is essential for gaining clinicians’ trust in the technology and therefore improve treatment. Today, the 2019 Coronavirus (COVID-19) infections are a major healthcare challenge around the world. Detecting COVID-19 in X-ray images is crucial for diagnosis, assessment and treatment. However, diagnostic uncertainty in a report is a challenging yet inevitable task for radiologists. In this paper, we investigate how Dropweights based Bayesian Convolutional Neural Networks (BCNN) can estimate uncertainty in Deep Learning solutions to improve the diagnostic performance of the human-machine combination using publicly available COVID-19 chest X-ray dataset and show that the uncertainty in prediction is strongly correlates with the accuracy of the prediction. We believe that the availability of uncertainty-aware deep learning solution will enable a wider adoption of Artiﬁcial Intelligence (AI) in a clinical setting.
Keywords: Bayesian Deep Learning, Predictive Entropy, Uncertainty Estimation, Dropweights, COVID-19
1 Introduction
In recent years, Deep Learning has achieved state of the art performance, similar to that of human experts in solving classiﬁcation tasks in computer vision from lung disease classiﬁcation, metastasis detection for breast cancer, skin lesion classiﬁcation, identifying diabetic retinopathy, attention deﬁcit hyperactivity disorder (ADHD), Alzheimer’s disease and improving reconstruction for MRI, PET/CT imaging. However, despite the promising results, deep learning for classiﬁcation tasks lacks the ability to say “I don’t know” in an ambiguous or unknown case. Hence, it is critical to estimate uncertainty in medical imaging as an additional insight to point predictions to improve the reliability in making decisions.

2

B. Ghoshal et al.

Dealing with Coronavirus (COVID-19) is one of the major healthcare challenges around the world today. COVID-19 represents a new strain of Coronavirus and presumably representing a mutation of other Coronaviruses [14].
The existing infrastructure (e.g. limited image data sources with expert labelled data set) for the detection of COVID-19 positive patients is insuﬃcient and manual detection is time-consuming. With the increase in global incidences, it is expected that Deep learning based solution will soon be developed and combined with clinical practices to provide cost-eﬀective, accurate and easily performed automated detection of COVID-19 to aid the screening process.
However, despite remarkable performance, deep learning models tend to make overconﬁdent predictions. Our objective is not to achieve state-of-the-art performance, but rather to evaluate the usefulness of estimating uncertainty approximating Bayesian Convolutional Neural Networks (BCNN) with Dropweights to improve the diagnostic performance of the human-machine combination [7,9]. This is crucial in diﬀerentiating COVID-19 patients from those without the disease, where the cost of an error is very high. Thus, in order to avoid COVID-19 misdiagnoses [13], it is necessary to estimate uncertainty in a model’s predictions.
In this paper, we investigate how Monte-Carlo Dropweights (MC Dropweights) Bayesian convolutional neural networks can estimate uncertainty in Deep Learning solution to improve the diagnostic performance of the human-machine combinations, using publicly available COVID-19 chest X-ray datasets, and show that the estimated uncertainty in prediction has a strong correlation with classiﬁcation accuracy, thus enabling the identiﬁcation of false predictions or unknown cases.

2 Related Research
Estimating uncertainty in deep neural networks is a challenging and unsolved problem. There are many measures to estimate uncertainty such as softmax variance, expected entropy, mutual information, predictive entropy and averaging predictions over multiple models.
Bayesian Neural Networks (BNN) provides a natural framework for modelling uncertainty [3]. However, BNN methods are intractable in computing the posterior of a network’s parameters. The most used approach to estimate uncertainty in deep learning try to place distributions over each of the network’s weight parameters [3] of a model.
There are many methods proposed for quantifying uncertainty or conﬁdence estimates approximated by Monte-Carlo Dropout, including Laplace approximation, Markov chain Monte Carlo (MCMC) methods, stochastic gradient MCMC variants such as Langevin Dynamics, Hamiltonian methods, including Multiplicative Normalizing Flows, Stochastic Batch Normalization, Maximum Softmax Probability, Heteroscedastic Classiﬁer, and Learned Conﬁdence Estimates including Deep Ensembles [6].

Estimating uncertainty in Deep Learning for COVID-19 Detection

3

3 Approximate Bayesian Convolutional Neural Networks (BCNN) and Model Uncertainty

Given dataset X = {x1, x2 . . . xN } and the corresponding labels Y = {y1, y2 . . . yN } where X ∈ Rd is a d-dimensional input vector and Y ∈ {1 . . . C} with yi ∈ {1 . . . C}, given C class label, a set of independent and identically distributed
(i.i.d.) training samples size N {xi, yi} for i = 1 to N , the objective is to ﬁnd a function f : X → Y using weights of neural net parameters w as close as possible to the original function that has generated the outputs Yˆ . The principled predic-
tive distribution of an unknown label yˆ of a test input data xˆ by marginalizing
the parameters:

p(yˆ|xˆ, X , Y) = P (yˆ|xˆ, w)P (w|X, Y, xˆ)dw

(1)

Unfortunately, ﬁnding the posterior distribution p(w|X, Y ) is often computationally intractable. Recently, Gal [6] proved that a gradient-based optimization procedure on the dropout neural network is equivalent to a speciﬁc variational approximation on a Bayesian neural network. Following Gal [6], Ghoshal et al. [8] also showed similar results for neural networks with MC-Dropweights. The model uncertainty is approximated by averaging stochastic feed forward Monte Carlo (MC) sampling during inference. At test time, the unseen samples are passed through the network before the Softmax predictions are analyzed.
Practically, the expectation of yˆ is called the predictive mean of the model. The predictive mean µpred over the MC iterations is then used as the ﬁnal prediction on the test sample:

1T

µpred ≈ T

p(yˆ|xˆ, X , Y)

(2)

t=1

For each test sample xˆ, the class with the largest predictive mean µpred is selected as the output prediction and the variance is the predictive uncertainty.

3.1 Uncertainty Estimation in Classiﬁcation
In order for COVID-19 detection to be meaningful, tolerance must typically be much tighter. Based on the input X-ray image, a network can be certain with high or low conﬁdence about its decision, indicated by the predictive posterior distribution.
However predictive uncertainty in deep learning actually results from two separate forms of uncertainty [5]:
1. Epistemic uncertainty or Model uncertainty accounts for uncertainty in the model parameters as it does not take all of the aspects of the data into account or the lack of training data. Epistemic uncertainty associated with the model reduces as the training data size increases.

4

B. Ghoshal et al.

2. Aleatoric uncertainty accounts for noise inherent in the observations due to class overlap, label noise, homoscedastic and heteroscedastic noise, which cannot be reduced even if more data were to be collected. In X-ray imaging, this can be caused by sensor noise due to random distribution of photons during scan acquisition.
Traditionally, it has been diﬃcult to implement model validation under epistemic uncertainty. Thus, we estimate epistemic uncertainty to obtain model uncertainty in deep learning prediction for chest radiograph diagnosis for COVID19. One of the measure of model uncertainty is predictive entropy H of the predictive distribution:

H(yˆ|xˆ, X , Y) = − p(yˆ = c|xˆ, X , Y) log p(yˆ = c|xˆ, X , Y)

(3)

C

where C ranges over all class labels. In general, the range of the obtained uncertainty values depend on datasets, network architectures, number of MC sampling, etc. Therefore, we normalise estimated uncertainty to report our results and facilitate the comparison across various sets and conﬁgurations.
Our analysis involved a comparison of two variational-dropweights based uncertainty measures, Predictive Entropy (PH) and Bayesian Active Learning by Disagreement (BALD)[11,15], in their application to COVID-19 image classiﬁcation.
The second uncertainty measure, Bayesian Active Learning by Disagreement (BALD), is based on mutual information that maximise the mutual information between model posterior density function and predictions density function approximated at as the diﬀerence between the entropy of the predictive distribution and the mean entropy of predictions across samples:

M I [yˆi, w|xˆi, X, Y] ≈ H [yˆi|xˆi, X, Y ] − E [H [yˆi|xˆiw]]

(4)

, with w the model parameters. Test points that maximise mutual information are points over which the
model is uncertain on average, but there are model parameters that produce erroneous predictions with a high conﬁdence. This is equivalent to points with high variance in the input to the softmax layer (the logits). Thus, each stochastic forward pass through the model would have the highest probability assigned to a diﬀerent class. It is expected from BALD measures epistemic uncertainty of the model, so it would not return a high value if there is aleoratic uncertainty present.

3.2 Relationship between the Accuracy and Uncertainty
The true error is the diﬀerence between estimated values and actual values. In order to assess the quality of predictive uncertainty, we leveraged Spearman’s correlation coeﬃcient between Predictive Entropy (PH) and Bayesian Active Learning by Disagreement (BALD). We quantiﬁed the predictive accuracy by

Estimating uncertainty in Deep Learning for COVID-19 Detection

5

1-Wasserstein distance (WD) to measure how much the estimated uncertainty correlates with the true errors [2,12]. The Wasserstein distance for the real data distribution Pr and the generated data distribution Pg is mathematically deﬁned as the greatest lower bound (inﬁmum) for any transport plan (i.e. the cost for the cheapest plan):

W (Pr, Pg) = inf E(x,y)∼γ [ x − y ]

(5)

γ∼Π(Pr ,Pg )

, Π(Pr, Pg) is the set of all possible joint probability distributions γ(x, y) whose marginals are respectively Pr and Pg. However, the equation (5) for the Wasserstein distance is intractable. Using the Kantorovich-Rubinstein duality, [2] simpliﬁed the calculation to

1

W (Pr, Pg) = K

sup Ex∼Pr [f (x)] − Ex∼Pg [f (x)]
f L≤K

(6)

, where sup (supremum) is the opposite of inf (inﬁmum); sup is the least upper bound and f is a 1-Lipschitz continuous functions {fw}w∈W , parameterized by w and the K-Lipschitz constraint |f (x1) − f (x2)| ≤ K|x1 − x2|. The error function can be conﬁgured as measuring the 1 - Wasserstein distance between Pr and Pg.

E (Pr ,

Pg )

=

W

(Pr ,

Pg )

=

max
w∈W

Ex∼Pr

[fw (x)]

−

Ez∼Pr (z) [fw (gθ (z ))]

(7)

The advantage of Wasserstein distance (WD) is that it can reﬂect the distance of two non-overlapping or little overlapping distributions.

4 Dataset
Radiologists frequently use X-ray images to detect lung inﬂammation, enlarged lymph nodes or pneumonia. Once the COVID-19 virus is inside the body, it begins infecting epithelial cells lining the lung. We can use X-rays to analyse the health of a patient’s lungs. Analysis of X-ray requires an expert and takes signiﬁcant time.

4.1 Data Preparation
We have selected 70 Posterioranterior (PA) view of the lungs X-ray images of COVID-19 positive cases from Dr. Joseph Cohen’s Github repository [4]. We augment the dataset with Kaggle’s Chest X-Ray Images (Pneumonia) from healthy patients.

6

B. Ghoshal et al.

5 Experiment

Instead of training a very deep model from scratch on a small dataset, we decided to run this experiment in a transfer learning setting, where we use a pre-trained VGG16 model and acquire data only to ﬁne-tune the original model. This is very suitable when the data is abound for an auxiliary domain, but very limited labelled data is available for the domain of experiment. Instead of introducing fully connected layers on top of the VGG16 convolutional base, we added a global average pooling layer and fed its output directly into fully-connected layer. Dropweights followed by a softmax activated layer is applied to the network as an approximation to the Gaussian Process (GP) and to cast as an approximate Bayesian inference, in the fully connected layer to estimate meaningful model uncertainty. The softmax layer outputs the probability distribution over each possible class label.
We resized all images to 512 x 512 pixels (using a bicubic interpolation over 4 x 4 pixel neighbourhood). The images were standardised using the mean and standard deviation values of the X-ray dataset. We split the whole dataset into 80% - 20% between training and testing. We took the average performance of the model on each of the 10 fold cross-validation. Real-time data augmentation was also applied, leveraging Keras ImageDataGenerator during training to prevent overﬁtting and enhance the learning capability of the model. Training images were ZCA whitened, rotated 20 degree, randomly ﬂipped along horizontally and vertically, scale outward and inward, shifted, and sheared. The Adam optimiser was used with a learning rate of 1e-5 with decay and batch size of 8. Early stopping was used whilst ﬁne-tuning for 50 epochs. Dropweights with rate {0.1, 0.3, 0.5, 0.7 and 0.9} added to fully-connected layer. During test time, Dropweights was active and Monte Carlo sampling was performed by feeding the input image with MC-samples {10, 25, 50, 100} through the Bayesian VGGNet.

Estimating uncertainty in Deep Learning for COVID-19 Detection

7

6 Results and Discussions

6.1 Uncertainty-Aware Prediction Performance of the Bayesian Models
Most of COVID-19 cases’ chest X-rays show bilateral pulmonary inﬁltrates with distinctive appearances. The below Figure 1 and 2 shows example input images and the corresponding predictive distributions generated by Bayesian DNN. While Bayesian model outputs prediction distributions for each class (ground truth in green and predicted class in red), for each input image, the class with the highest softmax output for predictive distribution mean is considered as the prediction and the predictive entropy of the output distributions (measured as in Equation (3)) as the estimated epistemic uncertainty. Based on the input image, a network can be certain with high or low conﬁdence about its decision, indicated by the predictive posterior distribution. This is because the uncertainty in weight space captured by the posterior is incorporated into the predictive uncertainty, giving us a way to model to say “I don’t know”.

Fig. 1. Example input images highest uncertainty and the corresponding predictive distributions generated by Bayesian DNN

Fig. 2. Example input images lowest uncertainty and the corresponding predictive distributions generated by Bayesian DNN

8

B. Ghoshal et al.

6.2 Bayesian Model Uncertainty
We measured the epistemic uncertainty associated with the predictive probabilities of the deep learning model by keeping dropweights on during test time. Figure 3 below shows Kernel Density Estimation with a Gaussian kernel is used to plot the output posterior distributions for all X-Rays test images, grouped by correct and erroneous predictions with variation of dropwights rate p for 100 MC samples of stochastic feed forward. The table below shows the eﬀect of variation of the dropweights rate, p, to the uncertainty measures.

Fig. 3. Distribution of Estimated Uncertainty
It shows that the estimated uncertainty is higher for errorous predictions. Therefore, uncertainty information provides as an additional insight to point prediction to refer the uncertain images to radiologists for further investigation [12], which improves the overall prediction performance.

Fig. 4. Bayesian DNN prediction performance

Estimating uncertainty in Deep Learning for COVID-19 Detection

9

Figure 4 shows the eﬀect of variation of the Dropweights rate p to the uncertainty measures (PH and BALD). The above results suggest, that predictive entropy as a measure of uncertainty is a better measure for uncertainty and should be considered over BALD. Regardless of values for the number of MC samples and Dropweights rate, we can observe a higher uncertainty for incorrect classiﬁcation. MC dropweights for uncertainty estimation can usually be used in every image classiﬁer to improve prediction accuracy of man–machine combination via uncertainty-aware referral with the additional computational load cost of performing multiple forward passes.

6.3 The relation between uncertainty and predictive accuracy
The table 1 in below shows that there is strong correlation between predictive entropy and the prediction error.

Spearmans’s Correlation Predictive Entropy BALD

Dropweights Rate: 0.9 0.9999

-0.1636

Dropweights Rate: 0.7 0.9999

-0.2030

Dropweights Rate:0.5 0.9999

0.4762

Dropweights Rate:0.3 0.9999

0.4039

Dropweights Rate:0.1 0.9999

0.7039

The ﬁgure 5 below shows the correlation between estimated uncertainty from PH and BALD and the error of prediction with variation of the Dropweights rate P . The above results show strong correlation with ρ = 0.99 between entropy of the probabilities as a measure of the epistemic uncertainty and prediction errors.

Fig. 5. Correlation between estimated predictive entropy as a measure of Uncertainty and Accuracy in prediction
Our experiments show that the prediction uncertainty correlates with accuracy, thus enabling the identiﬁcation of false predictions or unknown cases.
6.4 Performance improvement via Uncertainty-Aware COVID-19 Classiﬁcation and Referral
We performed predictions for all COVID-19 test images and sorted the predictions by their associated predictive uncertainty (PH). We then referred predictions

10

B. Ghoshal et al.

based on the various levels of uncertainty for further diagnosis and measured the accuracy of the predictions (threshold at 0.5) for the remaining cases. We observed the prediction accuracy increases with the fraction of referred images. Note that only non-referred images are considered to compute predictive accuracy. We have also observed the same behaviour in prediction accuracy for increasing levels of model uncertainty.

Fig. 6. Distribution of Estimated Uncertainty
Simulating a control experiment, we compared with randomly selected images, that is without using uncertainty in prediction (Fig. 6 (b), blue curve).
For a beginner radiologist performance (i.e. 60% prediction accuracy), solely relying on deep learning models will result in a more accurate prediction on overall diagnosis. However, for an experienced radiologist (i.e. 80% accuracy), the combined performance reaches almost 90% when rejecting either almost 40% of the most uncertain samples or samples with Hnorm >= 0.4. For less than 2% decisions referred for further inspections, there is a 95% conﬁdence interval of the two non-overlapping scenarios. Hence, estimated uncertainty provides as an additional insight to point prediction performance to improve the reliability of the automated system.
7 Visualizing Uncertainty and Interpretability
Deep learning models often been accused of being "black boxes", so they need to be precise, interpretable and the uncertainty in predictions must be well understood. Reliable estimated uncertainty alongside the visualisation of distinct features, as an additional insight to point prediction, will improve the ease of understanding in deep learning, resulting in a more informed decision-making process. We qualitatively compare in ﬁgure 7, the saliency maps [1] produced by various state-of-the-art methods e.g.Class Activation Map (CAM), Guided Backpropagation and Guided Gradient CAM and Gradients.

Estimating uncertainty in Deep Learning for COVID-19 Detection

11

Fig. 7. Saliency Map using various methods
8 Conclusion and Future work
In this work, Bayesian Deep Learning classiﬁer has been trained using transfer learning method on COVID-19 X-Ray images to estimate model uncertainty. On average, Bayesian inference improves the detection accuracy of the standard VGG16 model from 85.71% to 92.86% in our sample dataset based solely on X-ray images. Our experiment has shown a strong correlation between model uncertainty and accuracy of prediction. The estimated uncertainty in deep learning yields more reliable prediction, which can alert radiologists on false predictions, which will increase the acceptance of deep learning into clinical practice in disease detection.
With this Bayesian Deep Learning based classiﬁcation, studies correlating with multi "omics" dataset [10], and treatment responses could further reveal insights about imaging markers and ﬁndings towards improved diagnosis and treatment for Covid-19.
References
1. Adebayo, J., Gilmer, J., Muelly, M., Goodfellow, I., Hardt, M., Kim, B.: Sanity checks for saliency maps. In: Advances in Neural Information Processing Systems. pp. 9505–9515 (2018)
2. Arjovsky, M., Chintala, S., Bottou, L.: Wasserstein gan. arXiv preprint arXiv:1701.07875 (2017)
3. Blundell, C., Cornebise, J., Kavukcuoglu, K., Wierstra, D.: Weight uncertainty in neural networks. In Proceedings of the 32nd International Conference on Machine Learning, Proceedings of Machine Learning Research, pages 1613–1622 (2015)
4. Cohen, J.P.: Open database of covid-19 cases (2020), https://github.com/ ieee8023/covid-chestxray-dataset

12

B. Ghoshal et al.

5. Depeweg, S., Hernández-Lobato, J.M., Doshi-Velez, F., Udluft, S.: Decomposition of uncertainty in bayesian deep learning for eﬃcient and risk-sensitive learning. arXiv preprint arXiv:1710.07283 (2017)
6. Gal, Y.: Uncertainty in deep learning. Ph.D. thesis, University of Cambridge (2016) 7. Ghoshal, B., Lindskog, C., Tucker, A.: Estimating uncertainty in deep learning
for reporting conﬁdence: An application on cell type prediction in testes based on proteomics. In: International Symposium on Intelligent Data Analysis. Springer (2020) 8. Ghoshal, B., Tucker, A.: Estimating uncertainty in deep learning for reporting conﬁdence to clinicians in medical image segmentation and diseases detection. Computational Intelligence 1(1) (2019) 9. Ghoshal, B., Tucker, A., Sanghera, B., Wong, W.: Estimating uncertainty in deep learning for reporting conﬁdence to clinicians in medical image segmentation and diseases detection. Computational Intelligence - Special Issue on Foundations of Biomedical (Big) Data Science 1 (2019) 10. Gozes, O., Frid-Adar, M., Greenspan, H., Browning, P.D., Zhang, H., Ji, W., Bernheim, A., Siegel, E.: Rapid ai development cycle for the coronavirus (covid-19) pandemic: Initial results for automated detection & patient monitoring using deep learning ct image analysis. arXiv preprint arXiv:2003.05037 (2020) 11. Houlsby, N.: Eﬃcient Bayesian active learning and matrix modelling. Ph.D. thesis, University of Cambridge (2014) 12. Laves, M.H., Ihler, S., Ortmaier, T., Kahrs, L.A.: Quantifying the uncertainty of deep learning-based computer-aided diagnosis for patient safety. Current Directions in Biomedical Engineering 5(1), 223–226 (2019) 13. Li, Y., Xia, L.: Coronavirus disease 2019 (covid-19): Role of chest ct in diagnosis and management. American Journal of Roentgenology pp. 1–7 (2020) 14. Shan+, F., Gao+, Y., Wang, J., Shi, W., Shi, N., Han, M., Xue, Z., Shen, D., Shi, Y.: Lung infection quantiﬁcation of covid-19 in ct images with deep learning. arXiv preprint arXiv:2003.04655 (2020) 15. Smith, L., Gal, Y.: Understanding measures of uncertainty for adversarial example detection. arXiv preprint arXiv:1803.08533 (2018)

