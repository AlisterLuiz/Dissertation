Momentum Contrastive Learning for Few-Shot COVID-19 Diagnosis from Chest CT Images
Xiaocong Chena,∗, Lina Yaoa, Tao Zhoub, Jinming Donga, Yu Zhangc,∗
aSchool of Computer Science and Engineering at University of New South Wales, NSW 2052, Australia
bInception Institute of Artiﬁcial Intelligence, Abu Dhabi, UAE. cDepartment of Psychiatry and Behavioral Sciences, Stanford University, Stanford, CA
94305, USA.

arXiv:2006.13276v1 [eess.IV] 16 Jun 2020

Abstract
The current pandemic, caused by the outbreak of a novel coronavirus (COVID19) in December 2019, has led to a global emergency that has signiﬁcantly impacted economies, healthcare systems and personal wellbeing all around the world. Controlling the rapidly evolving disease requires highly sensitive and speciﬁc diagnostics. While real-time RT-PCR is the most commonly used, these can take up to 8 hours, and require signiﬁcant eﬀort from healthcare professionals. As such, there is a critical need for a quick and automatic diagnostic system. Diagnosis from chest CT images is a promising direction. However, current studies are limited by the lack of suﬃcient training samples, as acquiring annotated CT images is time-consuming. To this end, we propose a new deep learning algorithm for the automated diagnosis of COVID-19, which only requires a few samples for training. Speciﬁcally, we use contrastive learning to train an encoder which can capture expressive feature representations on large and publicly available lung datasets and adopt the prototypical network for classiﬁcation. We validate the eﬃcacy of the proposed model in comparison with other competing methods on two publicly available and annotated COVID-19 CT datasets. Our results demonstrate the superior performance of our model for the accurate diagnosis of COVID-19 based on chest CT images.
∗Corresponding authors: X. Chen. Email: xiaocong.chen@unsw.edu.au, Y. Zhang. E-mail: yzhangsu@stanford.edu

Preprint submitted to Medical Image Analysis Templates

June 25, 2020

Keywords: COVID-19 Diagnosis, Few-shot Learning, Contrastive Learning, Chest CT Images
1. Introduction
The latest coronavirus, COVID-19, was initially reported in Wuhan, China toward the end of 2019 and has since spread rapidly around the globe, leading to a worldwide crisis [1]. As an infectious lung disease, COVID-19 leads to severe acute respiratory distress syndrome (ARDS) and is accompanied by a series of side eﬀects that include a dry cough, fever, tiredness, shortness of breath, etc. As of May 24th 2020, more than 5.4 million individuals have been conﬁrmed as having COVID-19, with a roughly 6.3% case fatality rate around the world, according to the World Health Organization 1 around the world.
So far, no eﬀective treatment for COVID-19 has been found. One of the major hurdles is the lack of eﬃcient diagnosis methods. Therefore, an accurate and rapid diagnosis platform is urgently required to conduct COVID-19 screening and prevent its further spread. Currently, most tests are based on real-time reverse transcriptase polymerase chain reaction (RT-PCR). However, each PT-PCR test can take several hours to produce results. With the current spread rate of COVID-19, this is not acceptable. Further, the limited number of test kits makes this situation even more serious (Shan et al., 2020; Narin et al., 2020). Recent studies show that the RT-PCR suﬀering from low sensitivity and low accuracy, repeated entries are required (Long et al., 2020; Ai et al., 2020). This infers that patients will not be able to be conﬁrmed on time which increases the potential risk of spreading.
In order to address these challenges, scientists around the world are trying to develop new diagnostic systems. Some studies (Bernheim et al., 2020; Li and Xia, 2020) have demonstrated that chest computed tomography (CT) imaging can help in diagnosing COVID-19 rapidly. Salehi et al. (Salehi et al.,
1https://www.who.int/emergencies/diseases/novel-coronavirus-2019/situation-reports
2

2020) concluded that chest CT imaging is sensitive when diagnosing COVID-19 even when patients do not have clinical symptoms. Speciﬁcally, three typical radiographic features including consolidation, pleural eﬀusion and ground class opaciﬁcation can be easily observed (Huang et al., 2020; Wang et al., 2020; Shi et al., 2020a) on COVID-19 patients CT images.
With this in mind, several methods based on chest CT images been developed for diagnosing COVID-19. For instance, Butt et al. (Butt et al., 2020) adopted a convolutional neural Network (CNN) to classiﬁer patient’s CT images. In addition, there are a few works use the 3D CNN to conduct the diagnosis of COVID-19 as well based on chest CT scans (Gozes et al., 2020; Li et al., 2020; Zheng et al., 2020). Mei et al. (Mei et al., 2020) adopted the ResNet to conduct the rapid diagnosis of the COVID-19. Besides the diagnosis, lots of works are using the segmentation technique to conduct the detection (Chen et al., 2020a; Shi et al., 2020b; Chen et al., 2020d). All those existing methods are trained based on the limited available samples which have small number of patients and may not have capability to generalize to new patients. It is well-known that, the lack of labelled training data is the common problem, while deep learning based methods generally require a large volume of data to accurately train the models. Many research eﬀorts have been sought for alleviating this problem such as data augmentation or through generative adversarial network (GAN) (Zhao et al., 2019; Akkus et al., 2017; Oliveira et al., 2017; Pereira et al., 2016).However, these methods are highly sensitive with the parameter selection. Hand-tuned data augmentation methods like rotation may lead to over-ﬁtting (Eaton-Rosen et al., 2018) and the generated images by GAN can not simulate the real patient which may introduce unpredictable bias in testing phase (Zhao et al., 2019). Recently, few-shot learning has been attracting much attention in medical image analysis. In general, few-shot learning aims to leverage existing data to classify new tasks from similar domains. The basic workﬂow for few-shot learning is ﬁrst pretraining an embedding network on a large dataset (e.g. ImageNet) and then ﬁnetuning the weights of this network, ﬁnally applying it into a unseen small dataset (He et al., 2020). However, the performance is limitedly improved. One reason
3

lies in the ImageNet contains a broad range of categories of images and pre-train on ImageNet usually might bring in unrelevant information, so as not to learn a eﬀective embeddings for improving lung-speciﬁc feature representation. On the other hand, pre-training on ImageNet causes high computational cost, for example, ImageNet-1B normally required more than 50 GPU days.
To address this challenge, we develop an end-to-end trainable deep few-shot learning framework to make an accurate prediction with minimal training Chest CT Images. Speciﬁcally, we ﬁst use the instance discrimination task to enforce model to discriminate two images are the same instance or not. We generate diﬀerent views of the same images to augment the original dataset. As the goal at this stage is increasing variances other than discrimination, we can avoid the disadvantages of data augmentation mentioned previously.
We then deploy a self-supervised strategy (Oord et al., 2018) powered with momentum contrastive training to further boost the performance. The key idea is to build a dynamic dictionary to perform key, query look-up where the keys are sampled from data and encoded by the encoder. However, the key in dictionary is noisy and inconsistent due to the back-propagation (He et al., 2019). The momentum mechanism is applied to mitigate this eﬀect by updating key encoder and query encoder in diﬀerent scales. Finally, we utilized two public lung datasets to pre-train an embedding network and employ the prototypical network (Snell et al., 2017) to conduct the few-shot classiﬁcation, which learns a metric space where the classiﬁcation can be performed by measuring the distances to the derived prototypical representation of each class. The extensive experiments on two new datasets demonstrate that our model provides a promising tool for quick COVID-19 diagnosis with very limited available training data.
2. Problem Deﬁnition
Due to the shortage of the annotated COVID-19 CT images, the normal classiﬁcation methods may not able to work properly. Based on that, we formulate the COVID-19 diagnosis problem as a few-shot classiﬁcation problem. The few-
4

shot learning is designed for the case which only a few samples available in a new class on a classiﬁcation task. The few-shot learning can be deﬁned as a M -way, C-shot episodic task (Vinyals et al., 2016) where M represents the number of classes and C represents number of samples available for each class. The training set which never seen before can be represented as D = {(x0, y0), · · · , (xd, yM )}, d is the number of samples in this dataset. We randomly selected the support set and query set from D: i) The support set S can be partially or fully made up of M classes but only contain C +1 samples each. ii) We randomly select 1 sample from the C + 1 samples to form the test set (query set). Hence, COVID-19 diagnosis can be represented as a two-way, C-shot learning problem.
3. Methodology
In this section, we will introduce our proposed self-supervised COVID-19 diagnosis method. The overall ﬂowchart is illustrated in Fig. 1. We will describe the three major components of our method which include data augmentation, representation learning and the few-shot classiﬁcation.
3.1. Data Augmentation Data augmentation has been widely used in unsupervised representation
learning and supervised learning (Parkhi et al., 2012; Donahue and Simonyan, 2019; Donahue et al., 2014). A few existing approaches deﬁne the contrastive classiﬁcation task as changing abd image’s structure. For instance, Hjelm et al. (Hjelm et al., 2018) and Bachman et al. (Bachman et al., 2019) used globalto-local view for contrastive learning as shown in the ﬁrst example in Fig 2. Meanwhile, Oord et al. (Oord et al., 2018) and Henaﬀ et al. (H´enaﬀ et al., 2019) achieved neighbor prediction using the adjacency view (middle example, Fig 2). Those two methods can be view on the ﬁrst two images on 2. Besides, we also introduce the over-lapping view which is the third image on Fig 2.
In this study, we apply a stochastic data augmentation T which will randomly transfer a given example image x into two diﬀerent views denoted as
5

Figure 1: The overall architecture of our approach. Top: The pre-training stage, which includes data augmentation and representation learning. The pretext task is an instance discrimination task. Bottom: Few-shot classiﬁcation with 2-way, 1-shot example. For classiﬁcation, the support images and query image are encoded by the pre-trained encoding network. Query sample embeddings are compared with the centroid of training sample embeddings and ﬁne-tuning the pre-trained encoder.
xi, xj. We consider the pair xi, xj as positive. In this study, we apply two simple augmentation strategies in sequence: 1) random cropping, followed by a resizing operation back to the original size with random ﬂipping. 2) random cropping with color distortions followed by a resizing operation. When a new image is fed into the model, one of the above methods to perform is randomly selected for augmentation. This process is repeated twice to generate two diﬀerent views.
Note that we implement color distortions using the torchvision2 package in PyTorch (Paszke et al., 2019).
2https://pytorch.org/docs/stable/torchvision/index.html
6

A

B

D C

F E

Figure 2: Three possibilities for random cropping. Dashed boxes are augmented views. Crops A, C, E views will have random color distortions applied while B, D, F will not change if the method (2) is chosen. All the cropped sections will be resized back to the original input image size. For the instance discrimination task, the goal, given B, D, F,is to, determine whether or not A, C, E are in the same instances.

3.2. Contrastive Visual Embedding Using contrastive learning to learn visual embeddings was ﬁrst explored by
Hadsell et al. (Hadsell et al., 2006). The task can be deﬁned as: Given an image set {I = i1, · · · , ip}, xi ∈ Rd, the goal is to ﬁnd a mapping function G : Rd → Ra, a d to satisfy:

s(G(x), G(x+) s(G(x), G(x−)

(1)

where s(·, ·) is a function used to measure the similarity between two inputs. G is designed for dimension reduction and representation learning. Finally, x+, x− represent the positive and negative samples which means x+ being similar to x and x− dissimilar. It is worth to mentioned that, the contrastive learning is a type of unsupervised learning. A simple framework for the contrastive learning was proposed by Chen et al. (Chen et al., 2020b). The framework learns the representations by maximizing the agreement between diﬀerently augmented views xi, xj of the same data example x via a contrastive loss in the latent space. We adopt this framework in our model. Speciﬁcally,our representation learning stage consists of three modules: encoder, projection head and the contrastive loss function. Encoder. The neural network based encoder f (·) can extract representations from the augmented images. This framework is ﬂexible to adopt any type of

7

network architecture without constraints. In this study, we adopt ResNet (He et al., 2016) to obtain the representation hi, hj, hi = f (xi) = ResNet(xi) where hi is the Rd output of the average pooling layer. Projection Head. The project head g(·) is a function that can map the resulting representation int oapplication space of the contrastive loss. The most common projection head used is the multilayer perceptron (MLP) with one hidden layer (Chen et al., 2020b,c). In this case, we can express the zi (as well as zj) as:

zi = g(hi) = W 2σ(W 1hi)

(2)

where W 1, W 2 are the weights of the hidden layer and output layer respectively.

The σ(·) is the non-liner ReLU activation function which can be deﬁned as:



0 x ≤ 0

ReLU(x) =

(3)

x x > 0

We will examine the eﬀectiveness of this projection head in Section 4.

Contrastive Loss Function. The contrastive loss function is deﬁned for the

contrastive pre-text task. We only consider the instance discrimination task (Wu

et al., 2018) in this study. Given a set {xk} including a positive pair xi, xj, the

contrastive task aims to identify xj in set {xk}k=i for a given xi.

We deﬁne the contrastive task on pairs of augmented images from a randomly

selected minibatch with N samples. The augmentation process results in 2N

data points. To create the contrastive task, we need enough negative samples

to construct the loss function. Similar to Chen et al. (Doersch and Zisserman,

2017), we treat the other 2N − 2 examples as the negative samples. The simi-

larity function s(·, ·) can be deﬁned as the cosine similarity:

vu

s(v, u) =

(4)

vu

where v, u are two vectors. Based on this, we can deﬁne the loss function for a

pair of positive samples (i, j) as:

Li,j = − log

exp(s(zi, zj)/τ )

2N k=1

1k=i

exp(s(zi

,

zk

)/τ

)

(5)

8

Here 1k=i ∈ {0, 1} is the indicator which have value of 1 when k = i and 0 otherwise, and τ is the temperature parameter. This loss is known as the normalized temperature-scaled cross entropy loss (Sohn, 2016; Bachman et al., 2019; Oord et al., 2018). However, Eq.(5) only considers the positive samples and ignores negative samples. This may lead to potential bias. To avoid this, we introduce the momentum mechanism into our model.
Contrastive learning can also be expressed as training an encoder to conduct a dictionary lookup task. Consider an encoded query q and encoded samples xi, ..., xk which are the keys of the dictionary. If the query q is similar to the sample x+ this means there is a match. For the negative samples x−, there is no match in the dictionary. Based on this deﬁnition, He et al. proposed an unsupervised learning-based framework MoCo (He et al., 2019), by adopting contrastive learning.
Based on the above deﬁnition, the goal of contrastive learning is to build a discrete dictionary for high-dimensional continuous inputs. The core of MoCo is maintaining a dictionary with a queue. The beneﬁt of this is that the encoder can reuse the encoded keys from the previous mini-batch. In addition, the dictionary can be much larger than the mini-batch and easy to adjust. As the number of samples that can be included in the dictionary is ﬁxed, once the dictionary is full, it will progressively remove the oldest records. In this way, the consistency of the dictionary can be maintained as the oldest samples are often out-of-date and inconsistent with the new entries. Another approach, called Memory Bank (Wu et al., 2018), tries to store the historical records of the encoded samples. This approach maintains a bank of all the representations of the dataset. The dictionary then randomly samples from the memory bank directly for each mini-batch without back-propagation. However, this method will lead to inconsistency when sampling. To overcome this, back-propagation should be conducted to keep the sampling step up-to-date. A simple solution is to copy the key encoder fk from query encoder fq without the gradient. However, the encoder changes constantly which can lead to a noisy key representation and poor results. The momentum contrast was address to relief this problem, using
9

a diﬀerent method to update the gradient for fk:

θk ←− mθk + (1 − m)θq.

(6)

where θk is the parameter for fk, θq is the parameter for fq and m ∈ [0, 1) is the momentum coeﬃcient. We use the back-propagation to update the parameter

θq and use Eq.(6) to update θk. Beneﬁting from the momentum coeﬃcient, the θk’s update is smoother than θq. Based on the diﬀerent update strategies, the query and key will be encoded by diﬀerent encoders eventually.

Based on the above discussion, we use the dictionary as a queue to allow

the encoder to reuse the previous encoded sample. The loss function for the

pre-trained model can be written as:

exp(q, k+)/τ

L = − log exp(q, k+)/τ + k− exp(q, k−)/τ .

(7)

Diﬀerent from Eq.(5), here we need to consider the queue and the negative cases,

so we slightly modify the loss function to fulﬁl this requirement by introducing the positive examples k+ and negative examples k−, where qk = k+ ∪ k−. In the instance discrimination pre-text task, a positive pair is formed when a query

q and a key k are augmented from the same sample; otherwise, a negative pair

is created. Once the pre-training step is ﬁnished, we extract the pre-trained

encoder f (·) and integrate it into our classiﬁcation module.

3.3. Prototypical Network for Few-Shot Classiﬁcation
Another step in our workﬂow is classiﬁcation. In this stage, meta-learning is applied to ﬁne-tune the pre-trained encoder to ﬁt the class changes required by few-shot learning. Then we use the Prototypical Networks (Snell et al., 2017) to conduct the few-shot classiﬁcation. The prototypical network learns an embedding that maps all inputs into a mean vector c in the latent space to represent each class. The goal of the pre-trained encoder is to similar images close and dissimilar images separate in the latent space. The prototypical network has a similar aim so it to ﬁne-tune our pre-trained encoder. For class m, the centroid

10

embedding features can be written as:

1

cm = |S|

ψ(xd)

(8)

(xd,yM )∈S

where ψ(·) is the embedding function from the prototypical network. As the prototypical network is a metric based learning method, we use the Euclidean

distance to produce the distribution for all classes for a query q.

p(y = m|q) = exp(−d(ψ(q), cm)) .

(9)

m exp(−d(ψ(q), cm )

Eq.(9) is based on the softmax function over the distance between a query set’s

embedding and the features of the class. The loss function for this stage can be

deﬁned as:

Lmeta = d(ψ(q), cm) + log(d(ψ(q), cm ))

(10)

3.4. Training Strategy Algorithm 1 shows the whole pre-training workﬂow of our model.

4. Experiments
4.1. Datasets We evaluate our proposed model using two publicly available annotated
COVID-19 CT image datasets: (1) COVID-19 CT provided by Zhao et al. (Zhao et al., 2020) and (2) a dataset provided by the Italian Society of Medical and Interventional Radiology3, processed by MedSeg4. The proposed model requires proper pre-training. Diﬀerent from existing methods like Self-Trans (He et al., 2020) that use the ImageNet to pre-train the model, we use DeepLesion (Yan et al., 2018) and the Lung Image Database Consortium Image Collection (LIDCIDRI)5 to pre-train our model. DeepLesion contains over 32,000 lung CT images, while LIDC-IDRI has 244,617. Both datasets are public datasets and
3https://www.sirm.org/category/senza-categoria/covid-19/ 4http://medicalsegmentation.com/covid19/ 5https://wiki.cancerimagingarchive.net/display/Public/LIDC-IDRI
11

Algorithm 1: Training algorithm for the pre-training input : Batch size N, τ, fk, fq, g, T , qk
1 for sampled mini-batch {xk}Nk=1 do 2 for k ∈ {1, · · · , N } do

3

Select two data augmentation functions from T : t, t ;

4

x2k−1 = t(xk), x2k−1 = t (xk) ;

5

h2k−1 = fk(x2k−1), h2k = fq(x2k−1) ;

6

z2k−1 = g(h2k−1), z2k = g(h2k) ;

7 end

8 for i ∈ {1, · · · , 2N }, j ∈ {1, · · · , 2N } do

9

Calculate the similarity using Eq.(4) ;

10 end

11 Update fk to minimize Eq.(7);

12 Update fq by Eq.(6) ;

13 enqueue(qk, z2k−1) ;

14 dequeue(qk);

15 end

16 return fk;

focus on lung diseases. We use those two datasets without labels to pre-train the encoder network.
When dividing the support and query sets for classiﬁcation, we divide the dataset at a patient-level instead of CT level to avoid any possible over-ﬁtting. We report the basic statistics for the COVID-19 CT dataset and MegSeg in Table 1. We combine the two datasets for testing. Note that all CT images were resized to 512 × 512 using opencv26.
6https://opencv.org/

12

Table 1: Number of patients and number of CT images available in test datasets.
COVID-19 CT MegSeg

COVID-19

216

43

# of Patients

Non-COVID-19

171

0

COVID-19

349

110

# of CT images

Non-COVID-19

397

0

4.2. Experimental Settings For pre-training, we use the SGD optimizer with a weight decay of 0.0001 and
momentum of 0.9. The momentum update coeﬃcient is 0.999. The mini-batch size is set to 256 in eight GPUs. The initial learning rate is 0.03. The number of epochs is 200, and the learning rate is multiplied by 0.1 after 120 and 160 epochs, as described in (Wu et al., 2018). The encoder is ResNet-50. The twolayer MLP projection head has a 2048-D hidden layer with a ReLU activation function. The weights are initialized using He initialization (He et al., 2015), and the temperature parameter τ is set to 0.07. For the classiﬁcation stage, we follow the default settings of the prototypical net. The experiments were conducted on eight GPUS which includes six NVIDIA TITAN X Pascal GPUs and two NVIDIA TITAN RTX.
4.3. Evaluation and Results We evaluate our approach by using four metrics: i) Accuracy, which mea-
sures the percentage of correctly classiﬁed samples over the whole dataset; ii) Precision, which measures the percentage of true positives (TP) over all predicted positive samples; iii) Recall, used to measure the percentage of TPs over all positive samples; and iv) Area-under-the-curve (AUC) which measures the relation between FPs and TPs. For fair comparison, we train and test all baseline methods on COVID-19 CT and MegSeg dataset directly by using 10-fold cross-validation at a patient-level with cross-entropy loss function. The results are shown in in Table 2. We ﬁnd that two-way, one-shot method achieves very similar result to the ResNet-50 trained on the COVID-19 dataset. In addition,
13

Figure 3: Grad-CAM (Selvaraju et al., 2017) visualizations for learned features from several baseline methods. Top line is the original image set, followed by our methods, our methods with ImageNet pre-train, DenseNet-121 and ResNet-50. It is obviously that our method can learn lung’s features better.
we also provide the visualization of the learned feature on ﬁgure3. As aforementioned, our method is based on few-shot learning. We are interested in how the number of shots aﬀect model’s performance. Based on that, we conduct a few experiments to explore the relationship between the performance and the number of shots. We use the ResNet-50 as the baseline methods. The results
14

Table 2: Comparison Results between the proposed model and other methods. Our method

uses a two-way, one-shot strategy.

Accuracy

Precision

Recall

AUC

ResNet-50 0.873 ± 0.013 0.894 ± 0.012 0.874 ± 0.012 0.935 ± 0.014

ResNet-12 0.834 ± 0.014 0.851 ± 0.013 0.844 ± 0.014 0.901 ± 0.015

DenseNet-121 0.855 ± 0.013 0.867 ± 0.012 0.859 ± 0.012 0.894 ± 0.013

Ours

0.868 ± 0.012 0.883 ± 0.011 0.872 ± 0.012 0.931 ± 0.013

are shown in Table 3.

Table 3: Results for diﬀerent settings. Here, W indicates the number of ways and S the

number of shots. For instance, 2W1S represents the two way, one shot setting.

Accuracy

Precision

Recall

AUC

ResNet-50

0.873 ± 0.013 0.894 ± 0.012 0.874 ± 0.012 0.935 ± 0.014

Ours(2W,1S) 0.868 ± 0.012 0.883 ± 0.011 0.872 ± 0.012 0.931 ± 0.013

Ours(2W,2S) 0.872 ± 0.012 0.890 ± 0.011 0.875 ± 0.012 0.935 ± 0.012

Ours(2W,3S) 0.876 ± 0.012 0.895 ± 0.011 0.878 ± 0.011 0.938 ± 0.12

Ours(2W,4S) 0.881 ± 0.012 0.898 ± 0.011 0.882 ± 0.011 0.942 ± 0.012

It is not hard to ﬁnd that when the number of shots increasing, the performance increasing gradually. Speciﬁcally, when the number of shots is larger than 3, the result of our model is better then that of ResNet-50. These results show indicate that the pre-trained encoder can capture the features from unknown images very well.
4.4. Ablation Study In this section, we conduct the ablation studies to demonstrate the eﬀec-
tiveness of each component. The default setting is two-way, one-shot and the ResNet-50 use the same setting as previous section. In summary, we want to answer the following research questions: 1) What if pre-trained was conducted on ImageNet? 2) How data augmentation and projection head aﬀect the performance? 3)How important of the ﬁne-tuning stage?
15

First, we are interested in the pre-training dataset selection. We thus conduct the experiments again using ImageNet for pre-training on default setting. The results can be found in Table 4. As expected, we ﬁnd that the performance

Table 4: PreAblation study results for pre-training dataset selection. Both models use the

two-way, one-shot setting.

Accuracy

Precision

Recall

AUC

Ours 0.868 ± 0.012 0.883 ± 0.011 0.872 ± 0.012 0.931 ± 0.013

ImageNet 0.732 ± 0.023 0.744 ± 0.021 0.738 ± 0.023 0.870 ± 0.022

is worse when the model is pre-trained by ImageNet. As discussed before, an extra step may be required to conduct transfer learning from common items to lung CT images.
Next, we discover the role of the data augmentation and projection head played in our model. We’ve conducted the experiments on our model without projection head and without augmentation. The result can be found in Table 5. As we can see, the data augmentation have the signiﬁcant eﬀect with the result

Table 5: Data augmentation and projection head dﬀect

Accuracy

Precision

Recall

AUC

Ours 0.868 ± 0.012 0.883 ± 0.011 0.872 ± 0.012 0.931 ± 0.013

No Aug. 0.779 ± 0.021 0.791 ± 0.020 0.780 ± 0.021 0.889 ± 0.022

No Proj. 0.856 ± 0.013 0.875 ± 0.012 0.870 ± 0.013 0.910 ± 0.015

and the projection head can boost the performance. Finally, we want to examine the eﬀect of the ﬁne-tuning process. To investigate this problem, we ﬁrst pretrain the embedding network and make some modiﬁcations with the few-shot classiﬁcation stage by replace the prototypical network with frozen features liner classiﬁer. The frozen features linear classiﬁer means we directly apply the linear classiﬁer into the learned embedding network without any weights update procedure. Result can be found in Table 6.

16

Table 6: Eﬀect of ﬁne-tuning

Accuracy

Precision

Recall

AUC

Ours

0.868 ± 0.012 0.883 ± 0.011 0.872 ± 0.012 0.931 ± 0.013

Linear Classiﬁer 0.788 ± 0.011 0.795 ± 0.010 0.790 ± 0.010 0.892 ± 0.009

5. Conclusion
Nowadays, CT imaging attracts more and more attention as a screening tool for diagnosing COVID-19. It provides a visualization for community to monitor patient’s progression and can help to evaluate the severity of COVID-19 (Shan et al., 2020). However, the lack of the annotated CT scans are the biggest challenge. In this study, we proposed a new deep-learning based method which can be used for automatic screening of COVID-19 with limited samples. And it proved that such method achieves superior performance than ResNet-50 when the number of available samples is larger than three. ResNet is a well-known and widely used supervised learning model on medical image area. As a selfsupervised method which belongs to unsupervised learning ﬁeld, the results are better than ResNet would be remarkable.

References
Ai T, Yang Z, Hou H, Zhan C, Chen C, Lv W, Tao Q, Sun Z, Xia L. Correlation of chest ct and rt-pcr testing in coronavirus disease 2019 (covid-19) in china: a report of 1014 cases. Radiology 2020;:200642.
Akkus Z, Galimzianova A, Hoogi A, Rubin DL, Erickson BJ. Deep learning for brain mri segmentation: state of the art and future directions. Journal of digital imaging 2017;30(4):449–59.
Bachman P, Hjelm RD, Buchwalter W. Learning representations by maximizing mutual information across views. In: Advances in Neural Information Processing Systems. 2019. p. 15509–19.

17

Bernheim A, Mei X, Huang M, Yang Y, Fayad ZA, Zhang N, Diao K, Lin B, Zhu X, Li K, et al. Chest ct ﬁndings in coronavirus disease-19 (covid-19): relationship to duration of infection. Radiology 2020;:200463.
Butt C, Gill J, Chun D, Babu BA. Deep learning system to screen coronavirus disease 2019 pneumonia. Applied Intelligence 2020;:1.
Chen J, Wu L, Zhang J, Zhang L, Gong D, Zhao Y, Hu S, Wang Y, Hu X, Zheng B, et al. Deep learning-based model for detecting 2019 novel coronavirus pneumonia on high-resolution computed tomography: a prospective study. medRxiv 2020a;.
Chen T, Kornblith S, Norouzi M, Hinton G. A simple framework for contrastive learning of visual representations. arXiv preprint arXiv:200205709 2020b;.
Chen X, Fan H, Girshick R, He K. Improved baselines with momentum contrastive learning. arXiv preprint arXiv:200304297 2020c;.
Chen X, Yao L, Zhang Y. Residual attention u-net for automated multi-class segmentation of covid-19 chest ct images. arXiv preprint arXiv:200405645 2020d;.
Doersch C, Zisserman A. Multi-task self-supervised visual learning. In: Proceedings of the IEEE International Conference on Computer Vision. 2017. p. 2051–60.
Donahue J, Jia Y, Vinyals O, Hoﬀman J, Zhang N, Tzeng E, Darrell T. Decaf: A deep convolutional activation feature for generic visual recognition. In: International conference on machine learning. 2014. p. 647–55.
Donahue J, Simonyan K. Large scale adversarial representation learning. In: Advances in Neural Information Processing Systems. 2019. p. 10541–51.
Eaton-Rosen Z, Bragman F, Ourselin S, Cardoso MJ. Improving data augmentation for medical image segmentation 2018;.
18

Gozes O, Frid-Adar M, Greenspan H, Browning PD, Zhang H, Ji W, Bernheim A, Siegel E. Rapid ai development cycle for the coronavirus (covid-19) pandemic: Initial results for automated detection & patient monitoring using deep learning ct image analysis. arXiv preprint arXiv:200305037 2020;.
Hadsell R, Chopra S, LeCun Y. Dimensionality reduction by learning an invariant mapping. In: 2006 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR’06). IEEE; volume 2; 2006. p. 1735– 42.
He K, Fan H, Wu Y, Xie S, Girshick R. Momentum contrast for unsupervised visual representation learning. arXiv preprint arXiv:191105722 2019;.
He K, Zhang X, Ren S, Sun J. Delving deep into rectiﬁers: Surpassing humanlevel performance on imagenet classiﬁcation. In: Proceedings of the IEEE international conference on computer vision. 2015. p. 1026–34.
He K, Zhang X, Ren S, Sun J. Deep residual learning for image recognition. In: Proceedings of the IEEE conference on computer vision and pattern recognition. 2016. p. 770–8.
He X, Yang X, Zhang S, Zhao J, Zhang Y, Xing E, Xie P. Sample-eﬃcient deep learning for covid-19 diagnosis based on ct scans. medRxiv 2020;.
H´enaﬀ OJ, Srinivas A, De Fauw J, Razavi A, Doersch C, Eslami S, Oord Avd. Data-eﬃcient image recognition with contrastive predictive coding. arXiv preprint arXiv:190509272 2019;.
Hjelm RD, Fedorov A, Lavoie-Marchildon S, Grewal K, Bachman P, Trischler A, Bengio Y. Learning deep representations by mutual information estimation and maximization. arXiv preprint arXiv:180806670 2018;.
Huang C, Wang Y, Li X, Ren L, Zhao J, Hu Y, Zhang L, Fan G, Xu J, Gu X, et al. Clinical features of patients infected with 2019 novel coronavirus in wuhan, china. The Lancet 2020;395(10223):497–506.
19

Huang G, Liu Z, Van Der Maaten L, Weinberger KQ. Densely connected convolutional networks. In: Proceedings of the IEEE conference on computer vision and pattern recognition. 2017. p. 4700–8.
Li L, Qin L, Xu Z, Yin Y, Wang X, Kong B, Bai J, Lu Y, Fang Z, Song Q, et al. Artiﬁcial intelligence distinguishes covid-19 from community acquired pneumonia on chest ct. Radiology 2020;:200905.
Li Y, Xia L. Coronavirus disease 2019 (covid-19): Role of chest ct in diagnosis and management. American Journal of Roentgenology 2020;:1–7.
Long C, Xu H, Shen Q, Zhang X, Fan B, Wang C, Zeng B, Li Z, Li X, Li H. Diagnosis of the coronavirus disease (covid-19): rrt-pcr or ct? European Journal of Radiology 2020;:108961.
Mei X, Lee HC, Diao Ky, Huang M, Lin B, Liu C, Xie Z, Ma Y, Robson PM, Chung M, et al. Artiﬁcial intelligence–enabled rapid diagnosis of patients with covid-19. Nature Medicine 2020;:1–5.
Narin A, Kaya C, Pamuk Z. Automatic detection of coronavirus disease (covid-19) using x-ray images and deep convolutional neural networks. arXiv:200310849 2020;.
Oliveira A, Pereira S, Silva CA. Augmenting data when training a cnn for retinal vessel segmentation: How to warp? In: 2017 IEEE 5th Portuguese Meeting on Bioengineering (ENBENG). IEEE; 2017. p. 1–4.
Oord Avd, Li Y, Vinyals O. Representation learning with contrastive predictive coding. arXiv preprint arXiv:180703748 2018;.
Parkhi OM, Vedaldi A, Zisserman A, Jawahar C. Cats and dogs. In: 2012 IEEE conference on computer vision and pattern recognition. IEEE; 2012. p. 3498–505.
Paszke A, Gross S, Massa F, Lerer A, Bradbury J, Chanan G, Killeen T, Lin Z, Gimelshein N, Antiga L, et al. Pytorch: An imperative style, high-
20

performance deep learning library. In: Advances in Neural Information Processing Systems. 2019. p. 8024–35.
Pereira S, Pinto A, Alves V, Silva CA. Brain tumor segmentation using convolutional neural networks in mri images. IEEE transactions on medical imaging 2016;35(5):1240–51.
Salehi S, Abedi A, Balakrishnan S, Gholamrezanezhad A. Coronavirus disease 2019 (COVID-19): a systematic review of imaging ﬁndings in 919 patients. American Journal of Roentgenology 2020;:1–7.
Selvaraju RR, Cogswell M, Das A, Vedantam R, Parikh D, Batra D. Grad-cam: Visual explanations from deep networks via gradient-based localization. In: Proceedings of the IEEE international conference on computer vision. 2017. p. 618–26.
Shan F, Gao Y, Wang J, Shi W, Shi N, Han M, Xue Z, Shen D, Shi Y. Lung infection quantiﬁcation of covid-19 in ct images with deep learning. arXiv:200304655 2020;.
Shi F, Wang J, Shi J, Wu Z, Wang Q, Tang Z, He K, Shi Y, Shen D. Review of artiﬁcial intelligence techniques in imaging data acquisition, segmentation and diagnosis for covid-19. arXiv:200402731 2020a;.
Shi F, Xia L, Shan F, Wu D, Wei Y, Yuan H, Jiang H, Gao Y, Sui H, Shen D. Large-scale screening of covid-19 from community acquired pneumonia using infection size-aware classiﬁcation. arXiv preprint arXiv:200309860 2020b;.
Snell J, Swersky K, Zemel R. Prototypical networks for few-shot learning. In: Advances in neural information processing systems. 2017. p. 4077–87.
Sohn K. Improved deep metric learning with multi-class n-pair loss objective. In: Advances in neural information processing systems. 2016. p. 1857–65.
Vinyals O, Blundell C, Lillicrap T, Wierstra D, et al. Matching networks for one shot learning. In: Advances in neural information processing systems. 2016. p. 3630–8.
21

Wang Ls, Wang Yr, Ye Dw, Liu Qq. A review of the 2019 novel coronavirus (covid-19) based on current evidence. International Journal of Antimicrobial Agents 2020;:105948.
Wu Z, Xiong Y, Yu SX, Lin D. Unsupervised feature learning via non-parametric instance discrimination. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2018. p. 3733–42.
Yan K, Wang X, Lu L, Zhang L, Harrison AP, Bagheri M, Summers RM. Deep lesion graphs in the wild: relationship learning and organization of signiﬁcant radiology image ﬁndings in a diverse large-scale lesion database. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2018. p. 9261–70.
Zhao A, Balakrishnan G, Durand F, Guttag JV, Dalca AV. Data augmentation using learned transformations for one-shot medical image segmentation. In: Proceedings of the IEEE conference on computer vision and pattern recognition. 2019. p. 8543–53.
Zhao J, Zhang Y, He X, Xie P. Covid-ct-dataset: a ct scan dataset about covid-19. arXiv preprint arXiv:200313865 2020;.
Zheng C, Deng X, Fu Q, Zhou Q, Feng J, Ma H, Liu W, Wang X. Deep learningbased detection for covid-19 from chest ct using weak label. medRxiv 2020;.
22

