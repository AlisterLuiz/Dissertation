This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/RBME.2020.2987975, IEEE Reviews in Biomedical Engineering
1
Review of Artificial Intelligence Techniques in Imaging Data Acquisition, Segmentation and
Diagnosis for COVID-19
Feng Shi †, Jun Wang †, Jun Shi †, Ziyan Wu, Qian Wang, Zhenyu Tang, Kelei He, Yinghuan Shi, Dinggang Shen*

 Abstract—The pandemic of coronavirus disease 2019 (COVID-19) is spreading all over the world. Medical imaging such as X-ray and computed tomography (CT) plays an essential role in the global fight against COVID-19, whereas the recently emerging artificial intelligence (AI) technologies further strengthen the power of the imaging tools and help medical specialists. We hereby review the rapid responses in the community of medical imaging (empowered by AI) toward COVID-19. For example, AI-empowered image acquisition can significantly help automate the scanning procedure and also reshape the workflow with minimal contact to patients, providing the best protection to the imaging technicians. Also, AI can improve work efficiency by accurate delineation of infections in X-ray and CT images, facilitating subsequent quantification. Moreover, the computer-aided platforms help radiologists make clinical decisions, i.e., for disease diagnosis, tracking, and prognosis. In this review paper, we thus cover the entire pipeline of medical imaging and analysis techniques involved with COVID-19, including image acquisition, segmentation, diagnosis, and follow-up.
This work was supported in part by the Shanghai Science and Technology Foundation (18010500600, 19QC1400600), National Key Research and Development Program of China (2018YFC0116400), and Natural Science Foundation of Jiangsu Province (BK20181339). (* Corresponding author: Dinggang Shen)
† F. Shi, J. Wang, and J. Shi contributed equally to this work. F. Shi and D. Shen are with the Department of Research and Development, Shanghai United Imaging Intelligence Co., Ltd., Shanghai 200232, China (e-mail: feng.shi@united-imaging.com; Dinggang.Shen@gmail.com). J. Wang and J. Shi are with Key Laboratory of Specialty Fiber Optics and Optical Access Networks, Shanghai Institute for Advanced Communication and Data Science, School of Communication and Information Engineering, Shanghai University, Shanghai, 200444, China (e-mail: wangjun_shu@shu.edu.cn; junshi@shu.edu.cn). Z. Wu is with United Imaging Intelligence, Cambridge, MA 02140, USA (e-mail: ziyan.wu@united-imaging.com). Q. Wang is with the Institute for Medical Imaging Technology, School of Biomedical Engineering, Shanghai Jiao Tong University, Shanghai 200030, China (e-mail: wang.qian@sjtu.edu.cn). Z. Tang is with Beijing Advanced Innovation Center for Big Data and Brain Computing, Beihang University, Beijing 100191, China (e-mail: tangzhenyu@buaa.edu.cn). K. He is with the Medical School of Nanjing University, Nanjing, China. He is also with National Institute of Healthcare Data Science at Nanjing University, Nanjing 210093, China (e-mail: hkl@nju.edu.cn). Y. Shi is with the National Key Laboratory for Novel Software and Technology, Nanjing University, Nanjing, China. He is also with the National Institute of Healthcare Data Science at Nanjing University, Nanjing 210093, China (e-mail: syh@nju.edu.cn).

We particularly focus on the integration of AI with X-ray and CT, both of which are widely used in the frontline hospitals, in order to depict the latest progress of medical imaging and radiology fighting against COVID-19.
Index Terms—COVID-19, artificial intelligence, image acquisition, segmentation, diagnosis
I. INTRODUCTION
THE coronavirus disease 2019 (COVID-19), caused by severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2), is an ongoing pandemic. The number of people infected by the virus is increasing rapidly. Up to April 9, 2020, 1,436,198 cases of COVID-19 have been reported in over 200 countries and territories, resulting in approximately 85,521 deaths (with a fatal rate of 5.95%) [1]. This has led to great public health concern in the international community, as the World Health Organization (WHO) declared the outbreak to be a Public Health Emergency of International Concern (PHEIC) on January 30, 2020 and recognized it as a pandemic on March 11, 2020 [2, 3].
Reverse Transcription-Polymerase Chain Reaction (RT-PCR) test serves as the gold standard of confirming COVID-19 patients [4]. However, the RT-PCR assay tends to be inadequate in many areas that have been severely hit especially during early outbreak of this disease. The lab test also suffers from insufficient sensitivity, such as 71% reported in Fang et al. [5]. This is due to many factors, such as sample preparation and quality control [6]. In clinical practice, easily accessible imaging equipment, such as chest X-ray and thoracic CT, provide huge assistance to clinicians [7-12]. Particularly in China, many cases were identified as suspected of COVID-19, if characteristic manifestations in CT scans were observed [6]. The suspected patients, even without clinical symptoms (e.g., fever and coughing), were also hospitalized or quarantined for further lab tests. Given the current sensitivity of the nucleic acid tests, many suspected patients have to be tested multiple times several days apart before reaching a confident diagnosis. Hence, the imaging findings play a critical role in constraining the viral transmission and also fighting against COVID-19.
The workflow of imaging-based diagnosis for COVID-19, taking thoracic CT as an example, includes three stages in general, i.e., 1) pre-scan preparation, 2) image acquisition, and 3) disease diagnosis. In the pre-scan preparation stage, each subject is instructed and assisted by a technician to pose on the

A©utIhEoErizEed20lic2e0n.sTedhiussaerltimicilteedistofr:eIEeEtEo Xapclcoeres.sDaonwdnldoaodwendlooandS,eapltoenmgbewri0th2,r2i0g2h0tsafto0r7f:5u8ll:0te2xUt TaCndfrodmatIaEEmEinXinpglo,rere. -Ruessetraicntidonasnaaplpylsyi.s

This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/RBME.2020.2987975, IEEE Reviews in Biomedical Engineering

2

patient bed according to a given protocol. In the image acquisition stage, CT images are acquired during a single breath-hold. The scan ranges from the apex to the lung base. Scans are done from the level of the upper thoracic inlet to the inferior level of the costophrenic angle with the optimized parameters set by the radiologist(s), based on the patient’s body shape. From the acquired raw data, CT images are reconstructed and then transmitted through picture archiving and communication systems (PACS) for subsequent reading and diagnosis.
Artificial intelligence (AI), an emerging technology in the field of medical imaging, has contributed actively to fight COVID-19 [13]. Compared to the traditional imaging workflow that heavily relies on human labors, AI enables more safe, accurate and efficient imaging solutions. Recent AI-empowered applications in COVID-19 mainly include the dedicated imaging platform, the lung and infection region segmentation, the clinical assessment and diagnosis, as well as the pioneering basic and clinical research. Moreover, many commercial products have been developed, which successfully integrate AI to combat COVID-19 and clearly demonstrate the capability of the technology. The Medical Imaging Computing Seminar (MICS) 1 , a China’s leading alliance of medical imaging scholars and start-up companies, organized this first online seminar on COVID-19 on February 18, 2020, which attracted more than ten thousands of visits. All the above examples show the tremendous enthusiasm cast by the public for AI-empowered progress in the medical imaging field, especially during the ongoing pandemic.
Due to the importance of AI in all the spectrum of the imaging-based analysis of COVID-19, this review aims to extensively discuss the role of medical imaging, especially empowered by AI, in fighting the COVID-19, which will inspire future practical applications and methodological research. In the following, we first introduce intelligent imaging platforms for COVID-19, and then summarize popular machine learning methods in the imaging workflow, including segmentation, diagnosis and prognosis. Several publicly available datasets are also introduced. Finally, we discuss several open problems and challenges. We expect to provide guidance for researchers and radiologists through this review. Note that we review the most related medical-imaging-based COVID-19 studies up to March 31, 2020.
II. AI-EMPOWERED CONTACTLESS IMAGING WORKFLOWS
Healthcare practitioners are particularly vulnerable concerning the high risk of occupational viral exposure. Imaging specialists and technicians are of high priority, such that any potential contact with the virus could be under control. In addition to the personal protective equipment (PPE), one may consider dedicated imaging facilities and workflows, which are significantly important to reduce the risks and save lives.
1 http://www.mics.net.cn/

A. Conventional Imaging Workflow
Chest X-ray and CT are widely used in the screening and diagnosis of COVID-19 [7-12]. It is important to employ a contactless and automated image acquisition workflow to avoid the severe risks of infection during COVID-19 pandemic. However, the conventional imaging workflow includes inevitable contact between technicians and patients. Especially, in patient positioning, technicians first assist in posing the patient according to a given protocol, such as head-first versus feet-first, and supine versus prone in CT, followed by visually identifying the target body part location on the patient and manually adjusting the relative position and pose between the patient and the X-ray tube. This process puts the technicians in close contact with the patients, which leads to high risks of viral exposure. Thus, a contactless and automated imaging workflow is needed to minimize the contact.
B. AI-Empowered Imaging Workflow
Many modern X-ray and CT systems are equipped with cameras for patient monitoring purposes [14-17]. During the outbreak of COVID-19, those devices facilitate the implementation of a contactless scanning workflow. Technicians can monitor the patient from the control room via a live video stream from the camera. However, from only the overhead view of the camera, it is still challenging for the technician to determine the scanning parameters such as scan range. In this case, AI is able to automate the process [18-26] by identifying the pose and shape of the patient from the data acquired with visual sensors such as RGB, Time-of-Flight (TOF) pressure imaging [27] or thermal (FIR) cameras. Thus, the optimal scanning parameters can be determined.
One typical scanning parameter that can be estimated with AI-empowered visual sensors is the scan range that defines the starting and ending positions of the CT scan. Scan range can be identified by detecting anatomical joints of the subject from the images. Much recent work [28-30] has focused on estimating the 2D [31-36] or 3D keypoint locations [29, 37-40] on the patient body. These keypoint locations usually include major joints such as the neck, shoulders, elbows, ankles, wrists, and knees. Wang et al. [41] have shown that such an automated workflow can significantly improve scanning efficiency and reduce unnecessary radiation exposure. However, such keypoints usually represent only a very sparse sampling of the full 3D mesh [42] in the 3D space (that defines the digital human body).
Other important scanning parameters can be inferred by AI, including ISO-centering. ISO-centering refers to aligning the target body region of the subject, so that the center of the target body region overlaps with the scanner ISO center and thus the overall imaging quality is optimal. Studies have shown that, with better ISO-centering, radiation dosage can be reduced while maintaining similar imaging quality [43]. In order to align the target body region to the ISO center, and given that anatomical keypoints usually represent only a very sparse sampling of the full 3D mesh in the 3D space (defining the digital human body), Georgakis et al. [44] propose to recover human mesh from a single monocular RGB image using a

A©utIhEoErizEed20lic2e0n.sTedhiussaerltimicilteedistofr:eIEeEtEo Xapclcoeres.sDaonwdnldoaodwendlooandS,eapltoenmgbewri0th2,r2i0g2h0tsafto0r7f:5u8ll:0te2xUt TaCndfrodmatIaEEmEinXinpglo,rere. -Ruessetraicntidonasnaaplpylsyi.s

This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/RBME.2020.2987975, IEEE Reviews in Biomedical Engineering

3

parametric human model SMPL [45]. Unlike other related studies [46], they employ a hierarchical kinematic reasoning for each kinematic chain of the patient to iteratively refine the estimation of each anatomical keypoint to improve the system robustness to clutters and partial occlusions around the joints of the patient. Singh et al. [19] present a technique, using depth sensor data, to retrieve a full 3D patient mesh by fitting the depth data to a parametric human mesh model based on anatomical landmarks detected from RGB image. One recent solution proposed by Ren et al. [42] learns a model that can be trained just once and have the capability to be applied across multiple such applications based on dynamic multi-modal inference.
With this framework in application with an RGB-depth input sensor, even if one of the sensor modalities fails, the model above can still perform 3D patient body inference with the remaining data.
C. Applications in COVID-19
During the outbreak of COVID-19, several essential contactless imaging workflows were established[18, 41, 42], from the utilization of monitoring cameras in the scan room [14-16, 28], or on the device [47], to mobile CT platforms [18, 47-50] with better access to patients and flexible installation.
A notable example is an automated scanning workflow based on a mobile CT platform empowered by visual AI technologies [18], as shown in Fig. 1(a). The mobile platform is fully self-contained with an AI-based pre-scan and diagnosis system [47]. It was redesigned into a fully isolated scan room and control room. Each room has its own entrance to avoid any unnecessary interaction between technicians and patients.
After entering the scan room, the patient is instructed, by visual and audio prompts, to pose on the patient bed (Fig. 1(b)). Technicians can observe through the window and also the live video transmitted from the ceiling-mounted AI camera in the scan room, and correct the pose of the patient if necessary (Fig.

1(c)). Once the patient is deemed ready, either by the technician or the motion analysis algorithm, the patient positioning algorithm will automatically recover the 3D pose and fully-reconstructed mesh of the patient from the images captured with the camera [42]. Based on the 3D mesh, both the scan range and the 3D centerline of the target body part of the patient are estimated and converted into control signals and optimized scanning parameters for the technician to verify. If necessary, the technician can make adjustments. Once verified, the patient bed will be automatically aligned to ISO center and moved into CT gantry for scanning. After CT images are acquired, they will be processed and analyzed for screening and diagnosis purposes.
III. AI-AIDED IMAGE SEGMENTATION AND ITS APPLICATIONS
Segmentation is an essential step in image processing and analysis for assessment and quantification of COVID-19. It delineates the regions of interest (ROIs), e.g., lung, lobes, bronchopulmonary segments, and infected regions or lesions, in the chest X-ray or CT images. Segmented regions could be further used to extract handcrafted or self-learned features for diagnosis and other applications. This subsection would summarize the related segmentation works in COVID-19 and their applications.
CT provides high-quality 3D images for detecting COVID-19. To segment ROIs in CT, deep learning methods are widely used. The popular segmentation networks for COVID-19 include classic U-Net [51-56], UNet++ [57, 58], VB-Net [59]. Compared with CT, X-ray is more easily accessible around the world. However, due to the ribs projected onto soft tissues in 2D and thus confounding image contrast, the segmentation of X-ray images is even more challenging. Currently, there is no method developed for segmenting X-ray images for COVID-19. However, Gaal et al. [60] adopt an Attention-U-Net for lung segmentation in X-ray images for pneumonia, and although the research is not specified for

Fig. 1. (a) A mobile CT platform equipped with AI-empowered automated image acquisition workflow; (b) An example image captured by patient monitoring camera of CT system; (c) Positioning and scanning of patient operated remotely by a technician.
A©utIhEoErizEed20lic2e0n.sTedhiussaerltimicilteedistofr:eIEeEtEo Xapclcoeres.sDaonwdnldoaodwendlooandS,eapltoenmgbewri0th2,r2i0g2h0tsafto0r7f:5u8ll:0te2xUt TaCndfrodmatIaEEmEinXinpglo,rere. -Ruessetraicntidonasnaaplpylsyi.s

This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/RBME.2020.2987975, IEEE Reviews in Biomedical Engineering

4

Literature

TABLE I SUMMARY OF IMAGE SEGMENTATION METHODS IN COVID-19 APPLICATIONS

Modality

Method

Target ROI

Application

Highlights

Zheng et al. [51]

CT

Cao et al. [52]

CT

Huang et al. [53]

CT

Qi et al. [54]

CT

Gozes et al. [55]

CT

Li et al. [56]

CT

U-Net
U-Net
U-Net
U-Net U-Net/ Commercial Software U-Net

Lung
Lung Lesion Lung Lung lobes lesion Lung lobes Lesion Lung Leson Lesion

Diagnosis Quantification

Weakly-supervised method by pseudo labels

Quantification

Quantification Diagnosis Diagnosis

Combination of 2D and 3D methods

Chen et al. [57]

CT

UNet++

Lesion

Diagnosis

Jin et al. [58]

CT

UNet++

Lung Lesion

Diagnosis

Joint segmentation and classification

Lung

Shan et al. [59]

CT

VB-Net

Lung lobes Lung segments

Quantification

Human-in-the-loop

Lesion

Lung

Tang et al. [61]

CT

Commercial Software

Lesion Trachea

Quantification

Shen et al. [62]

CT

Threshold-based region growing [63]

Bronchus Lesion

Quantification

COVID-19, the method can be applied to the diagnosis of COVID-19 and other diseases easily.
Although now there are limited segmentation works directly related to COVID-19, many papers consider segmentation as a necessary process in analyzing COVID-19. Table I summarizes representative works involving image segmentation in COVID-19 studies.
A. Segmentation of Lung Regions and Lesions
In terms of target ROIs, the segmentation methods in COVID-19 applications can be mainly grouped into two categories, i.e., the lung-region-oriented methods and the lung-lesion-oriented methods. The lung-region-oriented methods aim to separate lung regions, i.e., whole lung and lung lobes, from other (background) regions in CT or X-ray, which is considered as a pre-requisite step in COVID-19 applications [51-55, 58, 59, 61]. For example, Jin et al. [58] propose a two-stage pipeline for screening COVID-19 in CT images, in which the whole lung region is first detected by an efficient segmentation network based on UNet++. The

lung-lesion-oriented methods aim to separate lesions (or metal and motion artifacts) in the lung from lung regions [52-59, 61, 62]. Because the lesions or nodules could be small with a variety of shapes and textures, locating the regions of the lesions or nodules is required and has often been considered a challenging detection task. Notably, in addition to segmentation, the attention mechanism is reported as an efficient localization method in screening [60], which can be adopted in COVID-19 applications.
B. Segmentation Methods
In the literature, there have been numerous techniques for lung segmentation with different purposes [64-68]. The U-Net is a commonly used technique for segmenting both lung regions and lung lesions in COVID applications [51-54]. The U-Net, a type of fully convolutional network proposed by Ronneberger [69], has a U-shape architecture with symmetric encoding and decoding signal paths. The layers of the same level in two paths are connected by the shortcut connections. In this case, the network can therefore learn better visual semantics as well as

A©utIhEoErizEed20lic2e0n.sTedhiussaerltimicilteedistofr:eIEeEtEo Xapclcoeres.sDaonwdnldoaodwendlooandS,eapltoenmgbewri0th2,r2i0g2h0tsafto0r7f:5u8ll:0te2xUt TaCndfrodmatIaEEmEinXinpglo,rere. -Ruessetraicntidonasnaaplpylsyi.s

This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/RBME.2020.2987975, IEEE Reviews in Biomedical Engineering

5

detailed contextures, which is suitable for medical image segmentation.
Various U-Net and its variants have been developed, achieving reasonable segmentation results in COVID-19 applications. Çiçek et al. [64] propose the 3D U-Net that uses the inter-slice information by replacing the layers in conventional U-Net with a 3D version. Milletari et al. [65] propose the V-Net which utilizes the residual blocks as the basic convolutional block, and optimize the network by a Dice loss. By equipping the convolutional blocks with the so-called bottleneck blocks, Shan et al. [59] use a VB-Net for more efficient segmentation. Zhou et al. [66] propose the UNet++, which is much more complex than U-Net, as the network inserts a nested convolutional structure between the encoding and decoding path. Obviously, this type of network can improve the performance of segmentation. However, it is more difficult to train. This network is also used for locating lesions in COVID-19 diagnosis [57]. Recently advanced attention mechanisms can learn the most discriminant part of the features in the network. Oktay et al. [68] propose an Attention U-Net that is capable of capturing fine structures in medical images, thereby suitable for segmenting lesions and lung nodules in COVID-19 applications.
Training a robust segmentation network requires sufficient labeled data. In COVID-19 image segmentation, adequate training data for segmentation tasks is often unavailable since manual delineation for lesions is labor-intensive and time-consuming. To address this, a straightforward method is to incorporate human knowledge. For example, Shan et al. [59] integrate human-in-the-loop strategy into the training of a VB-net based segmentation network, which involves interactivity with radiologists into the training of the network. Qi et al. [54] delineate the lesions in the lung using U-Net with the initial seeds given by a radiologist. Several other works used diagnostic knowledge and identified the infection regions by the attention mechanism [58]. Weakly-supervised machine learning methods are also used when the training data are insufficient for segmentation. For example, Zheng et al. [51] propose to use an unsupervised method to generate pseudo segmentation masks for the images. As lacking of annotated medical images is common in lung segmentation, unsupervised and semi-supervised methods are highly demanded for COVID-19 studies.
C. Applications in COVID-19
Segmentation can be used in various COVID-19 applications, among which diagnosis is frequently reported [51, 55-58, 70, 71]. For example, Li et al. [56] use U-Net for lung segmentation in a multi-center study for distinguishing COVID-19 from community-acquired pneumonia on Chest CT. Jin et al. propose an AI system for fast COVID-19 diagnosis [58]. The input to the classification model is the CT slices that have been segmented by a segmentation network.
Another application of image segmentation is quantification [52-54, 59, 61, 62], which further serves for many medical applications. For example, Shan et al. [59] propose a VB-Net for segmentation of lung, lung lobes and lung infection, which

provide accurate quantification data for medical studies, including quantitative assessment of progression in the follow-up, comprehensive prediction of severity in the enrollment, and visualization of lesion distribution using percentage of infection (POI). Cao et al. [52] assess longitudinal progression of COVID-19 by using voxel-level deep learning-based CT segmentation of pulmonary opacities. Huang et al. [53] segment lung region and GGO for quantitative evaluation, which is further used for monitoring the progression of COVID-19. Qi et al. segment lung lesions of COVID-19 patients using a U-Net based algorithm, and extract radiomics features for predicting hospital stay [54].
In summary, image segmentation plays an important role in COVID-19 applications, i.e., in lung delineation and lesion measurement. It facilitates radiologists in accurately identification of lung infection and prompting quantitative analysis and diagnosis of COVID-19.
IV. AI-ASSISTED DIFFERENTIAL DIAGNOSIS OF COVID-19
In outbreak areas, patients suspected of COVID-19 are in urgent need of diagnosis and proper treatment. Due to fast acquisition, X-ray and CT scans are widely performed to provide evidences for radiologists. However, medical images, especially chest CT, contain hundreds of slices, which takes a long time for the specialists to diagnose. Also, COVID-19 as a new disease has similar manifestations with various other types of pneumonia, which requires radiologists to accumulate many experiences for achieving a high diagnostic performance. Thus, AI-assisted diagnosis using medical images is highly desired. Segmentation discussed in the previous subsection could be used to preprocess the images, and here we focus on the methods that could take advantage of those segmentation results into the diagnosis. Table II lists the most relevant state-of-the-art studies in this direction.
A. X-ray based Screening of COVID-19
X-ray images are generally considered less sensitive than 3D chest CT images, despite being the typical first-line imaging modality used for patients under investigation of COVID-19. A recent study reported that X-ray shows normal in early or mild disease [72]. In particular, abnormal chest radiographs are found in 69% of the patients at the initial time of admission, and in 80% of the patients sometime after during hospitalization [72].
Radiological signs include airspace opacities, ground-glass opacity (GGO), and later consolidation. Bilateral, peripheral, and lower zone predominant distributions are mostly observed (90%). Pleural effusion is rare (3%) in comparison to parenchymal abnormalities [72].
Classification of COVID-19 from other pneumonia and healthy subjects have been explored. Ghoshal et al. [73] propose a Bayesian Convolutional Neural network to estimate the diagnosis uncertainty in COVID-19 prediction. 70 lung X-ray images of patients with COVID-19 are obtained from an online COVID-19 dataset [74], and non-COVID-19 images are obtained from Kaggle’s Chest X-Ray Images (Pneumonia). The experimental results show that Bayesian inference improves the detection accuracy of the standard VGG16 model

A©utIhEoErizEed20lic2e0n.sTedhiussaerltimicilteedistofr:eIEeEtEo Xapclcoeres.sDaonwdnldoaodwendlooandS,eapltoenmgbewri0th2,r2i0g2h0tsafto0r7f:5u8ll:0te2xUt TaCndfrodmatIaEEmEinXinpglo,rere. -Ruessetraicntidonasnaaplpylsyi.s

This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/RBME.2020.2987975, IEEE Reviews in Biomedical Engineering

6

from 85.7% to 92.9%. The authors further generate saliency maps to illustrate the locations focused by the deep network, to improve the understanding of deep learning results and facilitate a more informed decision-making process.
Narin et al. [10] propose three different deep learning models, i.e., ResNet50, InceptionV3, and Inception-ResNetV2, to detect COVID-19 infection from X-ray images. It is worth noting that the COVID-19 dataset [74] and Kaggle’s Chest X-Ray Images (Pneumonia) are also used to form the dataset in this study. Chest X-ray images of 50 COVID-19 patients and 50 normal chest X-ray images are included. The evaluation results show that the ResNet50 model achieves the highest classification performance with 98.0% accuracy, compared to 97.0% accuracy by InceptionV3 and 87% accuracy by Inception-ResNetV2.
Zhang et al. [75] present a ResNet based model to detect COVID-19 from X-ray images. This model has two tasks, i.e., one task for the classification between COVID-19 and non-COVID-19, and another task for anomaly detection. The anomaly detection task gives an anomaly score to optimize the COVID-19 score used for the classification. X-ray images from 70 COVID-19 patients and 1008 non-COVID-19 pneumonia patients are included from these two datasets. The sensitivity and specificity are 96.0% and 70.7%, respectively, along with an AUC of 0.952.
Also, Wang et al. [12] propose a deep convolutional neural network based model (COVID-Net) to detect COVID-19 cases using X-ray images. Similarly, from these two datasets, the dataset includes 5941 chest X-ray images from 1203 healthy people, 931 patients with bacterial pneumonia, 660 patients with viral pneumonia, and 45 patients with COVID-19. The COVID-Net obtains the testing accuracy of 83.5%.
In general, most current studies use X-ray images to classify between COVID-19 and other pneumonia and healthy subjects. The images are mainly from two online datasets, in which there are only 70 images from COVID-19 patients. With this limited number of COVID-19 images, it is insufficient to evaluate the robustness of the methods and also poses questions to the generalizability with respect to applications in other clinical centers. Also, the severity of subjects remain unknown; the future work could emphasize on early detection of COVID-19.
B. CT-based Screening and Severity Assessment of COVID-19
Dynamic radiological patterns in chest CT images of COVID-19 have been reported and summarized as 4 stages [80]. Briefly, 0-4 days after onset of the initial symptom is considered as the early stage. GGO could be observed subpleurally in the lower lobes unilaterally or bilaterally. The progressive stage is 5-8 days where diffuse GGO, crazy-paving pattern, and even consolidation could be found distributing in bilateral multi-lobes. In the peak stage (9-13 days), dense consolidation becomes more prevalent. When the infection becomes controlled, the absorption stage appears (usually after 14 days). Consolidation and crazy-paving pattern are gradually absorbed and only GGO is left. These radiological patterns provide important evidences for CT-based classification and severity assessment of COVID-19.

1) Classification of COVID-19 from non-COVID-19. There are a number of studies aiming to separate COVID-19 patients from non-COVID-19 subjects (that include common pneumonia subjects and non-pneumonia subjects). Chen et al. [57] predict the final label (COVID-19 or non- COVID-19) based on the appearance of segmented lesions, which is obtained from a UNet++ based segmentation model. They employ chest CT images of 51 COVID-19 patients and 55 patients with other diseases. In an additional dataset including 16 viral pneumonia and 11 non-pneumonia patients, the proposed model could identify all the viral pneumonia patients and 9 of non-pneumonia patients. The reading time of radiologists is shortened by 65% with the help of AI results.
Besides directly reading the segmented imaging information, Zheng et al. [51] employ deep learning method for diagnosis. Briefly, a U-Net model is used for lung segmentation, and the segmentation result is taken as the input of the 3D CNN for predicting the probability of COVID-19. Chest CT images of 540 subjects (i.e., 313 with COVID-19, and 229 without COVID-19) are used as training and testing data. The proposed model achieves a sensitivity of 90.7%, specificity of 91.1%, and AUC of 0.959. Similarly, Jin et al. [58] propose a UNet++ based segmentation model for locating lesions and a ResNet50 based classification model for diagnosis. That study includes chest CT images of 1136 cases (i.e., 723 COVID-19 positives, and 413 COVID-19 negatives). In the experiment, the sensitivity and specificity using the proposed UNet++ and ResNet50 combined model are 97.4% and 92.2%, respectively.
Besides 3D networks, Jin et al. [70] employ a 2D Deeplab v1 model for segmentation the lung and a 2D ResNet152 model for lung-mask slice based identification of positive COVID-19 cases. They use chest CT images from 496 COVID-19 positive cases and 1385 negative cases. Experimental results show that the proposed model achieves sensitivity of 94.1%, specificity of 95.5%, and AUC of 0.979.
2) Classification of COVID-19 from other pneumonia. Given that the common pneumonia especially viral pneumonia has similar radiological appearances with COVID-19, their differentiation would be more useful in facilitating the screening process in clinical practice.
A 2D CNN model is proposed in [76] on manually delineated region patches to classify between COVID-19 and typical viral pneumonia. Chest CT images from 99 patients (i.e., 44 COVID-19 and 55 typical viral pneumonia) are used. The testing dataset shows a total accuracy of 73.1%, along with a specificity of 67.0% and a sensitivity of 74.0%. Xu et al. [77] also use candidate infection regions segmented by a V-Net model, and the region patches are sent to a ResNet-18 network together with handcrafted features of relative infection distance from edge. They use chest CT images from 219 patients with COVID-19, 224 patients with Influenza-A, and 175 healthy persons. The model achieves an overall accuracy of 86.7%.

A©utIhEoErizEed20lic2e0n.sTedhiussaerltimicilteedistofr:eIEeEtEo Xapclcoeres.sDaonwdnldoaodwendlooandS,eapltoenmgbewri0th2,r2i0g2h0tsafto0r7f:5u8ll:0te2xUt TaCndfrodmatIaEEmEinXinpglo,rere. -Ruessetraicntidonasnaaplpylsyi.s

This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/RBME.2020.2987975, IEEE Reviews in Biomedical Engineering

7

TABLE II RELATED STUDIES WITH MEDICAL IMAGES FOR AI-ASSISTED DIAGNOSIS OF COVID-19

Literature

Modality

Subjects

Task

Method

Result

Ghoshal et al. [73]

X-Ray

Narin et al. [10]

X-Ray

Zhang et al. [75]

X-Ray

Wang et al. [12]

X-Ray

Chen et al. [57]

CT

Zheng et al. [51]

CT

Jin et al. [70]

CT

Jin et al. [58]

CT

Wang et al. [76]

CT

Ying et al. [71]

CT

Xu et al. [77]

CT

Li et al. [56]

CT

Shi et al. [78]

CT

70 COVID-19 Others (# of subjects not
available) 50 COVID-19
50 Normal 70 COVID-19 1008 Others 45 COVID-19 931 Bac. Pneu. 660 Vir. Pneu. 1203 Normal 51 COVID-19
55 Others 313 COVID-19
229 Others 496 COVID-19
1385 Others 723 COVID-19
413 Others 44 COVID-19 55 Vir. Pneu. 88 COVID-19 100 Bac. Pneu.
86 Normal 219 COVID-19
224 Influ.-A 175 Normal 468 COVID-19 1551 CAP 1445 Non-pneu. 1658 COVID-19 1027 CAP

Classification: COVID-19/ Others

CNN

Classification: COVID-19/ Normal

ResNet50

Classification: COVID-19/ Others

ResNet

Classification: COVID-19/ Bac. Pneu./ Vir. Pneu./
Normal

CNN

Classification: COVID-19/ Others

UNet++

Classification: COVID-19/ Others

U-Net CNN

Classification: COVID-19/ Others

CNN

Classification: COVID-19/ Others

UNet++ CNN

Classification: COVID-19/ Vir. Pneu.

CNN

Classification: COVID-19/ Bac. Pneu./ Normal

ResNet-50

Classification: COVID-19/ Influ.-A/ Normal

CNN

Classification: COVID-19/ CAP/
Non-pneu.

ResNet-50

Classification: COVID-19/CAP

RF

Tang et al. [79]

CT

176 COVID-19

Severity assessment

RF

Bac. Pneu.: Bacterial pneumonia; Vir. Pneu.: Viral pneumonia; Influ.-A: Influenza-A; Non-pneu.: Non- pneumonia

92.9% (Acc.) 98.0% (Acc.) 96.0% (Sens.) 70.7% (Spec.) 0.952 (AUC)
83.5% (Acc.)
95.2% (Acc.) 100% (Sens.) 93.6% (Spec.) 90.7% (Sens.) 91.1% (Spec.) 0.959 (AUC) 94.1% (Sens.) 95.5% (Spec.) 97.4% (Sens.) 92.2% (Spec.) 82.9% (Acc.)
86.0% (Acc.)
86.7% (Acc.)
90.0% (Sens.) 96.0% (Spec.) 87.9% (Acc.) 90.7% (Sens.) 83.3% (Spec.) 87.5% (Acc.) 93.3% (TPR) 74.5% (TNR)

A©utIhEoErizEed20lic2e0n.sTedhiussaerltimicilteedistofr:eIEeEtEo Xapclcoeres.sDaonwdnldoaodwendlooandS,eapltoenmgbewri0th2,r2i0g2h0tsafto0r7f:5u8ll:0te2xUt TaCndfrodmatIaEEmEinXinpglo,rere. -Ruessetraicntidonasnaaplpylsyi.s

This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/RBME.2020.2987975, IEEE Reviews in Biomedical Engineering

8

Ying et al. [71] use 2D slices including lung regions segmented by OpenCV. 15 slices of complete lungs are derived from each 3D chest CT images, and each 2D slice is used as the input of the proposed deep learning-based CT diagnosis system (called DeepPneumonia). A pretrained ResNet-50 is used and the Feature Pyramid Network (FPN) is added to extract the top-K details from each image. An attention module is coupled to learn the importance of every detail. Chest CT images from 88 patients with COVID-19, 101 patients with bacterial pneumonia, and 86 healthy persons are used. The model achieves results with an accuracy of 86.0% for pneumonia classification (COVID-19 or bacterial pneumonia), and an accuracy of 94.0% for pneumonia diagnosis (COVID-19 or healthy). Similarly, Li et al. [56] preprocess the 2D slices to extract lung regions using U-Net, and a ResNet50 model is followed with shared weights between 2D slices and then combined with max-pooling for diagnosis. A large chest CT dataset, which contains 4356 chest CT images (i.e., 1296 COVID-19, 1735 community-acquired pneumonia, and 1325 non-pneumonia) from 3322 patients are used. Results show a sensitivity of 90%, specificity of 96%, and AUC of 0.96 in identifying COVID-19.
Besides using neural networks for diagnosis, Shi et al. [78] employ a modified random forest. In the preprocessing stage, a 3D VB-Net [59] is adopted to segment the image into the left/right lung, 5 lung lobes, and 18 pulmonary segments. A number of hand-crafted features are calculated and used to train the random forest model. Data include chest CT images of 2685 patients, of which 1658 patients are of COVID-19 and 1027 patients are of community-acquired pneumonia. Experimental results show a sensitivity of 90.7%, specificity of 83.3%, and accuracy of 87.9% of differentiating COVID-19. Also, testing results are grouped based on infection sizes, showing that patients with small infections have low sensitivity to be identified.
3) Severity assessment of COVID-19. Besides early screening, the study of severity assessment is also important for treatment planning. Tang et al. [79] proposed an RF-based model for COVID-19 severity assessment (non-severe or severe). Chest CT images of 176 patients with conformed COVID-19 is used. A deep learning method VB-Net [78] is adopted to divide the lung into anatomical sub-regions (e.g., lobes and segments), based on which infection volumes and ratios of each anatomical sub-region are calculated and used as quantitative features to train a RF model. Results show a true positive rate of 93.3%, true negative rate of 74.5%, and accuracy of 87.5%.
In summary, a variety of studies have been proposed for CT-based COVID-19 diagnosis with generally promising results. In the next step, the research on screening of COVID-19 could facilitate early detection to help with the diagnosis uncertainty of radiologists. Also, the prediction of severity is of great importance that could help the estimation of the ICU event or clinical decision of treatment planning, which warrants more investigation.

V. AI IN FOLLOW-UP STUDIES
With the goal of evaluating the patient’s response and investigating their potential problems after clinical treatment, the follow-up step plays a significant role in COVID-19 treatment. Regarding the long incubation period of COVID-19 and its popular infectivity, to design the procedure of AI-empowered follow-up for COVID-19 is challenging.
As most of the current works focus on the pre-diagnosis of COVID-19, we notice that the works for studying the follow-up for COVID-19 are still very limited. There are only few attempts according to our knowledge. For example, the researchers of Shanghai United Imaging Intelligence (UII) attempt to use the machine learning-based method and visualization techniques to demonstrate the change of the volume size, density, and other clinical related factors in the infection regions of the patient. After that, the clinical report is automatically generated to reflect these changes as a data-driven guidance for clinical specialists to determine the following procedure (Fig. 2). In addition, the team from Perception Vision Company (PVmed) provided another follow-up solution for COVID-19. They tried to build a contrastive model to reflect the change of different CT images of the same patient, by aligning the infection regions and observing the changing trend of these quantitative values. Several other companies and institutes are also developing the follow-up function in their software platforms currently. Subsidiarily, Huang et al. [53] collect and analyze 126 patients by calculating the CT lung opacification percentage. They find the quantification of lung involvement could be employed to reflect the disease progression of COVID-19, which is helpful for the follow-up study.
It is worth noting that clinical specialists are taking their efforts to the diagnosis and treatment of COVID-19. Thus, the works for studying the follow-up of COVID-19 are still in the early stage and remain an open issue. We believe the previous techniques and work developed in segmentation, diagnosis, quantification, and assessment could be used to guide the development of AI-empowered follow-up study for COVID-19.
VI. PUBLIC IMAGING DATASETS FOR COVID-19
Data collection is the first step to develop machine learning methods for COVID-19 applications. Although there exist large public CT or X-ray datasets for lung diseases, both X-ray and CT scans for COVID-19 applications are not widely available at present, which greatly hinders the research and development of AI methods. Recently, several works on COVID-19 data collection have been reported.
Cohen et al. [74] creates COVID-19 Image Data Collection by assembling medical images from websites and publications, and it currently contains 123 frontal view X-rays. The COVID-CT dataset [81] includes 288 CT slices for COVID-19 confirmed cases thus far. It is collected from over 700 preprinted literature on COVID-19 from medRxiv and bioRxiv. The Coronacases Initiative also shares confirmed cases of COVID-19 on the website (https://coronacases.org). Currently,

A©utIhEoErizEed20lic2e0n.sTedhiussaerltimicilteedistofr:eIEeEtEo Xapclcoeres.sDaonwdnldoaodwendlooandS,eapltoenmgbewri0th2,r2i0g2h0tsafto0r7f:5u8ll:0te2xUt TaCndfrodmatIaEEmEinXinpglo,rere. -Ruessetraicntidonasnaaplpylsyi.s

This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/RBME.2020.2987975, IEEE Reviews in Biomedical Engineering
9

Fig. 2. The follow-up measurement for a COVID-19 patient.

it includes 3D CT images of 10 confirmed COVID-19 cases. Also, the COVID-19 CT segmentation dataset (http://medicalsegmentation.com/covid19/) contains 100 axial CT slices from 60 patients with manual segmentations, in the form of JPG images. It is worth noting that the current public datasets still have a very limited number of images for training and testing of AI algorithms, and the quality of datasets is not sufficient.
VII. DISCUSSION AND FUTURE WORK
The application of AI methods on COVID-19 research is just the beginning. As introduced above, attempts have been made to apply AI to the entire pipeline of the imaging-based diagnosis of COVID-19. However, there are still many works to be conducted in the future, as explained one by one in the following paragraphs.
As mentioned, AI-empowered image acquisition workflows have proven to make the scanning procedure not only more efficient, but also effective in protecting medical staffs from COVID-19 infection. Looking ahead, it is expected that more AI-empowered applications will be integrated into the image acquisition workflow, to facilitate better scan quality and reduced radiation dosage consumed by patients. For example, more precise AI-based automated ISO-centering and scan range determination are required to ensure optimal image quality. Moreover, X-ray exposure parameters can be automatically calculated and optimized with AI inferred body region thickness of the patient, ensuring that just the right amount of radiation is used during the scan, which is particularly important for low-dose imaging.
Medical images usually show negative radiological signs in the early stage of the disease, and thus the study of this stage is important to assist with the clinical diagnosis uncertainty. Meanwhile, many current AI studies for segmentation and diagnosis are based on small samples, which may lead to the overfitting of results. To make the results clinically useful, the

quality and number of data need to be further improved. Also, existing studies generally use U-Net for image segmentation and CNN models (i.e., ResNet) for diagnosis. It is worth noting that interpretability has been a core issue for AI application in health care. Recent studies have proposed Explainable Artificial Intelligence (XAI) methods [82, 83] with finer localization map than the conventional class activation mapping (CAM) method to highlight important regions that are closely associated with the predicted results. That may promote the use of AI-assisted diagnosis in clinical practice.
Deep learning has become the dominant approach in fighting against COVID-19. However, the imaging data in COVID-19 applications may have incomplete, inexact and inaccurate labels, which provides a challenge for training an accurate segmentation and diagnostic network. In this way, weakly supervised deep learning methods could be leveraged. Further, manually labeling imaging data is expensive and time-consuming, which also encourages the investigation of self-supervised deep learning [84, 85] and deep transfer learning methods [86]. Also, as deep learning for both segmentation and abnormality classification has been shown to be promising in studies with noisy labels [87], they shall be also included for potential application for COVID-19 diagnosis.
Follow-up is critical in diagnosing COVID-19 and evaluating treatment. Although there are still limited studies, we believe that the methods from other related studies could be borrowed. 1) In the prognosis of other pneumonia diseases, machine learning-based methodology could inspire the follow-up study of COVID-19 [88-91]. 2) The follow-up inside and outside of hospitals could be combined as a long period tracking for the COVID patients. 3) Multidisciplinary integration, i.e., medical imaging [92], natural language processing [93], and oncology and fusion [93], could benefit the overall follow-up procedure of measurement for COVID-19.

A©utIhEoErizEed20lic2e0n.sTedhiussaerltimicilteedistofr:eIEeEtEo Xapclcoeres.sDaonwdnldoaodwendlooandS,eapltoenmgbewri0th2,r2i0g2h0tsafto0r7f:5u8ll:0te2xUt TaCndfrodmatIaEEmEinXinpglo,rere. -Ruessetraicntidonasnaaplpylsyi.s

This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/RBME.2020.2987975, IEEE Reviews in Biomedical Engineering

10

VIII. CONCLUSION The COVID-19 is a disease that has spread all over the world. [17]

Intelligent medical imaging has played an important role in [18]

fighting against COVID-19. This paper discusses how AI

provides safe, accurate and efficient imaging solutions in

COVID-19 applications. The intelligent imaging platforms, clinical diagnosis, and pioneering research are reviewed in [19]

detail, which covers the entire pipeline of AI-empowered

imaging applications in COVID-19. Two imaging modalities, i.e., X-ray and CT, are used to demonstrates the effectiveness of [20]

AI-empowered medical imaging for COVID-19.

It is worth noting that imaging only provides partial

information about patients with COVID-19. Thus, it is [21]

important to combine imaging data with both clinical

manifestations and laboratory examination results to help better

screening, detection and diagnosis of COVID-19. In this case, [22]

we believe AI will demonstrate its natural capability in fusing

information from these multi-source data, for performing

accurate and efficient diagnosis, analysis and follow-up.

[23]

REFERENCES

[24]

[1]

WHO. (April 10, 2020). Coronavirus disease 2019 (COVID-19)

Situation

Report

-

80.

Available:

https://www.who.int/docs/default-source/coronaviruse/situation-re

ports/20200409-sitrep-80-covid-19.pdf?sfvrsn=1b685d64_4

[25]

[2]

WHO. (2020, 30 January, 2020). Statement on the second meeting

of the International Health Regulations (2005) Emergency Committee regarding the outbreak of novel coronavirus

[26]

(2019-nCoV).

[3]

WHO. (2020). WHO Director-General's opening remarks at the

[4]

media briefing on COVID-19. T. Ai, Z. Yang, H. Hou, C. Zhan, C. Chen, W. Lv, et al., "Correlation

[27]

of Chest CT and RT-PCR Testing in Coronavirus Disease 2019

(COVID-19) in China: A Report of 1014 Cases," Radiology, p.

200642, 2020.

[5]

Y. Fang, H. Zhang, J. Xie, M. Lin, L. Ying, P. Pang, et al., [28]

"Sensitivity of chest CT for COVID-19: comparison to RT-PCR,"

Radiology, p. 200432, 2020.

[6]

T. Liang, Handbook of COVID-19 prevention and treatment, 2020.

[7]

J. P. Kanne, "Chest CT findings in 2019 novel coronavirus [29]

(2019-nCoV) infections from Wuhan, China: key points for the

radiologist," Radiology, p. 200241, 2020.

[8]

A. Bernheim, X. Mei, M. Huang, Y. Yang, Z. A. Fayad, N. Zhang, et al., "Chest CT findings in coronavirus disease-19 (COVID-19):

[30]

relationship to duration of infection," Radiology, p. 200463, 2020.

[9]

X. Xie, Z. Zhong, W. Zhao, C. Zheng, F. Wang, and J. J. R. Liu,

"Chest CT for typical 2019-nCoV pneumonia: relationship to negative RT-PCR testing," Radiology, p. 200343, 2020.

[31]

[10]

A. Narin, C. Kaya, and Z. Pamuk, "Automatic detection of

coronavirus disease (COVID-19) using X-ray images and deep convolutional neural networks," arXiv:2003.10849, 2020.

[32]

[11]

I. D. Apostolopoulos and T. Bessiana, "Covid-19: Automatic

detection from X-Ray images utilizing transfer Learning with

convolutional neural networks," arXiv:2003.11617, 2020.

[12]

L. Wang and A. Wong, "COVID-Net: A tailored deep convolutional [33]

neural network design for detection of COVID-19 cases from chest

radiography images," arXiv:2003.09871 2020.

[13]

L. A. Bullock Joseph, Pham Katherine Hoffmann, Lam Cynthia, [34]

Luengo-Oroz Miguel A., "Mapping the landscape of artificial

intelligence applications against COVID-19," arXiv:2003.11336, 2020.

[35]

[14]

J.-H. Lee, D.-i. Kim, and M.-k. Cho, "Computed tomography

apparatus and method of controlling X-ray by using the same," ed: Google Patents, 2017.

[36]

[15]

P. Forthmann and G. Pfleiderer, "Augmented display device for use

[16]

in a medical imaging laboratory," ed: Google Patents, 2019. V. T. Jensen, "Method and system of acquiring images with a

[37]

medical imaging device," ed: Google Patents, 2009.

S. Scheib, "Dosimetric end-to-end verification devices, systems,

and methods," ed: Google Patents, 2019.

United imaging's emergency radiology departments support mobile

cabin hospitals, facilitate 5G remote diagnosis. Available:

https://www.prnewswire.com/news-releases/united-imagings-emer

gency-radiology-departments-support-mobile-cabin-hospitals-facili

tate-5g-remote-diagnosis-301010528.html

V. K. Singh, K. Ma, B. Tamersoy, Y. Chang, A. Wimmer, T. F.

Odonnell, et al., "DARWIN: Deformable patient avatar

representation with deep image network," in Medical Image

Computing and Computer Assisted Intervention, 2017, pp. 497-504.

V. Singh, Y.-J. Chang, K. Ma, M. Wels, G. Soza, and T. Chen,

"Estimating a patient surface model for optimizing the medical

scanning workflow," in International Conference on Medical Image

Computing and Computer-Assisted Intervention, 2014, pp. 472-479.

Siemens CT scanner SOMATOM Force, SOMATOM Drive or

SOMATOM

Edge

Plus.

Available:

https://www.siemens-healthineers.com/computed-tomography/tech

nologies-and-innovations/fast-integrated-workflow

J. Li, U. K. Udayasankar, T. L. Toth, J. Seamans, W. C. Small, and

M. K. Kalra, "Automatic patient centering for MDCT: effect on

radiation dose," American Journal of Roentgenology, vol. 188, pp.

547-552, 2007.

C. J. B. i. Martin and i. journal, "Optimisation in general

radiography," Biomedical Imaging and Intervention Journal, vol. 3,

2007.

F. Achilles, A. E. Ichim, H. Coskun, F. Tombari, S. Noachtar, and N.

Navab, "Patient MoCap: Human pose estimation under blanket

occlusion for hospital monitoring applications," in Medical Image

Computing and Computer Assisted Intervention, 2016, pp. 491-499.

GE

Xtream

camera.

Available:

https://www.gehealthcare.com/products/computed-tomography/rev

olution-maxima

FAST

integrated

workflow.

Available:

https://new.siemens.com/global/en/company/stories/research-techn

ologies/artificial-intelligence/artificial-intelligence-imaging-techni

ques.html

L. Casas, N. Navab, and S. Demirci, "Patient 3D body pose

estimation from pressure imaging," International Journal of

Computer Assisted Radiology and Surgery, vol. 14, pp. 517-524,

2019.

Z. Cao, T. Simon, S.-E. Wei, and Y. Sheikh, "Realtime multi-person

2d pose estimation using part affinity fields," in Proceedings of the

IEEE Conference on Computer Vision and Pattern Recognition,

2017, pp. 7291-7299.

W. Yang, W. Ouyang, X. Wang, J. Ren, H. Li, and X. Wang, "3D

human pose estimation in the wild by adversarial learning," in

Proceedings of the IEEE Conference on Computer Vision and

Pattern Recognition, 2018, pp. 5255-5264.

S. Liu and S. Ostadabbas, "Seeing under the cover: A physics

guided learning approach for in-bed pose estimation," in

International Conference on Medical Image Computing and

Computer-Assisted Intervention, 2019, pp. 236-245.

A. Toshev and C. Szegedy, "Deeppose: Human pose estimation via

deep neural networks," in Proceedings of the IEEE Conference on

Computer Vision and Pattern Recognition, 2014, pp. 1653-1660.

K. Sun, B. Xiao, D. Liu, and J. Wang, "Deep high-resolution

representation learning for human pose estimation," in Proceedings

of the IEEE Conference on Computer Vision and Pattern

Recognition, 2019, pp. 5693-5703.

W. Tang, P. Yu, and Y. Wu, "Deeply learned compositional models

for human pose estimation," in Proceedings of the European

Conference on Computer Vision (ECCV), 2018, pp. 190-206.

B. Xiao, H. Wu, and Y. Wei, "Simple baselines for human pose

estimation and tracking," in Proceedings of the European

conference on computer vision (ECCV), 2018, pp. 466-481.

Y. Luo, J. Ren, Z. Wang, W. Sun, J. Pan, J. Liu, et al., "LSTM pose

machines," in Proceedings of the IEEE Conference on Computer

Vision and Pattern Recognition, 2018, pp. 5207-5215.

X. Nie, J. Feng, Y. Zuo, and S. Yan, "Human pose estimation with

parsing induced learner," in Proceedings of the IEEE Conference on

Computer Vision and Pattern Recognition, 2018, pp. 2100-2108.

G. Varol, D. Ceylan, B. Russell, J. Yang, E. Yumer, I. Laptev, et al.,

"Bodynet: Volumetric inference of 3d human body shapes," in

A©utIhEoErizEed20lic2e0n.sTedhiussaerltimicilteedistofr:eIEeEtEo Xapclcoeres.sDaonwdnldoaodwendlooandS,eapltoenmgbewri0th2,r2i0g2h0tsafto0r7f:5u8ll:0te2xUt TaCndfrodmatIaEEmEinXinpglo,rere. -Ruessetraicntidonasnaaplpylsyi.s

This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/RBME.2020.2987975, IEEE Reviews in Biomedical Engineering

11

Proceedings of the European Conference on Computer Vision

pneumonia on high-resolution computed tomography: a prospective

(ECCV), 2018, pp. 20-36.

study," MedRxiv, 2020.

[38]

H. Rhodin, M. Salzmann, and P. Fua, "Unsupervised [58]

S. Jin, B. Wang, H. Xu, C. Luo, L. Wei, W. Zhao, et al., "AI-assisted

geometry-aware representation for 3d human pose estimation," in

CT imaging analysis for COVID-19 screening: Building and

Proceedings of the European Conference on Computer Vision

deploying a medical AI system in four weeks," MedRxiv, 2020.

(ECCV), 2018, pp. 750-767.

[59]

F. Shan, Y. Gao, J. Wang, W. Shi, N. Shi, M. Han, et al., "Lung

[39]

G. Pavlakos, L. Zhu, X. Zhou, and K. Daniilidis, "Learning to

infection quantification of COVID-19 in CT images with deep

estimate 3D human pose and shape from a single color image," in

learning," arXiv:2003.04655, 2020.

Proceedings of the IEEE Conference on Computer Vision and [60]

G. Gaál, B. Maga, and A. Lukács, "Attention U-Net based

Pattern Recognition, 2018, pp. 459-468.

adversarial architectures for chest X-ray lung segmentation,"

[40]

V. Srivastav, T. Issenhuth, A. Kadkhodamohammadi, M. de

arXiv:2003.10304, 2020.

Mathelin, A. Gangi, and N. J. a. p. a. Padoy, "MVOR: A multi-view [61]

L. Tang, X. Zhang, Y. Wang, and X. Zeng, "Severe COVID-19

RGB-D operating room dataset for 2D and 3D human pose

Pneumonia: Assessing inflammation burden with Volume-rendered

estimation," arXiv:1808.08180, 2018.

Chest CT," Radiology: Cardiothoracic Imaging, vol. 2, p. e200044,

[41]

Y. Wang, X. Lu, J. Liu, X. Li, R. Hu, X. Meng, et al., "Precise

2020.

pulmonary scanning and reducing medical radiation exposure by [62]

C. Shen, N. Yu, S. Cai, J. Zhou, J. Sheng, K. Liu, et al.,

developing a clinically applicable intelligent CT system: Towards

"Quantitative computed tomography analysis for stratifying the

improving patient care," Preprints with The EBioMedicine, 2020.

severity of Coronavirus Disease 2019," Journal of Pharmaceutical

[42]

R. Li, C. Cai, G. Georgakis, S. Karanam, T. Chen, and Z. Wu,

Analysis, 2020.

"Towards robust RGB-D human mesh recovery," arXiv:1911.07383, [63]

B. C. Lassen, C. Jacobs, J. M. Kuhnigk, B. Van Ginneken, and E. M.

2019.

Van Rikxoort, "Robust semi-automatic segmentation of pulmonary

[43]

R. Booij, R. P. Budde, M. L. Dijkshoorn, and M. van Straten,

subsolid nodules in chest computed tomography scans," Physics in

"Accuracy of automated patient positioning in CT using a 3D

Medicine & Biology, vol. 60, pp. 1307-1323.

camera for body contour detection," European Radiology, vol. 29, [64]

Ö. Çiçek, A. Abdulkadir, S. S. Lienkamp, T. Brox, and O.

pp. 2079-2088, 2019.

Ronneberger, "3D U-Net: learning dense volumetric segmentation

[44]

G. Georgakis, R. Li, S. Karanam, T. Chen, J. Kosecka, and Z. Wu,

from sparse annotation," in International Conference on Medical

"Hierarchical hinematic human mesh recovery," arXiv:2003.04232,

Image Computing and Computer-Assisted Intervention, 2016, pp.

2020.

424-432.

[45]

M. Loper, N. Mahmood, J. Romero, G. Pons-Moll, and M. Black, [65]

F. Milletari, N. Navab, and S.-A. Ahmadi, "V-net: Fully

"SMPL: A skinned multi-person linear model," ACM Transactions

convolutional neural networks for volumetric medical image

on Graphics, vol. 34, p. 16, 2015.

segmentation," in 2016 Fourth International Conference on 3D

[46]

A. Kanazawa, M. J. Black, D. W. Jacobs, and J. Malik, "End-to-end

Vision (3DV), 2016, pp. 565-571.

recovery of human shape and pose," in Proceedings of the IEEE [66]

Z. Zhou, M. M. R. Siddiquee, N. Tajbakhsh, and J. Liang, "UNet++:

Conference on Computer Vision and Pattern Recognition, 2018, pp.

A nested U-Net architecture for medical image segmentation," in

7122-7131.

Deep Learning in Medical Image Analysis and Multimodal

[47]

United imaging sends out more than 100 CT scanners and X-ray

Learning for Clinical Decision Support, ed: Springer, 2018, pp.

machines to aid diagnosis of the coronavirus. Available:

3-11.

https://www.itnonline.com/content/united-imaging-sends-out-more [67]

F. Isensee, J. Petersen, A. Klein, D. Zimmerer, P. F. Jaeger, S. Kohl,

-100-ct-scanners-and-x-ray-machines-aid-diagnosis-coronavirus

et al., "nnU-Net: Self-adapting framework for U-Net-based medical

[48]

(2020, 3rd, April). United imaging aids fight against coronavirus.

image segmentation," arXiv:1809.10486, 2018.

Available:

[68]

O. Oktay, J. Schlemper, L. L. Folgoc, M. Lee, M. Heinrich, K.

https://www.auntminnie.com/index.aspx?sec=log&itemID=128062

Misawa, et al., "Attention U-Net: Learning where to look for the

[49]

CIMC delivers mobile CT scan cabin to Huangzhou General

pancreas," arXiv:1804.03999, 2018.

Hospital

to

diagnose

coronavirus.

Available: [69]

O. Ronneberger, P. Fischer, and T. Brox, "U-Net: Convolutional

https://www.hhmglobal.com/industry-updates/press-releases/cimc-

networks for biomedical image segmentation," in International

delivers-mobile-ct-scan-cabin-to-huangzhou-general-hospital-to-di

Conference on Medical Image Computing and Computer-Assisted

agnose-coronavirus

Intervention, 2015, pp. 234-241.

[50]

Prehospital CT scans possible with mobile stroke unit Available: [70]

C. Jin, W. Cheny, Y. Cao, Z. Xu, X. Zhang, L. Deng, et al.,

https://www.ems1.com/ems-products/ambulances/articles/prehospi

"Development and evaluation of an AI system for COVID-19

tal-ct-scans-possible-with-mobile-stroke-unit-4JKu37U2neG4k68j/

diagnosis," MedRxiv, 2020.

[51]

C. Zheng, X. Deng, Q. Fu, Q. Zhou, J. Feng, H. Ma, et al., "Deep [71]

S. Ying, S. Zheng, L. Li, X. Zhang, X. Zhang, Z. Huang, et al.,

learning-based detection for COVID-19 from chest CT using weak

"Deep learning enables accurate diagnosis of novel Coronavirus

label," MedRxiv, 2020.

(COVID-19) with CT images.," MedRxiv, 2020.

[52]

Y. Cao, Z. Xu, J. Feng, C. Jin, X. Han, H. Wu, et al., "Longitudinal [72]

H. Y. F. Wong, H. Y. S. Lam, A. H.-T. Fong, S. T. Leung, T. W.-Y.

assessment of COVID-19 using a deep learning–based quantitative

Chin, C. S. Y. Lo, et al., "Frequency and distribution of chest

CT pipeline: Illustration of two cases," Radiology: Cardiothoracic

radiographic findings in COVID-19 positive patients," Radiology, p.

Imaging, vol. 2, p. e200082, 2020.

201160, 2020.

[53]

L. Huang, R. Han, T. Ai, P. Yu, H. Kang, Q. Tao, et al., "Serial [73]

B. Ghoshal and A. Tucker, "Estimating uncertainty and

quantitative chest CT assessment of COVID-19: Deep-Learning

interpretability in deep learning for coronavirus (COVID-19)

Approach," Radiology: Cardiothoracic Imaging, vol. 2, p. e200075,

detection," arXiv:2003.10769, 2020.

2020.

[74]

J. P. Cohen, P. Morrison, and L. Dao, "COVID-19 image data

[54]

X. Qi, Z. Jiang, Q. Yu, C. Shao, H. Zhang, H. Yue, et al., "Machine

collection," arXiv 2003.11597, 2020.

learning-based CT radiomics model for predicting hospital stay in [75]

J. Zhang, Y. Xie, Y. Li, C. Shen, and Y. Xia, "COVID-19 screening

patients with pneumonia associated with SARS-CoV-2 infection: A

on Chest X-ray images using deep learning based anomaly

multicenter study," MedRxiv, 2020.

detection," arXiv:2003.12338, 2020.

[55]

O. Gozes, M. Frid-Adar, H. Greenspan, P. D. Browning, H. Zhang, [76]

S. Wang, B. Kang, J. Ma, X. Zeng, M. Xiao, J. Guo, et al., "A deep

W. Ji, et al., "Rapid AI development cycle for the coronavirus

learning algorithm using CT images to screen for Corona Virus

(covid-19) pandemic: Initial results for automated detection &

Disease (COVID-19)," MedRxiv, 2020.

patient monitoring using deep learning ct image analysis," [77]

X. Xu, X. Jiang, C. Ma, P. Du, X. Li, S. Lv, et al., "Deep learning

arXiv:2003.05037, 2020.

system to screen Coronavirus disease 2019 pneumonia,"

[56]

L. Li, L. Qin, Z. Xu, Y. Yin, X. Wang, B. Kong, et al., "Artificial

arXiv:2002.09334, 2020.

intelligence distinguishes COVID-19 from community acquired [78]

F. Shi, L. Xia, F. Shan, D. Wu, Y. Wei, H. Yuan, et al., "Large-scale

pneumonia on chest CT," Radiology, p. 200905, 2020.

screening of COVID-19 from community acquired pneumonia

[57]

J. Chen, L. Wu, J. Zhang, L. Zhang, D. Gong, Y. Zhao, et al., "Deep

using infection size-aware classification," arXiv:2003.09860, 2020.

learning-based model for detecting 2019 novel coronavirus [79]

Z. Tang, W. Zhao, X. Xie, Z. Zhong, F. Shi, J. Liu, et al., "Severity

A©utIhEoErizEed20lic2e0n.sTedhiussaerltimicilteedistofr:eIEeEtEo Xapclcoeres.sDaonwdnldoaodwendlooandS,eapltoenmgbewri0th2,r2i0g2h0tsafto0r7f:5u8ll:0te2xUt TaCndfrodmatIaEEmEinXinpglo,rere. -Ruessetraicntidonasnaaplpylsyi.s

This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/RBME.2020.2987975, IEEE Reviews in Biomedical Engineering

12

assessment of coronavirus disease 2019 (COVID-19) using

quantitative features from chest CT images," arXiv:2003.11988,

2020.

[80]

F. Pan, T. Ye, P. Sun, S. Gui, B. Liang, L. Li, et al., "Time course of

lung changes on chest CT during recovery from 2019 novel

coronavirus (COVID-19) pneumonia," Radiology, p. 200370, 2020.

[81]

J. Zhao, Y. Zhang, X. He, and P. Xie, "COVID-CT-Dataset: A CT

scan dataset about COVID-19," 2020.

[82]

A. B. Arrieta, N. Díaz-Rodríguez, J. Del Ser, A. Bennetot, S. Tabik,

A. Barbado, et al., "Explainable Artificial Intelligence (XAI):

Concepts, taxonomies, opportunities and challenges toward

responsible AI," Information Fusion, vol. 58, pp. 82-115, 2020.

[83]

J.-M. Fellous, G. Sapiro, A. Rossi, H. S. Mayberg, and M. Ferrante,

"Explainable artificial intelligence for neuroscience: behavioral

neurostimulation," Frontiers in Neuroscience, vol. 13, p. 1346,

2019.

[84]

M. Noroozi, A. Vinjimoor, P. Favaro, and H. Pirsiavash, "Boosting

self-supervised learning via knowledge transfer," in Proceedings of

the IEEE Conference on Computer Vision and Pattern Recognition,

2018, pp. 9359-9367.

[85]

C. Doersch, A. Gupta, and A. A. Efros, "Unsupervised visual

representation learning by context prediction," in Proceedings of the

IEEE International Conference on Computer Vision, 2015, pp.

1422-1430.

[86]

C. Tan, F. Sun, T. Kong, W. Zhang, C. Yang, and C. Liu, "A survey

on deep transfer learning," in International Conference on Artificial

Neural Networks, 2018, pp. 270-279.

[87]

X. Ouyang, Z. Xue, Y. Zhan, X. S. Zhou, Q. Wang, Y. Zhou, et al.,

"Weakly Supervised Segmentation Framework with Uncertainty: A

Study on Pneumothorax Segmentation in Chest X-ray," in

International Conference on Medical Image Computing and

Computer-Assisted Intervention, 2019, pp. 613-621.

[88]

Y. Xu, A. Hosny, R. Zeleznik, C. Parmar, T. P. Coroller, I. Franco, et

al., "Deep learning predicts lung cancer treatment response from

serial medical imaging," Clinical Cancer Research, vol. 25, pp.

3266-3275, 2019.

[89]

K. Kourou, T. P. Exarchos, K. P. Exarchos, M. V. Karamouzis, and

D. I. Fotiadis, "Machine learning applications in cancer prognosis

and prediction," Computational and Structural Biotechnology

Journal, vol. 13, pp. 8-17, 2015.

[90]

D. W. Kim, S. H. Lee, S. Kwon, W. Nam, I. Cha, and H. J. Kim,

"Deep learning-based survival prediction of oral cancer patients,"

Scientific Reports, vol. 9, p. 6994, 2019.

[91]

J. Hao, Y. Kim, T. Mallavarapu, J. H. Oh, and M. Kang,

"Interpretable deep neural network for cancer survival analysis by

integrating genomic and clinical data," BMC Medical Genomics,

vol. 12, pp. 1-13, 2019.

[92]

X. Wang, Y. Peng, L. Lu, Z. Lu, and R. M. Summers, "TieNet:

Text-image embedding network for common thorax disease

classification and reporting in chest X-rays," in Proceedings of the

IEEE Conference on Computer Vision and Pattern Recognition,

2018, pp. 9049-9058.

[93]

J. Yuan, H. Liao, R. Luo, and J. Luo, "Automatic radiology report

generation based on multi-view image fusion and medical concept

enrichment," in International Conference on Medical Image

Computing and Computer-Assisted Intervention, 2019, pp. 721-729.

Feng Shi received his Ph.D. degree from Institute of Automation, Chinese Academy of Sciences. He has been an Assistant Professor at the University of North Carolina at Chapel Hill, NC, and Cedar Sinai Medical Center at Los Angeles, CA. He is currently a Research Scientist in Shanghai United Imaging Intelligence, China. His research interests are the projects that involve image processing and artificial intelligence techniques to develop computer-assisted clinical decision support systems.

Jun Wang received his Ph.D. degree from Nanjing University of Science and Technology, Nanjing (NUST), China, in 2011. He has been a Research Assistant in the Hong Kong Polytechnic University, China, and a postdoc research fellow in the University of North Carolina at Chapel Hill, USA, respectively. He is currently an Associate Professor with Shanghai Institute for Advanced Communication and Data Science, School of Communication and Information Engineering, Shanghai University, Shanghai, China. He has published more than 50 articles in international/national journals. His research interests include machine learning and medical image analysis.
Jun Shi received the B.S. degree and the Ph.D. degree from the Department of Electronic Engineering and Information Science, University of Science and Technology of China in 2000 and 2005, respectively. In 2005, he joined the School of Communication and Information Engineering, Shanghai University, China, where he has been a Professor since 2015. From 2011 to 2012, he was a visiting scholar with the University of North Carolina at Chapel Hill, USA. His current research interests include machine learning in medical imaging.
Ziyan Wu is a Principal Expert Scientist of Vision and Robotics at United Imaging Intelligence, Cambridge, MA. He received a Ph.D. degree in Computer and Systems Engineering from Rensselaer Polytechnic Institute, Troy in 2014. He received a B.S. degree and an M.S. degree in Measurement Technology and Instruments, both from Beihang University in China in 2006 and 2009. He was affiliated with the DHS Center of Excellence on Explosives Detection, Mitigation and Response (ALERT). His research interests include 3D object recognition and pose estimation, explainable AI, visual perception in medical environments, scene understanding, and video surveillance.
Qian Wang is an Associate Professor in Institute for Medical Imaging Technology (IMIT), School of Biomedical Engineering, Shanghai Jiao Tong University (SJTU). He received the Ph.D. degree in computer science from the University of North Carolina at Chapel Hill (UNC) in 2013. His research interests cover medical image registration, segmentation, synthesis and many translational medical studies. He has published more than 120 peer-reviewed papers in the field of medical image computing.

A©utIhEoErizEed20lic2e0n.sTedhiussaerltimicilteedistofr:eIEeEtEo Xapclcoeres.sDaonwdnldoaodwendlooandS,eapltoenmgbewri0th2,r2i0g2h0tsafto0r7f:5u8ll:0te2xUt TaCndfrodmatIaEEmEinXinpglo,rere. -Ruessetraicntidonasnaaplpylsyi.s

This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/RBME.2020.2987975, IEEE Reviews in Biomedical Engineering
13
Zhenyu Tang is an Associate Professor in Beijing Advanced Innovation Center for Big Data and Brain Computing, Beihang University. He received his Ph.D. degree in Computer Engineering from the University of Duisburg-Essen in 2011, and he worked as post-doc in the University of North Carolina at Chapel Hill and Automation institute of Chinese Academy of Science, respectively. His research interests include medical image analysis, computer vision, and pattern recognition.
Kelei He received the Ph.D. degree in computer science and technology from Nanjing University, China. He is currently an Assistant Professor with Medical School of Nanjing University, China. He has also served as the core member of the National Institute of Healthcare Data Science at Nanjing University, China. His research interests include medical image analysis, computer vision and deep learning.
Yinghuan Shi is currently an Associate Professor in the Department of Computer Science and Technology of Nanjing University, China. He received his Ph.D. and B.Sc. degrees from Department of Computer Science of Nanjing University in 2013 and 2007, respectively. His research interests include machine learning, computer vision and medical image analysis. He has published more than 60 research papers in related journals and conferences.
Dinggang Shen, Professor, IEEE Fellow, AIMBE Fellow, IAPR Fellow. His research interests include medical image analysis, computer vision, and pattern recognition. He has published more than 1000 papers in the international journals and conference proceedings, with H-index 98. He serves as an editorial board member for eight international journals, and was General Chair for MICCAI 2019.
A©utIhEoErizEed20lic2e0n.sTedhiussaerltimicilteedistofr:eIEeEtEo Xapclcoeres.sDaonwdnldoaodwendlooandS,eapltoenmgbewri0th2,r2i0g2h0tsafto0r7f:5u8ll:0te2xUt TaCndfrodmatIaEEmEinXinpglo,rere. -Ruessetraicntidonasnaaplpylsyi.s

