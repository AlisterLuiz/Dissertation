
@online{OWD2020,
  title   = {Daily COVID-19 Tests},
  url     = {https://ourworldindata.org/grapher/full-list-covid-19-tests-per-day?time=2020-02-20..latest},
  journal = {Our World in Data},
  year    = 2020,
  urldate = {2020-09-20}
}

@online{CDC2020, title={About COVID-19}, url={https://www.cdc.gov/coronavirus/2019-ncov/cdcresponse/about-COVID-19.html}, journal={Centers for Disease Control and Prevention}, publisher={Centers for Disease Control and Prevention}, author={CDC}, year=2020}

@online{CDCa2020, title={Social Distancing}, url={https://www.cdc.gov/coronavirus/2019-ncov/prevent-getting-sick/social-distancing.html}, journal={Centers for Disease Control and Prevention}, publisher={Centers for Disease Control and Prevention}, author={CDC}, year=2020}

@online{WHO2020,
  title   = {Coronavirus},
  url     = {https://www.who.int/health-topics/coronavirus},
  journal = {World Health Organization},
  author  = {{World Health Organization}},
  year    = 2020,
  urldate = {2020-09-20}
}

@online{WMC2020,
  author  = {{Wuhan Municipal Health Commission}},
  title   = {Wellness Commission on the current pneumonia outbreak in our city.},
  year    = 2020,
  url     = {https://web.archive.org/web/20200109215413/http://wjw.wuhan.gov.cn/front/web/showDetail/2019123108989},
  urldate = {2020-09-28}
}

@book{WHOa2020,
  series      = {Novel Coronavirus (2019-nCoV)},
  title       = {Situation Report - 1},
  url         = {https://www.who.int/docs/default-source/coronaviruse/situation-reports/20200121-sitrep-1-2019-ncov.pdf?sfvrsn=20a99c10_4},
  institution = {World Health Organization},
  author      = {{World Health Organization}},
  year        = 2020,
  collection  = {Novel Coronavirus (2019-nCoV)}
}


@online{EDW2020,
  title     = {Coronavirus U.S. case confirmation: 1st coronavirus case is in Seattle},
  url       = {https://www.nbcnews.com/health/health-news/1st-case-coronavirus-china-confirmed-u-s-n1119486},
  journal   = {NBCNews.com},
  publisher = {NBCUniversal News Group},
  author    = {Edwards, Erika},
  year      = {2020},
  month     = {Jan},
  urldate   = {2020-09-28}
}

@online{EDWa2020,
  title     = {New coronavirus outbreak: WHO declares it a global public health emergency},
  url       = {https://www.nbcnews.com/health/health-news/new-coronavirus-outbreak-declared-global-public-health-emergency-who-says-n1126836},
  journal   = {NBCNews.com},
  publisher = {NBCUniversal News Group},
  author    = {Edwards, Erika},
  year      = {2020},
  month     = {Jan}
}


@online{MCM2020,
  title     = {Coronavirus timeline: Tracking the critical moments of COVID-19},
  url       = {https://www.nbcnews.com/health/health-news/coronavirus-timeline-tracking-critical-moments-covid-19-n1154341},
  journal   = {NBCNews.com},
  publisher = {NBCUniversal News Group},
  author    = {Muccari, Robin and Chow, Denise and Murphy, Joe},
  year      = {2020},
  month     = {Jul},
  urldate   = {2020-09-28}
}


@online{WBY2020,
  title     = {Dow closes down 1,000 points as coronavirus fears slam Wall Street},
  url       = {https://www.nbcnews.com/business/markets/dow-plunges-950-points-fears-coronavirus-will-tank-global-economic-n1141546},
  journal   = {NBCNews.com},
  publisher = {NBCUniversal News Group},
  author    = {White, Martha C. and Bayly, Lucy},
  year      = {2020},
  month     = {Feb},
  urldate   = {2020-09-28}
}

@online{SHA2020,
  title     = {Senate passes \$8.3 billion emergency bill to combat coronavirus},
  url       = {https://www.nbcnews.com/politics/congress/senate-passes-8-3-billion-emergency-bill-combat-coronavirus-n1150521},
  journal   = {NBCNews.com},
  publisher = {NBCUniversal News Group},
  author    = {Shabad, Rebecca},
  year      = {2020},
  month     = {Mar},
  urldate   = {2020-10-02}
}

@online{JAS2020,
  title     = {WHO declares COVID-19 disease to be a pandemic},
  url       = {https://www.nbcnews.com/health/health-news/live-blog/coronavirus-updates-live-u-s-cases-top-1-000-spread-n1155241/ncrd1155646#blogHeader},
  journal   = {NBCNews.com},
  publisher = {NBCUniversal News Group},
  author    = {Jason},
  year      = {2020},
  month     = {Apr},
  urldate   = {2020-10-02}
}

@online{GIV2020,
  title     = {U.S. surpasses China with coronavirus cases as global total tops 500,000},
  url       = {https://www.nbcnews.com/news/world/u-s-surpasses-china-coronavirus-cases-global-total-tops-500-n1170136},
  journal   = {NBCNews.com},
  publisher = {NBCUniversal News Group},
  author    = {Givetash, Linda},
  year      = {2020},
  month     = {Mar},
  urldate   = {2020-10-02}
}

@online{PHI2020,
  title     = {Coronavirus cases top 1 million worldwide, researchers say},
  url       = {https://www.foxnews.com/health/coronavirus-cases-top-1-million-worldwide-johns-hopkins},
  journal   = {Fox News},
  publisher = {FOX News Network},
  author    = {Phillips, Morgan},
  year      = {2020},
  month     = {Apr},
  urldate   = {2020-10-02}
}

@online{ECDC2020,
  title     = {The daily number of new reported cases of COVID-19 by country worldwide},
  url       = {https://www.ecdc.europa.eu/en/publications-data/download-todays-data-geographic-distribution-covid-19-cases-worldwide},
  journal   = {European Centre for Disease Prevention and Control},
  author    = {European Centre for Disease Prevention and Control},
  publisher = {European Centre for Disease Prevention and Control},
  year      = {2020},
  month     = {Oct},
  urldate   = {2020-10-02}
}

@misc{WMR2020,
  title   = {Coronavirus Cases Distribution},
  url     = {https://www.worldometers.info/coronavirus/},
  journal = {Worldometer}
}

@misc{GGN2020,
  title     = {Coronavirus (COVID-19)},
  url       = {https://news.google.com/covid19/map?hl=en-US&gl=US&ceid=US:en},
  journal   = {Google News},
  publisher = {Google}
}

@online{IAEA2020,
  title     = {How is the COVID-19 Virus Detected using Real Time RT-PCR?},
  url       = {https://www.iaea.org/newscenter/news/how-is-the-covid-19-virus-detected-using-real-time-rt-pcr},
  journal   = {IAEA},
  publisher = {International Atomic Energy Agency (IAEA)},
  author    = {Jawerth, Nicole},
  year      = {2020},
  month     = {Mar},
  urldate   = {2020-10-02}
}

@article{KLL+2020,
  title    = {Variation in False-Negative Rate of Reverse Transcriptase Polymerase Chain Reaction–Based SARS-CoV-2 Tests by Time Since Exposure},
  journal  = {Annals of Internal Medicine},
  volume   = {173},
  number   = {4},
  pages    = {262-267},
  year     = {2020},
  doi      = {10.7326/M20-1495},
  note     = {PMID: 32422057},
  author   = {Kucirka LM and Lauer SA and Laeyendecker O and Boon D and Lessler J},
  url      = { 
        https://doi.org/10.7326/M20-1495
},
  eprint   = { 
        https://doi.org/10.7326/M20-1495
},
  abstract = { Background: Tests for severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2) based on reverse transcriptase polymerase chain reaction (RT-PCR) are being used to rule out infection among high-risk persons, such as exposed inpatients and health care workers. It is critical to understand how the predictive value of the test varies with time from exposure and symptom onset to avoid being falsely reassured by negative test results. Objective: To estimate the false-negative rate by day since infection. Design: Literature review and pooled analysis. Setting: 7 previously published studies providing data on RT-PCR performance by time since symptom onset or SARS-CoV-2 exposure using samples from the upper respiratory tract (n = 1330). Patients: A mix of inpatients and outpatients with SARS-CoV-2 infection. Measurements: A Bayesian hierarchical model was fitted to estimate the false-negative rate by day since exposure and symptom onset. Results: Over the 4 days of infection before the typical time of symptom onset (day 5), the probability of a false-negative result in an infected person decreases from 100\% (95\% CI, 100\% to 100\%) on day 1 to 67\% (CI, 27\% to 94\%) on day 4. On the day of symptom onset, the median false-negative rate was 38\% (CI, 18\% to 65\%). This decreased to 20\% (CI, 12\% to 30\%) on day 8 (3 days after symptom onset) then began to increase again, from 21\% (CI, 13\% to 31\%) on day 9 to 66\% (CI, 54\% to 77\%) on day 21. Limitation: Imprecise estimates due to heterogeneity in the design of studies on which results were based. Conclusion: Care must be taken in interpreting RT-PCR tests for SARS-CoV-2 infection—particularly early in the course of infection—when using these results as a basis for removing precautions intended to prevent onward transmission. If clinical suspicion is high, infection should not be ruled out on the basis of RT-PCR alone, and the clinical and epidemiologic situation should be carefully considered. Primary Funding Source: National Institute of Allergy and Infectious Diseases, Johns Hopkins Health System, and U.S. Centers for Disease Control and Prevention. }
}

 @article{CMA+2020,
  title        = {CT Imaging Features of 2019 Novel Coronavirus (2019-nCoV)},
  volume       = {295},
  issn         = {0033-8419, 1527-1315},
  doi          = {10.1148/radiol.2020200230},
  abstractnote = {The 2019 novel coronavirus manifests with characteristic chest CT imaging features, which are helpful to the radiologist for the early detection and diagnosis of this emerging global health emergency.},
  number       = {1},
  journal      = {Radiology},
  author       = {Chung, Michael and Bernheim, Adam and Mei, Xueyan and Zhang, Ning and Huang, Mingqian and Zeng, Xianjun and Cui, Jiufa and Xu, Wenjian and Yang, Yang and Fayad, Zahi A. and et al.},
  year         = {2020},
  month        = {Apr},
  pages        = {202–207}
}

 @article{RAJ+2020,
  title        = {Clinical, laboratory and imaging features of COVID-19: A systematic review and meta-analysis},
  volume       = {34},
  issn         = {14778939},
  doi          = {10.1016/j.tmaid.2020.101623},
  abstractnote = {Introduction: An epidemic of Coronavirus Disease 2019 (COVID-19) began in December 2019 in China leading to a Public Health Emergency of International Concern (PHEIC). Clinical, laboratory, and imaging features have been partially characterized in some observational studies. No systematic reviews on COVID-19 have been published to date.
Methods: We performed a systematic literature review with meta-analysis, using three databases to assess clinical, laboratory, imaging features, and outcomes of COVID-19 confirmed cases. Observational studies and also case reports, were included, and analyzed separately. We performed a random-effects model meta-analysis to calculate pooled prevalences and 95% confidence intervals (95%CI).
Results: 660 articles were retrieved for the time frame (1/1/2020-2/23/2020). After screening, 27 articles were selected for full-text assessment, 19 being finally included for qualitative and quantitative analyses. Additionally, 39 case report articles were included and analyzed separately. For 656 patients, fever (88.7%, 95%CI 84.5–92.9%), cough (57.6%, 95%CI 40.8–74.4%) and dyspnea (45.6%, 95%CI 10.9–80.4%) were the most prevalent manifestations. Among the patients, 20.3% (95%CI 10.0–30.6%) required intensive care unit (ICU), 32.8% presented with acute respiratory distress syndrome (ARDS) (95%CI 13.7–51.8), 6.2% (95%CI 3.1–9.3) with shock. Some 13.9% (95%CI 6.2–21.5%) of hospitalized patients had fatal outcomes (case fatality rate, CFR).
Conclusion: COVID-19 brings a huge burden to healthcare facilities, especially in patients with comorbidities. ICU was required for approximately 20% of polymorbid, COVID-19 infected patients and hospitalization was associated with a CFR of > 13%. As this virus spreads globally, countries need to urgently prepare human resources, infrastructure and facilities to treat severe COVID-19.},
  journal      = {Travel Medicine and Infectious Disease},
  author       = {Rodriguez-Morales, Alfonso J. and Cardona-Ospina, Jaime A. and Gutiérrez-Ocampo, Estefanía and Villamizar-Peña, Rhuvi and Holguin-Rivera, Yeimer and Escalera-Antezana, Juan Pablo and Alvarado-Arnez, Lucia Elena and Bonilla-Aldana, D. Katterine and Franco-Paredes, Carlos and Henao-Martinez, Andrés F. and et al.},
  year         = {2020},
  month        = {Mar},
  pages        = {101623}
}

 @article{CYX+2020,
  title        = {Clinical features of patients infected with 2019 novel coronavirus in Wuhan, China},
  volume       = {395},
  issn         = {01406736},
  doi          = {10.1016/S0140-6736(20)30183-5},
  abstractnote = {Background A recent cluster of pneumonia cases in Wuhan, China, was caused by a novel betacoronavirus, the 2019 novel coronavirus (2019-nCoV). We report the epidemiological, clinical, laboratory, and radiological characteristics and treatment and clinical outcomes of these patients.},
  number       = {10223},
  journal      = {The Lancet},
  author       = {Huang, Chaolin and Wang, Yeming and Li, Xingwang and Ren, Lili and Zhao, Jianping and Hu, Yi and Zhang, Li and Fan, Guohui and Xu, Jiuyang and Gu, Xiaoying and et al.},
  year         = {2020},
  month        = {Feb},
  pages        = {497–506}
}

 @article{CNM+2020,
  title        = {Epidemiological and clinical characteristics of 99 cases of 2019 novel coronavirus pneumonia in Wuhan, China: a descriptive study},
  volume       = {395},
  issn         = {01406736},
  doi          = {10.1016/S0140-6736(20)30211-7},
  abstractnote = {Background In December, 2019, a pneumonia associated with the 2019 novel coronavirus (2019-nCoV) emerged in Wuhan, China. We aimed to further clarify the epidemiological and clinical characteristics of 2019-nCoV pneumonia.},
  number       = {10223},
  journal      = {The Lancet},
  author       = {Chen, Nanshan and Zhou, Min and Dong, Xuan and Qu, Jieming and Gong, Fengyun and Han, Yang and Qiu, Yang and Wang, Jingli and Liu, Ying and Wei, Yuan and et al.},
  year         = {2020},
  month        = {Feb},
  pages        = {507–513}
}

 @article{WDC+2020,
  title        = {Clinical Characteristics of 138 Hospitalized Patients With 2019 Novel Coronavirus–Infected Pneumonia in Wuhan, China},
  volume       = {323},
  issn         = {0098-7484},
  doi          = {10.1001/jama.2020.1585},
  abstractnote = {In December 2019, novel coronavirus (2019-nCoV)–infected pneumonia (NCIP) occurred in Wuhan, China. The number of cases has increased rapidly but information on the clinical characteristics of affected patients is limited.To describe the epidemiological and clinical characteristics of NCIP.Retrospective, single-center case series of the 138 consecutive hospitalized patients with confirmed NCIP at Zhongnan Hospital of Wuhan University in Wuhan, China, from January 1 to January 28, 2020; final date of follow-up was February 3, 2020.Documented NCIP.Epidemiological, demographic, clinical, laboratory, radiological, and treatment data were collected and analyzed. Outcomes of critically ill patients and noncritically ill patients were compared. Presumed hospital-related transmission was suspected if a cluster of health professionals or hospitalized patients in the same wards became infected and a possible source of infection could be tracked.Of 138 hospitalized patients with NCIP, the median age was 56 years (interquartile range, 42-68; range, 22-92 years) and 75 (54.3%) were men. Hospital-associated transmission was suspected as the presumed mechanism of infection for affected health professionals (40 [29%]) and hospitalized patients (17 [12.3%]). Common symptoms included fever (136 [98.6%]), fatigue (96 [69.6%]), and dry cough (82 [59.4%]). Lymphopenia (lymphocyte count, 0.8 × 109/L [interquartile range {IQR}, 0.6-1.1]) occurred in 97 patients (70.3%), prolonged prothrombin time (13.0 seconds [IQR, 12.3-13.7]) in 80 patients (58%), and elevated lactate dehydrogenase (261 U/L [IQR, 182-403]) in 55 patients (39.9%). Chest computed tomographic scans showed bilateral patchy shadows or ground glass opacity in the lungs of all patients. Most patients received antiviral therapy (oseltamivir, 124 [89.9%]), and many received antibacterial therapy (moxifloxacin, 89 [64.4%]; ceftriaxone, 34 [24.6%]; azithromycin, 25 [18.1%]) and glucocorticoid therapy (62 [44.9%]). Thirty-six patients (26.1%) were transferred to the intensive care unit (ICU) because of complications, including acute respiratory distress syndrome (22 [61.1%]), arrhythmia (16 [44.4%]), and shock (11 [30.6%]). The median time from first symptom to dyspnea was 5.0 days, to hospital admission was 7.0 days, and to ARDS was 8.0 days. Patients treated in the ICU (n = 36), compared with patients not treated in the ICU (n = 102), were older (median age, 66 years vs 51 years), were more likely to have underlying comorbidities (26 [72.2%] vs 38 [37.3%]), and were more likely to have dyspnea (23 [63.9%] vs 20 [19.6%]), and anorexia (24 [66.7%] vs 31 [30.4%]). Of the 36 cases in the ICU, 4 (11.1%) received high-flow oxygen therapy, 15 (41.7%) received noninvasive ventilation, and 17 (47.2%) received invasive ventilation (4 were switched to extracorporeal membrane oxygenation). As of February 3, 47 patients (34.1%) were discharged and 6 died (overall mortality, 4.3%), but the remaining patients are still hospitalized. Among those discharged alive (n = 47), the median hospital stay was 10 days (IQR, 7.0-14.0).In this single-center case series of 138 hospitalized patients with confirmed NCIP in Wuhan, China, presumed hospital-related transmission of 2019-nCoV was suspected in 41% of patients, 26% of patients received ICU care, and mortality was 4.3%.},
  number       = {11},
  journal      = {JAMA},
  author       = {Wang, Dawei and Hu, Bo and Hu, Chang and Zhu, Fangfang and Liu, Xing and Zhang, Jing and Wang, Binbin and Xiang, Hui and Cheng, Zhenshun and Xiong, Yong and et al.},
  year         = {2020},
  pages        = {1061–1069}
}

 @article{LKY+2020,
  title        = {Clinical characteristics of novel coronavirus cases in tertiary hospitals in Hubei Province:},
  volume       = {133},
  issn         = {0366-6999},
  doi          = {10.1097/CM9.0000000000000744},
  abstractnote = {Background: The 2019 novel coronavirus (2019-nCoV) causing an outbreak of pneumonia in Wuhan, Hubei province of China was isolated in January 2020. This study aims to investigate its epidemiologic history, and analyze the clinical characteristics, treatment regimens, and prognosis of patients infected with 2019-nCoV during this outbreak.
Methods: Clinical data from 137 2019-nCoV-infected patients admitted to the respiratory departments of nine tertiary hospitals in Hubei province from December 30, 2019 to January 24, 2020 were retrospectively collected, including general status, clinical manifestations, laboratory test results, imaging characteristics, and treatment regimens.
Results: None of the 137 patients (61 males, 76 females, aged 20–83 years, median age 57 years) had a deﬁnite history of exposure to Huanan Seafood Wholesale Market. Major initial symptoms included fever (112/137, 81.8%), coughing (66/137, 48.2%), and muscle pain or fatigue (44/137, 32.1%), with other, less typical initial symptoms observed at low frequency, including heart palpitations, diarrhea, and headache. Nearly 80% of the patients had normal or decreased white blood cell counts, and 72.3% (99/137) had lymphocytopenia. Lung involvement was present in all cases, with most chest computed tomography scans showing lesions in multiple lung lobes, some of which were dense; ground-glass opacity co-existed with consolidation shadows or cordlike shadows. Given the lack of effective drugs, treatment focused on symptomatic and respiratory support. Immunoglobulin G was delivered to some critically ill patients according to their conditions. Systemic corticosteroid treatment did not show signiﬁcant beneﬁts. Notably, early respiratory support facilitated disease recovery and improved prognosis. The risk of death was primarily associated with age, underlying chronic diseases, and median interval from the appearance of initial symptoms to dyspnea.
Conclusions: The majority of patients with 2019-nCoV pneumonia present with fever as the ﬁrst symptom, and most of them still showed typical manifestations of viral pneumonia on chest imaging. Middle-aged and elderly patients with underlying comorbidities are susceptible to respiratory failure and may have a poorer prognosis.},
  number       = {9},
  journal      = {Chinese Medical Journal},
  author       = {Liu, Kui and Fang, Yuan-Yuan and Deng, Yan and Liu, Wei and Wang, Mei-Fang and Ma, Jing-Ping and Xiao, Wei and Wang, Ying-Nan and Zhong, Min-Hua and Li, Cheng-Hong and et al.},
  year         = {2020},
  month        = {May},
  pages        = {1025–1031}
}

@article{CML+2020,
  title   = {Epidemiologic and Clinical Characteristics of Novel Coronavirus Infections Involving 13 Patients Outside Wuhan, China},
  volume  = {323},
  doi     = {10.1001/jama.2020.1623},
  number  = {11},
  journal = {Jama},
  author  = {Chang, De and Lin, Minggui and Wei, Lai and Xie, Lixin and Zhu, Guangfa and Cruz, Charles S. Dela and Sharma, Lokesh},
  year    = {2020},
  pages   = {1092}
}

 @article{PYH+2020,
  title        = {Initial CT findings and temporal changes in patients with the novel coronavirus pneumonia (2019-nCoV): a study of 63 patients in Wuhan, China},
  volume       = {30},
  issn         = {0938-7994, 1432-1084},
  doi          = {10.1007/s00330-020-06731-x},
  abstractnote = {Objectives The purpose of this study was to observe the imaging characteristics of the novel coronavirus pneumonia.
Methods Sixty-three confirmed patients were enrolled from December 30, 2019 to January 31, 2020. High-resolution CT (HRCT) of the chest was performed. The number of affected lobes, ground glass nodules (GGO), patchy/punctate ground glass opacities, patchy consolidation, fibrous stripes and irregular solid nodules in each patient’s chest CT image were recorded. Additionally, we performed imaging follow-up of these patients.
Results CT images of 63 confirmed patients were collected. M/F ratio: 33/30. The mean age was 44.9 ± 15.2 years. The mean number of affected lobes was 3.3 ± 1.8. Nineteen (30.2%) patients had one affected lobe, five (7.9%) patients had two affected lobes, four (6.3%) patients had three affected lobes, seven (11.1%) patients had four affected lobes while 28 (44.4%) patients had 5 affected lobes. Fifty-four (85.7%) patients had patchy/punctate ground glass opacities, 14 (22.2%) patients had GGO, 12 (19.0%) patients had patchy consolidation, 11 (17.5%) patients had fibrous stripes and 8 (12.7%) patients had irregular solid nodules. Fifty-four (85.7%) patients progressed, including single GGO increased, enlarged and consolidated; fibrous stripe enlarged, while solid nodules increased and enlarged.
Conclusions Imaging changes in novel viral pneumonia are rapid. The manifestations of the novel coronavirus pneumonia are diverse. Imaging changes of typical viral pneumonia and some specific imaging features were observed. Therefore, we need to strengthen the recognition of image changes to help clinicians to diagnose quickly and accurately.},
  number       = {6},
  journal      = {European Radiology},
  author       = {Pan, Yueying and Guan, Hanxiong and Zhou, Shuchang and Wang, Yujin and Li, Qian and Zhu, Tingting and Hu, Qiongjie and Xia, Liming},
  year         = {2020},
  month        = {Jun},
  pages        = {3306–3309}
}

 @article{ZMX+2020,
  title   = {[Clinical features of 2019 novel coronavirus pneumonia in the early stage from a fever clinic in Beijing]},
  volume  = {43},
  issn    = {1001-0939},
  doi     = {10.3760/cma.j.issn.1001-0939.2020.03.015},
  number  = {3},
  journal = {Zhonghua jie he he hu xi za zhi = Zhonghua jiehe he huxi zazhi = Chinese journal of tuberculosis and respiratory diseases},
  author  = {Zhang, MQ and Wang, XH and Chen, YL and Zhao, KL and Cai, YQ and An, CL and Lin, MG and Mu, XD},
  year    = {2020},
  month   = {Mar},
  pages   = {215—218}
}

 @article{SFJ+2020,
  title        = {Review of Artificial Intelligence Techniques in Imaging Data Acquisition, Segmentation and Diagnosis for COVID-19},
  issn         = {1941-1189},
  doi          = {10.1109/RBME.2020.2987975},
  abstractnote = {The pandemic of coronavirus disease 2019 (COVID-19) is spreading all over the world. Medical imaging such as X-ray and computed tomography (CT) plays an essential role in the global fight against COVID-19, whereas the recently emerging artificial intelligence (AI) technologies further strengthen the power of the imaging tools and help medical specialists. We hereby review the rapid responses in the community of medical imaging (empowered by AI) toward COVID-19. For example, AI-empowered image acquisition can significantly help automate the scanning procedure and also reshape the workflow with minimal contact to patients, providing the best protection to the imaging technicians. Also, AI can improve work efficiency by accurate delineation of infections in X-ray and CT images, facilitating subsequent quantification. Moreover, the computer-aided platforms help radiologists make clinical decisions, i.e., for disease diagnosis, tracking, and prognosis. In this review paper, we thus cover the entire pipeline of medical imaging and analysis techniques involved with COVID-19, including image acquisition, segmentation, diagnosis, and follow-up. We particularly focus on the integration of AI with X-ray and CT, both of which are widely used in the frontline hospitals, in order to depict the latest progress of medical imaging and radiology fighting against COVID-19.},
  journal      = {IEEE Reviews in Biomedical Engineering},
  author       = {Shi, Feng and Wang, Jun and Shi, Jun and Wu, Ziyan and Wang, Qian and Tang, Zhenyu and He, Kelei and Shi, Yinghuan and Shen, Dinggang},
  year         = {2020},
  pages        = {1–1}
}

@online{UIH2020,
  title   = {United Imaging's Emergency Radiology Departments Support Mobile Cabin Hospitals, Facilitate 5G Remote Diagnosis},
  url     = {https://www.prnewswire.com/news-releases/united-imagings-emergency-radiology-departments-support-mobile-cabin-hospitals-facilitate-5g-remote-diagnosis-301010528.html},
  journal = {PR Newswire: news distribution, targeting and monitoring},
  author  = {United Imaging Healthcare Co., Ltd.},
  year    = {2020},
  month   = {Feb}
}

 @article{SVK+2017,
  place        = {Cham},
  series       = {Lecture Notes in Computer Science},
  title        = {DARWIN: Deformable Patient Avatar Representation With Deep Image Network},
  volume       = {10434},
  isbn         = {978-3-319-66184-1},
  issn         = {0302-9743},
  abstractnote = {In this paper, we present a technical approach to robustly estimate the detailed patient body surface mesh under clothing cover from a single snapshot of a range sensor. Existing methods either lack level of detail of the estimated patient body model, fail to estimate the body model robustly under clothing cover, or lack sufficient evaluation over real patient datasets. In this work, we overcome these limitations by learning deep convolutional networks over real clinical dataset with large variation and augmentation. Our approach is validated with experiments conducted over 1063 human subjects from 3 different hospitals and surface errors are measured against groundtruth from CT data.},
  publisher    = {Springer International Publishing},
  author       = {Singh, Vivek and Ma, Kai and Tamersoy, Birgi and Chang, Yao-Jen and Wimmer, Andreas and O’Donnell, Thomas and Chen, Terrence},
  year         = {2017},
  pages        = {497–504},
  collection   = {Lecture Notes in Computer Science}
}

 @article{SVY+2017,
  place        = {Cham},
  series       = {Lecture Notes in Computer Science},
  title        = {Estimating a Patient Surface Model for Optimizing the Medical Scanning Workflow},
  volume       = {8673},
  isbn         = {978-3-319-10403-4},
  issn         = {0302-9743},
  abstractnote = {In this paper, we present the idea of equipping a tomographic medical scanner with a range imaging device (e.g. a 3D camera) to improve the current scanning workflow. A novel technical approach is proposed to robustly estimate patient surface geometry by a single snapshot from the camera. Leveraging the information of the patient surface geometry can provide significant clinical benefits, including automation of the scan, motion compensation for better image quality, sanity check of patient movement, augmented reality for guidance, patient specific dose optimization, and more. Our approach overcomes the technical difficulties resulting from suboptimal camera placement due to practical considerations. Experimental results on more than 30 patients from a real CT scanner demonstrate the robustness of our approach.},
  publisher    = {Springer International Publishing},
  author       = {Singh, Vivek and Chang, Yao-jen and Ma, Kai and Wels, Michael and Soza, Grzegorz and Chen, Terrence},
  pages        = {472–479},
  collection   = {Lecture Notes in Computer Science}
}

@online{SIE2020,
  title   = {SOMATOM Edge Plus},
  url     = {https://www.siemens-healthineers.com/en-us/computed-tomography/single-source-ct/somatom-edge-plus},
  journal = {SOMATOM Edge Plus - Siemens Healthineers USA},
  author  = {Siemens},
  year = {2020}
}

 @article{LJU+2020,
  title        = {Automatic Patient Centering for MDCT: Effect on Radiation Dose},
  volume       = {188},
  issn         = {0361-803X, 1546-3141},
  doi          = {10.2214/AJR.06.0370},
  abstractnote = {OBJECTIVE. The purpose of this study was to determine with phantom and patient imaging the effect of an automatic patient-centering technique on the radiation dose associated with MDCT. SUBJECTS AND METHODS. A 32-cm CT dose index (CTDI) phantom was scanned with 64-MDCT in three positions: gantry isocenter and 30 and 60 mm below the isocenter of the scanner gantry. In each position, surface, peripheral, and volume CTDIs were estimated with a standard 10-cm pencil ionization chamber. The institutional review board approved the study with 63 patients (36 men, 27 women; mean age, 51 years; age range, 22–83 years) undergoing chest (n = 18) or abdominal (n = 45) CT using the z-axis automatic exposure control technique. Each patient was positioned according to the region being scanned and then was centered in the gantry. Before scanning of a patient, automatic centering software was used to estimate patient off-centering and percentage of dose reduction with optimum recentering. Data were analyzed with linear correlation and the Student’s t test.
RESULTS. Peripheral and surface CTDIs increased approximately 12–18% with 30-mm off-center distance and 41–49% with 60-mm off-center distance. Approximately 95% (60/63) of patients were not positioned accurately in the gantry isocenter. The mean radiation dose saving with automatic centering of all patients was 13.0% ± 0.9% (range, 2.6–29.9%). There was strong correlation between off-center distance and percentage of surface CTDI reduction with recentering of patients in the gantry isocenter (r2 = 0.85, p < 0.0001).
CONCLUSION. Surfaces doses can be reduced if radiologic technologists can better center patients within the CT gantry. Automatic centering technique can help in optimum patient centering and result in as much as 30% reduction in surface dose.},
  number       = {2},
  journal      = {American Journal of Roentgenology},
  author       = {Li, Jianhai and Udayasankar, Unni K. and Toth, Thomas L. and Seamans, John and Small, William C. and Kalra, Mannudeep K.},
  year         = {2007},
  month        = {Feb},
  pages        = {547–552}
}

 @article{MAR2020,
  title        = {Optimisation in general radiography},
  volume       = {3},
  issn         = {1823-5530},
  url          = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3097657/},
  doi          = {10.2349/biij.3.2.e18},
  abstractnote = {Radiography using film has been an established method for imaging the internal organs of the body for over 100 years. Surveys carried out during the 1980s identified a wide range in patient doses showing that there was scope for dosage reduction in many hospitals. This paper discusses factors that need to be considered in optimising the performance of radiographic equipment. The most important factor is choice of the screen/film combination, and the preparation of automatic exposure control devices to suit its characteristics. Tube potential determines the photon energies in the X-ray beam, with the selection involving a compromise between image contrast and the dose to the patient. Allied to this is the choice of anti-scatter grid, as a high grid ratio effectively removes the larger component of scatter when using higher tube potentials. However, a high grid ratio attenuates the X-ray beam more heavily. Decisions about grids and use of low attenuation components are particularly important for paediatric radiography, which uses lower energy X-ray beams. Another factor which can reduce patient dose is the use of copper filtration to remove more low-energy X-rays. Regular surveys of patient dose and comparisons with diagnostic reference levels that provide a guide representing good practice enable units for which doses are higher to be identified. Causes can then be investigated and changes implemented to address any shortfalls. Application of these methods has led to a gradual reduction in doses in many countries.},
  number       = {2},
  journal      = {Biomedical Imaging and Intervention Journal},
  author       = {Martin, CJ},
  year         = {2007},
  month        = {Apr}
}

 @article{AFA+2020,
  place        = {Cham},
  series       = {Lecture Notes in Computer Science},
  title        = {Patient MoCap: Human Pose Estimation Under Blanket Occlusion for Hospital Monitoring Applications},
  volume       = {9900},
  isbn         = {978-3-319-46719-1},
  url          = {http://link.springer.com/10.1007/978-3-319-46720-7_57},
  doi          = {10.1007/978-3-319-46720-7_57},
  abstractnote = {Motion analysis is typically used for a range of diagnostic procedures in the hospital. While automatic pose estimation from RGB-D input has entered the hospital in the domain of rehabilitation medicine and gait analysis, no such method is available for bed-ridden patients. However, patient pose estimation in the bed is required in several ﬁelds such as sleep laboratories, epilepsy monitoring and intensive care units. In this work, we propose a learning-based method that allows to automatically infer 3D patient pose from depth images. To this end we rely on a combination of convolutional neural network and recurrent neural network, which we train on a large database that covers a range of motions in the hospital bed. We compare to a state of the art pose estimation method which is trained on the same data and show the superior result of our method. Furthermore, we show that our method can estimate the joint positions under a simulated occluding blanket with an average joint error of 7.56 cm.},
  booktitle    = {Medical Image Computing and Computer-Assisted Intervention – MICCAI 2016},
  publisher    = {Springer International Publishing},
  author       = {Achilles, Felix and Ichim, Alexandru-Eugen and Coskun, Huseyin and Tombari, Federico and Noachtar, Soheyl and Navab, Nassir},
  editor       = {Ourselin, Sebastien and Joskowicz, Leo and Sabuncu, Mert R. and Unal, Gozde and Wells, William},
  year         = {2016},
  pages        = {491–499},
  collection   = {Lecture Notes in Computer Science}
}

@online{SIE(2)2020,
  title   = {Künstliche Intelligenz: How Artificial Intelligence Can Improve CT Scans},
  url     = {https://new.siemens.com/global/en/company/stories/research-technologies/artificial-intelligence/artificial-intelligence-imaging-techniques.html},
  journal = {siemens.com Global Website},
  author  = {Siemens},
  year = {2020}
}

 @article{WYX+2020,
  place        = {Netherlands},
  title        = {Precise pulmonary scanning and reducing medical radiation exposure by developing a clinically applicable intelligent CT system: Toward improving patient care},
  volume       = {54},
  issn         = {2352-3964},
  abstractnote = {Interstitial lung disease requires frequent re-examination, which directly causes excessive cumulative radiation exposure. To date, AI has not been applied to CT for enhancing clinical care; thus, we hypothesize AI may empower CT with intelligence to realize automatic and accurate pulmonary scanning, thus dramatically decrease medical radiation exposure without compromising patient care. Facial boundary detection was realized by recognizing adjacent jaw position through training and testing a region proposal network (RPN) on 76,882 human faces using a preinstalled 2-dimensional camera; the lung-fields was then segmented by V-Net on another training set with 314 subjects and calculated the moving distance of the scanning couch based on a pre-generated calibration table. A multi-cohort study, including 1,186 patients was used for validation and radiation dose quantification under three clinical scenarios. A U-HAPPY (United imaging Human Automatic Planbox for PulmonarY) scanning CT was designed. Error distance of RPN was 4·46±0·02 pixels with a success rate of 98·7% in training set and 2·23±0·10 pixels with 100% success rate in testing set. Average Dice’s coefficient was 0·99 in training set and 0·96 in testing set. A calibration table with 1,344,000 matches was generated to support the linkage between camera and scanner. This real-time automation makes an accurate plan-box to cover exact location and area needed to scan, thus reducing amounts of radiation exposures significantly (all, P<0·001). U-HAPPY CT designed for pulmonary imaging acquisition standardization is promising for reducing patient risk and optimizing public health expenditures. The National Natural Science Foundation of China.},
  journal      = {EBioMedicine},
  publisher    = {Elsevier B.V},
  author       = {Wang, Yang and Lu, Xiaofan and Zhang, Yingwei and Zhang, Xin and Wang, Kun and Liu, Jiani and Li, Xin and Hu, Renfang and Meng, Xiaolin and Dou, Shidan and et al.},
  year         = {2020},
  pages        = {102724–102724}
}

@article{CXZ+2020,
  title     = {Deep learning-based detection for COVID-19 from chest CT using weak label},
  author    = {Zheng, Chuansheng and Deng, Xianbo and Fu, Qing and Zhou, Qiang and Feng, Jiapei and Ma, Hui and Liu, Wenyu and Wang, Xinggang},
  journal   = {medRxiv},
  year      = {2020},
  publisher = {Cold Spring Harbor Laboratory Press}
}

 @article{CYZ+2020,
  title   = {Longitudinal Assessment of COVID-19 Using a Deep Learning–based Quantitative CT Pipeline: Illustration of Two Cases},
  volume  = {2},
  issn    = {2638-6135},
  doi     = {10.1148/ryct.2020200082},
  number  = {2},
  journal = {Radiology: Cardiothoracic Imaging},
  author  = {Cao, Yukun and Xu, Zhanwei and Feng, Jianjiang and Jin, Cheng and Han, Xiaoyu and Wu, Hanping and Shi, Heshui},
  year    = {2020},
  month   = {Apr},
  pages   = {e200082}
}

 @article{HLR+2020,
  title   = {Serial Quantitative Chest CT Assessment of COVID-19: Deep-Learning Approach},
  volume  = {2},
  issn    = {2638-6135},
  doi     = {10.1148/ryct.2020200075},
  number  = {2},
  journal = {Radiology: Cardiothoracic Imaging},
  author  = {Huang, Lu and Han, Rui and Ai, Tao and Yu, Pengxin and Kang, Han and Tao, Qian and Xia, Liming},
  year    = {2020},
  month   = {Apr},
  pages   = {e200075}
}

 @article{YHQ+2020,
  title        = {Machine learning-based CT radiomics method for predicting hospital stay in patients with pneumonia associated with SARS-CoV-2 infection: a multicenter study},
  volume       = {8},
  issn         = {23055839, 23055847},
  doi          = {10.21037/atm-20-3026},
  abstractnote = {Background: The coronavirus disease 2019 (COVID-19) has become a global challenge since the December 2019. The hospital stay is one of the prognostic indicators, and its predicting model based on CT radiomics features is important for assessing the patients’ clinical outcome. The study aimed to develop and test machine learning-based CT radiomics models for predicting hospital stay in patients with COVID-19 pneumonia.
Methods: This retrospective, multicenter study enrolled patients with laboratory-confirmed SARS-CoV-2 infection and their initial CT images from 5 designated hospitals in Ankang, Lishui, Lanzhou, Linxia, and Zhenjiang between January 23, 2020 and February 8, 2020. Patients were classified into short-term (≤10 days) and long-term hospital stay (>10 days). CT radiomics models based on logistic regression (LR) and random forest (RF) were developed on features from pneumonia lesions in first four centers. The predictive performance was evaluated in fifth center (test dataset) on lung lobe- and patients-level.
Results: A total of 52 patients were enrolled from designated hospitals. As of February 20, 21 patients remained in hospital or with non-findings in CT were excluded. Therefore, 31 patients with 72 lesion segments were included in analysis. The CT radiomics models based on 6 second-order features were effective in discriminating short- and long-term hospital stay in patients with COVID-19 pneumonia, with areas under the curves of 0.97 (95% CI, 0.83–1.0) and 0.92 (95% CI, 0.67–1.0) by LR and RF, respectively, in test. The LR and RF model showed a sensitivity and specificity of 1.0 and 0.89, 0.75 and 1.0 in test respectively. As of February 28, a prospective cohort of six discharged patients were all correctly recognized as long-term stay using RF and LR models.
Conclusions: The machine learning-based CT radiomics features and models showed feasibility and accuracy for predicting hospital stay in patients with COVID-19 pneumonia.},
  number       = {14},
  journal      = {Annals of Translational Medicine},
  author       = {Yue, Hongmei and Yu, Qian and Liu, Chuan and Huang, Yifei and Jiang, Zicheng and Shao, Chuxiao and Zhang, Hongguang and Ma, Baoyi and Wang, Yuancheng and Xie, Guanghang and et al.},
  year         = {2020},
  month        = {Jul},
  pages        = {859–859}
}

@article{GOM+2020,
  title   = {Rapid ai development cycle for the coronavirus (covid-19) pandemic: Initial results for automated detection \& patient monitoring using deep learning ct image analysis},
  author  = {Gozes, Ophir and Frid-Adar, Maayan and Greenspan, Hayit and Browning, Patrick D and Zhang, Huangqi and Ji, Wenbin and Bernheim, Adam and Siegel, Eliot},
  journal = {arXiv preprint arXiv:2003.05037},
  year    = {2020}
}

 @article{LLL+2020,
  title        = {Artificial Intelligence Distinguishes COVID-19 from Community Acquired Pneumonia on Chest CT},
  issn         = {0033-8419},
  url          = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7233473/},
  doi          = {10.1148/radiol.2020200905},
  abstractnote = {Background
Coronavirus disease has widely spread all over the world since the beginning of 2020. It is desirable to develop automatic and accurate detection of COVID-19 using chest CT.

Purpose
To develop a fully automatic framework to detect COVID-19 using chest CT and evaluate its performances.

Materials and Methods
In this retrospective and multi-center study, a deep learning model, COVID-19 detection neural network (COVNet), was developed to extract visual features from volumetric chest CT exams for the detection of COVID-19. Community acquired pneumonia (CAP) and other non-pneumonia CT exams were included to test the robustness of the model. The datasets were collected from 6 hospitals between August 2016 and February 2020. Diagnostic performance was assessed by the area under the receiver operating characteristic curve (AUC), sensitivity and specificity.

Results
The collected dataset consisted of 4356 chest CT exams from 3,322 patients. The average age is 49±15 years and there were slightly more male patients than female (1838 vs 1484; p-value=0.29). The per-exam sensitivity and specificity for detecting COVID-19 in the independent test set was 114 of 127 (90% [95% CI: 83%, 94%]) and 294 of 307 (96% [95% CI: 93%, 98%]), respectively, with an AUC of 0.96 (p-value<0.001). The per-exam sensitivity and specificity for detecting CAP in the independent test set was 87% (152 of 175) and 92% (239 of 259), respectively, with an AUC of 0.95 (95% CI: 0.93, 0.97).

Conclusions
A deep learning model can accurately detect COVID-19 and differentiate it from community acquired pneumonia and other lung diseases.},
  journal      = {Radiology},
  author       = {Li, Lin and Qin, Lixin and Xu, Zeguo and Yin, Youbing and Wang, Xin and Kong, Bin and Bai, Junjie and Lu, Yi and Fang, Zhenghan and Song, Qi and et al.},
  year         = {2020},
  month        = {Mar}
}

 @book{CJL+2020,
  title        = {Deep learning-based model for detecting 2019 novel coronavirus pneumonia on high-resolution computed tomography: a prospective study},
  url          = {http://medrxiv.org/lookup/doi/10.1101/2020.02.25.20021568},
  doi          = {10.1101/2020.02.25.20021568},
  abstractnote = {Background: Computed tomography (CT) is the preferred imaging method for diagnosing 2019 novel coronavirus (COVID19) pneumonia. Our research aimed to construct a system based on deep learning for detecting COVID-19 pneumonia on high resolution CT, relieve working pressure of radiologists and contribute to the control of the epidemic.
Methods: For model development and validation, 46,096 anonymous images from 106 admitted patients, including 51 patients of laboratory confirmed COVID-19 pneumonia and 55 control patients of other diseases in Renmin Hospital of Wuhan University (Wuhan, Hubei province, China) were retrospectively collected and processed. Twenty-seven consecutive patients undergoing CT scans in Feb, 5, 2020 in Renmin Hospital of Wuhan University were prospectively collected to evaluate and compare the efficiency of radiologists against 2019-CoV pneumonia with that of the model.
Findings: The model achieved a per-patient sensitivity of 100%, specificity of 93.55%, accuracy of 95.24%, PPV of 84.62%, and NPV of 100%; a per-image sensitivity of 94.34%, specificity of 99.16%, accuracy of 98.85%, PPV of 88.37%, and NPV of 99.61% in retrospective dataset. For 27 prospective patients, the model achieved a comparable performance to that of expert radiologist. With the assistance of the model, the reading time of radiologists was greatly decreased by 65%.
Conclusion: The deep learning model showed a comparable performance with expert radiologist, and greatly improve the efficiency of radiologists in clinical practice. It holds great potential to relieve the pressure of frontline radiologists, improve early diagnosis, isolation and treatment, and thus contribute to the control of the epidemic.},
  institution  = {Infectious Diseases (except HIV/AIDS)},
  author       = {Chen, Jun and Wu, Lianlian and Zhang, Jun and Zhang, Liang and Gong, Dexin and Zhao, Yilin and Hu, Shan and Wang, Yonggui and Hu, Xiao and Zheng, Biqing and et al.},
  year         = {2020},
  month        = {Feb}
}

 @book{JSB+2020,
  title        = {AI-assisted CT imaging analysis for COVID-19 screening: Building and deploying a medical AI system in four weeks},
  url          = {http://medrxiv.org/lookup/doi/10.1101/2020.03.19.20039354},
  doi          = {10.1101/2020.03.19.20039354},
  abstractnote = {The sudden outbreak of novel coronavirus 2019 (COVID-19) increased the diagnostic burden of radiologists. In the time of an epidemic crisis, we hoped artificial intelligence (AI) to help reduce physician workload in regions with the outbreak, and improve the diagnosis accuracy for physicians before they could acquire enough experience with the new disease. Here, we present our experience in building and deploying an AI system that automatically analyzes CT images to detect COVID-19 pneumonia features.  Different from conventional medical AI, we were dealing with an epidemic crisis.  Working in an interdisciplinary team of over 30 people with medical and / or AI background, geographically distributed in Beijing and Wuhan, we were able to overcome a series of challenges in this particular situation and deploy the system in four weeks.  Using 1,136 training cases (723 positives for COVID-19) from five hospitals, we were able to achieve a sensitivity of 0.974 and specificity of 0.922 on the test dataset, which included a variety of pulmonary diseases.  Besides, the system automatically highlighted all lesion regions for faster examination.  As of today, we have deployed the system in 16 hospitals, and it is performing over 1,300 screenings per day.},
  institution  = {Health Informatics},
  author       = {Jin, Shuo and Wang, Bo and Xu, Haibo and Luo, Chuan and Wei, Lai and Zhao, Wei and Hou, Xuexue and Ma, Wenshuo and Xu, Zhengqing and Zheng, Zhuozhao and et al.},
  year         = {2020},
  month        = {Mar}
}

@article{SFY+2020,
  title   = {Lung infection quantification of covid-19 in ct images with deep learning},
  author  = {Shan, Fei and Gao, Yaozong and Wang, Jun and Shi, Weiya and Shi, Nannan and Han, Miaofei and Xue, Zhong and Shi, Yuxin},
  journal = {arXiv preprint arXiv:2003.04655},
  year    = {2020}
}

 @article{TLX+2020,
  title   = {Severe COVID-19 Pneumonia: Assessing Inflammation Burden with Volume-rendered Chest CT},
  volume  = {2},
  issn    = {2638-6135},
  doi     = {10.1148/ryct.2020200044},
  number  = {2},
  journal = {Radiology: Cardiothoracic Imaging},
  author  = {Tang, Lei and Zhang, Xiaoyong and Wang, Yvquan and Zeng, Xianchun},
  year    = {2020},
  month   = {Apr},
  pages   = {e200044}
}

 @article{SCN+2020,
  title        = {Quantitative computed tomography analysis for stratifying the severity of Coronavirus Disease 2019},
  volume       = {10},
  issn         = {20951779},
  doi          = {10.1016/j.jpha.2020.03.004},
  abstractnote = {Purpose: To examine the feasibility of using a computer tool for stratifying the severity of Coronavirus Disease 2019 (COVID-19) based on computed tomography (CT) images. Materials and methods: We retrospectively examined 44 confirmed COVID-19 cases. All cases were evaluated separately by radiologists (visually) and through an in-house computer software. The degree of lesions was visually scored by the radiologist, as follows, for each of the 5 lung lobes: 0, no lesion present; 1, < 1/3 involvement; 2, >1/3 and <2/3 involvement; and 3, > 2/3 involvement. Lesion density was assessed based on the proportion of ground-glass opacity (GGO), consolidation and fibrosis of the lesions. The parameters obtained using the computer tool included lung volume (mL), lesion volume (mL), lesion percentage (%), and mean lesion density (HU) of the whole lung, right lung, left lung, and each lobe. The scores obtained by the radiologists and quantitative results generated by the computer software were tested for correlation. A Chi-square test was used to test the consistency of radiologist- and computer-derived lesion percentage in the right/left lung, upper/lower lobe, and each of the 5 lobes. Result: The results showed a strong to moderate correlation between lesion percentage scores obtained by radiologists and the computer software (r ranged from 0.7679 to 0.8373, P< 0.05), and a moderate correlation between the proportion of GGO and mean lesion density (r = -0.5894, P < 0.05), and proportion of consolidation and mean lesion density (r = 0.6282, P < 0.05). Computer-aided quantification showed a statistical significant higher lesion percentage for lower lobes than that assessed by the radiologists (χ2 =8.160, P = 0.004). Conclusions: Our experiments demonstrated that the computer tool could reliably and accurately assess the severity and distribution of pneumonia on CT scans.},
  number       = {2},
  journal      = {Journal of Pharmaceutical Analysis},
  author       = {Shen, Cong and Yu, Nan and Cai, Shubo and Zhou, Jie and Sheng, Jiexin and Liu, Kang and Zhou, Heping and Guo, Youmin and Niu, Gang},
  year         = {2020},
  month        = {Apr},
  pages        = {123–129}
}

 @article{LBC+2020,
  title        = {Robust semi-automatic segmentation of pulmonary subsolid nodules in chest computed tomography scans},
  volume       = {60},
  doi          = {10.1088/0031-9155/60/3/1307},
  abstractnote = {The malignancy of lung nodules is most often detected by analyzing changes of the nodule diameter in follow-up scans. A recent study showed that comparing the volume or the mass of a nodule over time is much more significant than comparing the diameter. Since the survival rate is higher when the disease is still in an early stage it is important to detect the growth rate as soon as possible. However manual segmentation of a volume is time-consuming. Whereas there are several well evaluated methods for the segmentation of solid nodules, less work is done on subsolid nodules which actually show a higher malignancy rate than solid nodules. In this work we present a fast, semi-automatic method for segmentation of subsolid nodules.As minimal user interaction the method expects a user-drawn stroke on the largest diameter of the nodule. First, a threshold-based region growing is performed based on intensity analysis of the nodule region and surrounding parenchyma. In the next step the chest wall is removed by a combination of a connected component analyses and convex hull calculation. Finally, attached vessels are detached by morphological operations.The method was evaluated on all nodules of the publicly available LIDC/IDRI database that were manually segmented and rated as non-solid or part-solid by four radiologists (Dataset 1) and three radiologists (Dataset 2). For these 59 nodules the Jaccard index for the agreement of the proposed method with the manual reference segmentations was 0.52/0.50 (Dataset 1/Dataset 2) compared to an inter-observer agreement of the manual segmentations of 0.54/0.58 (Dataset 1/Dataset 2). Furthermore, the inter-observer agreement using the proposed method (i.e. different input strokes) was analyzed and gave a Jaccard index of 0.74/0.74 (Dataset 1/Dataset 2).The presented method provides satisfactory segmentation results with minimal observer effort in minimal time and can reduce the inter-observer variability for segmentation of subsolid nodules in clinical routine.},
  number       = {3},
  journal      = {Physics in Medicine and Biology},
  publisher    = {IOP Publishing},
  author       = {Lassen, B. C. and Jacobs, C. and Kuhnigk, J.-M. and Ginneken, B. van and Rikxoort, E. M. van},
  year         = {2015},
  month        = {Jan},
  pages        = {1307–1323}
}

 @article{RFT2015,
  title        = {U-Net: Convolutional Networks for Biomedical Image Segmentation},
  url          = {http://arxiv.org/abs/1505.04597},
  abstractnote = {There is large consent that successful training of deep networks requires many thousand annotated training samples. In this paper, we present a network and training strategy that relies on the strong use of data augmentation to use the available annotated samples more efficiently. The architecture consists of a contracting path to capture context and a symmetric expanding path that enables precise localization. We show that such a network can be trained end-to-end from very few images and outperforms the prior best method (a sliding-window convolutional network) on the ISBI challenge for segmentation of neuronal structures in electron microscopic stacks. Using the same network trained on transmitted light microscopy images (phase contrast and DIC) we won the ISBI cell tracking challenge 2015 in these categories by a large margin. Moreover, the network is fast. Segmentation of a 512x512 image takes less than a second on a recent GPU. The full implementation (based on Caffe) and the trained networks are available at http://lmb.informatik.uni-freiburg.de/people/ronneber/u-net .},
  note         = {arXiv: 1505.04597},
  journal      = {arXiv:1505.04597 [cs]},
  author       = {Ronneberger, Olaf and Fischer, Philipp and Brox, Thomas},
  year         = {2015},
  month        = {May}
}

 @article{SCS+2020,
  title        = {Automated Segmentation of Colorectal Tumor in 3D MRI Using 3D Multiscale Densely Connected Convolutional Neural Network},
  volume       = {2019},
  issn         = {2040-2295},
  url          = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6374810/},
  doi          = {10.1155/2019/1075434},
  abstractnote = {The main goal of this work is to automatically segment colorectal tumors in 3D T2-weighted (T2w) MRI with reasonable accuracy. For such a purpose, a novel deep learning-based algorithm suited for volumetric colorectal tumor segmentation is proposed. The proposed CNN architecture, based on densely connected neural network, contains multiscale dense interconnectivity between layers of fine and coarse scales, thus leveraging multiscale contextual information in the network to get better flow of information throughout the network. Additionally, the 3D level-set algorithm was incorporated as a postprocessing task to refine contours of the network predicted segmentation. The method was assessed on T2-weighted 3D MRI of 43 patients diagnosed with locally advanced colorectal tumor (cT3/T4). Cross validation was performed in 100 rounds by partitioning the dataset into 30 volumes for training and 13 for testing. Three performance metrics were computed to assess the similarity between predicted segmentation and the ground truth (i.e., manual segmentation by an expert radiologist/oncologist), including Dice similarity coefficient (DSC), recall rate (RR), and average surface distance (ASD). The above performance metrics were computed in terms of mean and standard deviation (mean ± standard deviation). The DSC, RR, and ASD were 0.8406 ± 0.0191, 0.8513 ± 0.0201, and 2.6407 ± 2.7975 before postprocessing, and these performance metrics became 0.8585 ± 0.0184, 0.8719 ± 0.0195, and 2.5401 ± 2.402 after postprocessing, respectively. We compared our proposed method to other existing volumetric medical image segmentation baseline methods (particularly 3D U-net and DenseVoxNet) in our segmentation tasks. The experimental results reveal that the proposed method has achieved better performance in colorectal tumor segmentation in volumetric MRI than the other baseline techniques.},
  journal      = {Journal of Healthcare Engineering},
  author       = {Soomro, Mumtaz Hussain and Coppotelli, Matteo and Conforto, Silvia and Schmid, Maurizio and Giunta, Gaetano and Del Secco, Lorenzo and Neri, Emanuele and Caruso, Damiano and Rengo, Marco and Laghi, Andrea},
  year         = {2019},
  month        = {Jan}
}

 @article{MNA+2020,
  title        = {V-Net: Fully Convolutional Neural Networks for Volumetric Medical Image Segmentation},
  url          = {http://arxiv.org/abs/1606.04797},
  abstractnote = {Convolutional Neural Networks (CNNs) have been recently employed to solve problems from both the computer vision and medical image analysis fields. Despite their popularity, most approaches are only able to process 2D images while most medical data used in clinical practice consists of 3D volumes. In this work we propose an approach to 3D image segmentation based on a volumetric, fully convolutional, neural network. Our CNN is trained end-to-end on MRI volumes depicting prostate, and learns to predict segmentation for the whole volume at once. We introduce a novel objective function, that we optimise during training, based on Dice coefficient. In this way we can deal with situations where there is a strong imbalance between the number of foreground and background voxels. To cope with the limited number of annotated volumes available for training, we augment the data applying random non-linear transformations and histogram matching. We show in our experimental evaluation that our approach achieves good performances on challenging test data while requiring only a fraction of the processing time needed by other previous methods.},
  note         = {arXiv: 1606.04797},
  journal      = {arXiv:1606.04797 [cs]},
  author       = {Milletari, Fausto and Navab, Nassir and Ahmadi, Seyed-Ahmad},
  year         = {2016},
  month        = {Jun}
}

 @inproceedings{ZSM+2020,
  place        = {Cham},
  title        = {UNet++: A Nested U-Net Architecture for Medical Image Segmentation},
  isbn         = {978-3-030-00889-5},
  abstractnote = {In this paper, we present UNet++, a new, more powerful architecture for medical image segmentation. Our architecture is essentially a deeply-supervised encoder-decoder network where the encoder and decoder sub-networks are connected through a series of nested, dense skip pathways. The re-designed skip pathways aim at reducing the semantic gap between the feature maps of the encoder and decoder sub-networks. We argue that the optimizer would deal with an easier learning task when the feature maps from the decoder and encoder networks are semantically similar. We have evaluated UNet++ in comparison with U-Net and wide U-Net architectures across multiple medical image segmentation tasks: nodule segmentation in the low-dose CT scans of chest, nuclei segmentation in the microscopy images, liver segmentation in abdominal CT scans, and polyp segmentation in colonoscopy videos. Our experiments demonstrate that UNet++ with deep supervision achieves an average IoU gain of 3.9 and 3.4 points over U-Net and wide U-Net, respectively.},
  booktitle    = {Deep Learning in Medical Image Analysis and Multimodal Learning for Clinical Decision Support},
  publisher    = {Springer International Publishing},
  author       = {Zhou, Zongwei and Rahman Siddiquee, Md Mahfuzur and Tajbakhsh, Nima and Liang, Jianming},
  editor       = {Stoyanov, Danail and Taylor, Zeike and Carneiro, Gustavo and Syeda-Mahmood, Tanveer and Martel, Anne and Maier-Hein, Lena and Tavares, João Manuel R.S. and Bradley, Andrew and Papa, João Paulo and Belagiannis, Vasileios and et al.},
  year         = {2018},
  pages        = {3–11}
}

 @article{OSF+2020,
  title        = {Attention U-Net: Learning Where to Look for the Pancreas},
  url          = {http://arxiv.org/abs/1804.03999},
  abstractnote = {We propose a novel attention gate (AG) model for medical imaging that automatically learns to focus on target structures of varying shapes and sizes. Models trained with AGs implicitly learn to suppress irrelevant regions in an input image while highlighting salient features useful for a specific task. This enables us to eliminate the necessity of using explicit external tissue/organ localisation modules of cascaded convolutional neural networks (CNNs). AGs can be easily integrated into standard CNN architectures such as the U-Net model with minimal computational overhead while increasing the model sensitivity and prediction accuracy. The proposed Attention U-Net architecture is evaluated on two large CT abdominal datasets for multi-class image segmentation. Experimental results show that AGs consistently improve the prediction performance of U-Net across different datasets and training sizes while preserving computational efficiency. The code for the proposed architecture is publicly available.},
  note         = {arXiv: 1804.03999},
  journal      = {arXiv:1804.03999 [cs]},
  author       = {Oktay, Ozan and Schlemper, Jo and Folgoc, Loic Le and Lee, Matthew and Heinrich, Mattias and Misawa, Kazunari and Mori, Kensaku and McDonagh, Steven and Hammerla, Nils Y. and Kainz, Bernhard and et al.},
  year         = {2018},
  month        = {May}
}

 @article{GHT2020,
  title        = {Estimating Uncertainty and Interpretability in Deep Learning for Coronavirus (COVID-19) Detection},
  url          = {http://arxiv.org/abs/2003.10769},
  abstractnote = {Deep Learning has achieved state of the art performance in medical imaging. However, these methods for disease detection focus exclusively on improving the accuracy of classification or predictions without quantifying uncertainty in a decision. Knowing how much confidence there is in a computer-based medical diagnosis is essential for gaining clinicians trust in the technology and therefore improve treatment. Today, the 2019 Coronavirus (SARS-CoV-2) infections are a major healthcare challenge around the world. Detecting COVID-19 in X-ray images is crucial for diagnosis, assessment and treatment. However, diagnostic uncertainty in the report is a challenging and yet inevitable task for radiologist. In this paper, we investigate how drop-weights based Bayesian Convolutional Neural Networks (BCNN) can estimate uncertainty in Deep Learning solution to improve the diagnostic performance of the human-machine team using publicly available COVID-19 chest X-ray dataset and show that the uncertainty in prediction is highly correlates with accuracy of prediction. We believe that the availability of uncertainty-aware deep learning solution will enable a wider adoption of Artificial Intelligence (AI) in a clinical setting.},
  note         = {arXiv: 2003.10769},
  journal      = {arXiv:2003.10769 [cs, eess, stat]},
  author       = {Ghoshal, Biraja and Tucker, Allan},
  year         = {2020},
  month        = {Mar}
}

@article{AKP2020,
  author = {Narin, Ali and Kaya, Ceren and Pamuk, Ziynet},
  year   = {2020},
  month  = {03},
  pages  = {},
  title  = {Automatic Detection of Coronavirus Disease (COVID-19) Using X-ray Images and Deep Convolutional Neural Networks}
}

@article{ZXS+2020,
  author = {Zhang, Jianpeng and Xie, Yutong and Li, Yi and Shen, Chunhua and Xia, Yong},
  year   = {2020},
  month  = {03},
  pages  = {},
  title  = {COVID-19 Screening on Chest X-ray Images Using Deep Learning based Anomaly Detection}
}

 @article{LWA2020,
  title        = {COVID-Net: A Tailored Deep Convolutional Neural Network Design for Detection of COVID-19 Cases from Chest X-Ray Images},
  url          = {http://arxiv.org/abs/2003.09871},
  abstractnote = {The COVID-19 pandemic continues to have a devastating effect on the health and well-being of the global population. A critical step in the fight against COVID-19 is effective screening of infected patients, with one of the key screening approaches being radiology examination using chest radiography. Motivated by this and inspired by the open source efforts of the research community, in this study we introduce COVID-Net, a deep convolutional neural network design tailored for the detection of COVID-19 cases from chest X-ray (CXR) images that is open source and available to the general public. To the best of the authors’ knowledge, COVID-Net is one of the first open source network designs for COVID-19 detection from CXR images at the time of initial release. We also introduce COVIDx, an open access benchmark dataset that we generated comprising of 13,975 CXR images across 13,870 patient patient cases, with the largest number of publicly available COVID-19 positive cases to the best of the authors’ knowledge. Furthermore, we investigate how COVID-Net makes predictions using an explainability method in an attempt to not only gain deeper insights into critical factors associated with COVID cases, which can aid clinicians in improved screening, but also audit COVID-Net in a responsible and transparent manner to validate that it is making decisions based on relevant information from the CXR images. By no means a production-ready solution, the hope is that the open access COVID-Net, along with the description on constructing the open source COVIDx dataset, will be leveraged and build upon by both researchers and citizen data scientists alike to accelerate the development of highly accurate yet practical deep learning solutions for detecting COVID-19 cases and accelerate treatment of those who need it the most.},
  note         = {arXiv: 2003.09871},
  journal      = {arXiv:2003.09871 [cs, eess]},
  author       = {Wang, Linda and Wong, Alexander},
  year         = {2020},
  month        = {May}
}

@article{JCW+2020,
  title     = {Development and Evaluation of an AI System for COVID-19 Diagnosis},
  author    = {Jin, Cheng and Chen, Weixiang and Cao, Yukun and Xu, Zhanwei and Zhang, Xin and Deng, Lei and Zheng, Chuansheng and Zhou, Jie and Shi, Heshui and Feng, Jianjiang},
  journal   = {medRxiv},
  year      = {2020},
  publisher = {Cold Spring Harbor Laboratory Press}
}

@article{SZL+2020,
  title     = {Deep learning enables accurate diagnosis of novel coronavirus (COVID-19) with CT images},
  author    = {Song, Ying and Zheng, Shuangjia and Li, Liang and Zhang, Xiang and Zhang, Xiaodong and Huang, Ziwang and Chen, Jianwen and Zhao, Huiying and Jie, Yusheng and Wang, Ruixuan and others},
  journal   = {medRxiv},
  year      = {2020},
  publisher = {Cold Spring Harbor Laboratory Press}
}

@article{WBX+2020,
  title     = {A deep learning algorithm using CT images to screen for Corona Virus Disease (COVID-19)},
  author    = {Wang, Shuai and Kang, Bo and Ma, Jinlu and Zeng, Xianjun and Xiao, Mingming and Guo, Jia and Cai, Mengjiao and Yang, Jingyi and Li, Yaodong and Meng, Xiangfei and others},
  journal   = {MedRxiv},
  year      = {2020},
  publisher = {Cold Spring Harbor Laboratory Press}
}

@article{SXF+2020,
  title   = {Large-scale screening of covid-19 from community acquired pneumonia using infection size-aware classification},
  author  = {Shi, Feng and Xia, Liming and Shan, Fei and Wu, Dijia and Wei, Ying and Yuan, Huan and Jiang, Huiting and Gao, Yaozong and Sui, He and Shen, Dinggang},
  journal = {arXiv preprint arXiv:2003.09860},
  year    = {2020}
}

@article{WLA+2020,
  title     = {Frequency and distribution of chest radiographic findings in COVID-19 positive patients},
  author    = {Wong, Ho Yuen Frank and Lam, Hiu Yin Sonia and Fong, Ambrose Ho-Tung and Leung, Siu Ting and Chin, Thomas Wing-Yan and Lo, Christine Shing Yen and Lui, Macy Mei-Sze and Lee, Jonan Chun Yin and Chiu, Keith Wan-Hang and Chung, Tom and others},
  journal   = {Radiology},
  pages     = {201160},
  year      = {2020},
  publisher = {Radiological Society of North America}
}

 @article{JMD2020,
  title        = {COVID-19 Image Data Collection},
  url          = {http://arxiv.org/abs/2003.11597},
  abstractnote = {This paper describes the initial COVID-19 open image data collection. It was created by assembling medical images from websites and publications and currently contains 123 frontal view X-rays.},
  note         = {arXiv: 2003.11597},
  journal      = {arXiv:2003.11597 [cs, eess, q-bio]},
  author       = {Cohen, Joseph Paul and Morrison, Paul and Dao, Lan},
  year         = {2020},
  month        = {Mar}
}

 @article{MOB+2020,
  title        = {Automatic Hierarchical Classification of Kelps Using Deep Residual Features},
  volume       = {20},
  issn         = {1424-8220},
  doi          = {10.3390/s20020447},
  abstractnote = {Across the globe, remote image data is rapidly being collected for the assessment of benthic communities from shallow to extremely deep waters on continental slopes to the abyssal seas. Exploiting this data is presently limited by the time it takes for experts to identify organisms found in these images. With this limitation in mind, a large effort has been made globally to introduce automation and machine learning algorithms to accelerate both classiﬁcation and assessment of marine benthic biota. One major issue lies with organisms that move with swell and currents, such as kelps. This paper presents an automatic hierarchical classiﬁcation method local binary classiﬁcation as opposed to the conventional ﬂat classiﬁcation to classify kelps in images collected by autonomous underwater vehicles. The proposed kelp classiﬁcation approach exploits learned feature representations extracted from deep residual networks. We show that these generic features outperform the traditional off-the-shelf CNN features and the conventional hand-crafted features. Experiments also demonstrate that the hierarchical classiﬁcation method outperforms the traditional parallel multi-class classiﬁcations by a signiﬁcant margin (90.0% vs. 57.6% and 77.2% vs. 59.0%) on Benthoz15 and Rottnest datasets respectively. Furthermore, we compare different hierarchical classiﬁcation approaches and experimentally show that the sibling hierarchical training approach outperforms the inclusive hierarchical approach by a signiﬁcant margin. We also report an application of our proposed method to study the change in kelp cover over time for annually repeated AUV surveys.},
  number       = {2},
  journal      = {Sensors},
  author       = {Mahmood, Ammar and Ospina, Ana Giraldo and Bennamoun, Mohammed and An, Senjian and Sohel, Ferdous and Boussaid, Farid and Hovey, Renae and Fisher, Robert B. and Kendrick, Gary A.},
  year         = {2020},
  month        = {Jan},
  pages        = {447}
}

@inproceedings{AGM+2018,
  title     = {Sanity checks for saliency maps},
  author    = {Adebayo, Julius and Gilmer, Justin and Muelly, Michael and Goodfellow, Ian and Hardt, Moritz and Kim, Been},
  booktitle = {Advances in Neural Information Processing Systems},
  pages     = {9505--9515},
  year      = {2018}
}

 @article{ZMF2013,
  title        = {Visualizing and Understanding Convolutional Networks},
  url          = {http://arxiv.org/abs/1311.2901},
  abstractnote = {Large Convolutional Network models have recently demonstrated impressive classification performance on the ImageNet benchmark. However there is no clear understanding of why they perform so well, or how they might be improved. In this paper we address both issues. We introduce a novel visualization technique that gives insight into the function of intermediate feature layers and the operation of the classifier. We also perform an ablation study to discover the performance contribution from different model layers. This enables us to find model architectures that outperform Krizhevsky etal on the ImageNet classification benchmark. We show our ImageNet model generalizes well to other datasets: when the softmax classifier is retrained, it convincingly beats the current state-of-the-art results on Caltech-101 and Caltech-256 datasets.},
  note         = {arXiv: 1311.2901},
  journal      = {arXiv:1311.2901 [cs]},
  author       = {Zeiler, Matthew D. and Fergus, Rob},
  year         = {2013},
  month        = {Nov}
}

 @article{ZKL+2015,
  title        = {Learning Deep Features for Discriminative Localization},
  url          = {http://arxiv.org/abs/1512.04150},
  abstractnote = {In this work, we revisit the global average pooling layer proposed in [13], and shed light on how it explicitly enables the convolutional neural network to have remarkable localization ability despite being trained on image-level labels. While this technique was previously proposed as a means for regularizing training, we find that it actually builds a generic localizable deep representation that can be applied to a variety of tasks. Despite the apparent simplicity of global average pooling, we are able to achieve 37.1% top-5 error for object localization on ILSVRC 2014, which is remarkably close to the 34.2% top-5 error achieved by a fully supervised CNN approach. We demonstrate that our network is able to localize the discriminative image regions on a variety of tasks despite not being trained for them},
  note         = {arXiv: 1512.04150},
  journal      = {arXiv:1512.04150 [cs]},
  author       = {Zhou, Bolei and Khosla, Aditya and Lapedriza, Agata and Oliva, Aude and Torralba, Antonio},
  year         = {2015},
  month        = {Dec}
}

@article{RCD+2017,
  title     = {Grad-cam: Visual explanations from deep networks via gradient-based localization},
  author    = {Selvaraju, Ramprasaath R and Cogswell, Michael and Das, Abhishek and Vedantam, Ramakrishna and Parikh, Devi and Batra, Dhruv},
  booktitle = {Proceedings of the IEEE international conference on computer vision},
  pages     = {618--626},
  year      = {2017}
}

 @article{HSX+2020,
  title   = {Artificial intelligence for the detection of COVID-19 pneumonia on chest CT using multinational datasets},
  volume  = {11},
  issn    = {2041-1723},
  doi     = {10.1038/s41467-020-17971-2},
  number  = {1},
  journal = {Nature Communications},
  author  = {Harmon, Stephanie A. and Sanford, Thomas H. and Xu, Sheng and Turkbey, Evrim B. and Roth, Holger and Xu, Ziyue and Yang, Dong and Myronenko, Andriy and Anderson, Victoria and Amalou, Amel and et al.},
  year    = {2020},
  month   = {Dec},
  pages   = {4080}
}

 @article{OTY+2020,
  title        = {Automated detection of COVID-19 cases using deep neural networks with X-ray images},
  volume       = {121},
  issn         = {00104825},
  doi          = {10.1016/j.compbiomed.2020.103792},
  abstractnote = {The novel coronavirus 2019 (COVID-2019), which first appeared in Wuhan city of China in December 2019, spread rapidly around the world and became a pandemic. It has caused a devastating effect on both daily lives, public health, and the global economy. It is critical to detect the positive cases as early as possible so as to prevent the further spread of this epidemic and to quickly treat affected patients. The need for auxiliary diagnostic tools has increased as there are no accurate automated toolkits available. Recent findings obtained using radiology imaging techniques suggest that such images contain salient information about the COVID-19 virus. Application of advanced artificial intelligence (AI) techniques coupled with radiological imaging can be helpful for the ac­ curate detection of this disease, and can also be assistive to overcome the problem of a lack of specialized physicians in remote villages. In this study, a new model for automatic COVID-19 detection using raw chest X-ray images is presented. The proposed model is developed to provide accurate diagnostics for binary classification (COVID vs. No-Findings) and multi-class classification (COVID vs. No-Findings vs. Pneumonia). Our model produced a classification accuracy of 98.08% for binary classes and 87.02% for multi-class cases. The DarkNet model was used in our study as a classifier for the you only look once (YOLO) real time object detection system. We implemented 17 convolutional layers and introduced different filtering on each layer. Our model (available at (https://github.com/muhammedtalo/COVID-19)) can be employed to assist radiologists in validating their initial screening, and can also be employed via cloud to immediately screen patients.},
  journal      = {Computers in Biology and Medicine},
  author       = {Ozturk, Tulin and Talo, Muhammed and Yildirim, Eylul Azra and Baloglu, Ulas Baran and Yildirim, Ozal and Rajendra Acharya, U.},
  year         = {2020},
  month        = {Jun},
  pages        = {103792}
}

@article{ADD+2020,
  title     = {Explainable Artificial Intelligence (XAI): Concepts, taxonomies, opportunities and challenges toward responsible AI},
  author    = {Arrieta, Alejandro Barredo and D{\'\i}az-Rodr{\'\i}guez, Natalia and Del Ser, Javier and Bennetot, Adrien and Tabik, Siham and Barbado, Alberto and Garc{\'\i}a, Salvador and Gil-L{\'o}pez, Sergio and Molina, Daniel and Benjamins, Richard and others},
  journal   = {Information Fusion},
  volume    = {58},
  pages     = {82--115},
  year      = {2020},
  publisher = {Elsevier}
}

@article{FSA+2020,
  title     = {Explainable artificial intelligence for neuroscience: Behavioral neurostimulation},
  author    = {Fellous, Jean-Marc and Sapiro, Guillermo and Rossi, Andrew and Mayberg, Helen S and Ferrante, Michele},
  journal   = {Frontiers in Neuroscience},
  volume    = {13},
  pages     = {1346},
  year      = {2019},
  publisher = {Frontiers}
}

@article{NVF+2018,
  title     = {Boosting self-supervised learning via knowledge transfer},
  author    = {Noroozi, Mehdi and Vinjimoor, Ananth and Favaro, Paolo and Pirsiavash, Hamed},
  booktitle = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
  pages     = {9359--9367},
  year      = {2018}
}

@article{DGE2015,
  title     = {Unsupervised visual representation learning by context prediction},
  author    = {Doersch, Carl and Gupta, Abhinav and Efros, Alexei A},
  booktitle = {Proceedings of the IEEE international conference on computer vision},
  pages     = {1422--1430},
  year      = {2015}
}

@article{TFK+2018,
  title        = {A survey on deep transfer learning},
  author       = {Tan, Chuanqi and Sun, Fuchun and Kong, Tao and Zhang, Wenchang and Yang, Chao and Liu, Chunfang},
  booktitle    = {International conference on artificial neural networks},
  pages        = {270--279},
  year         = {2018},
  organization = {Springer}
}

@online{SCR,
  title   = {What is Scrum?},
  url     = {https://www.scrum.org/resources/what-is-scrum},
  journal = {Scrum.org},
  author  = {SCRUM},
  year = {2018}
}

@misc{SFP,
  title   = {The Scrum Framework Poster},
  url     = {https://www.scrum.org/resources/scrum-framework-poster},
  journal = {Scrum.org}
}

@online{GAN2020, title={COVID-19 image data collection}, url={https://github.com/ieee8023/covid-chestxray-dataset}, journal={GitHub}, author={Ganglia, B}, year=2020}

@online{MOO2018, title={Chest X-Ray Images (Pneumonia)}, url={https://www.kaggle.com/paultimothymooney/chest-xray-pneumonia}, journal={Kaggle}, author={Mooney, Paul}, year={2018}, month={Mar}}

@online{AGC2020, title={Open Source COVID-19 Dataset}, url={https://github.com/agchung}, journal={GitHub}, publisher={Audrey G Chung}, author={Chung, Audrey G}, year=2020}

@article{MUR2020,
	title={COVID-19 on chest radiographs: a multireader evaluation of an artificial intelligence system},
	author={Murphy, Keelin and Smits, Henk and Knoops, Arnoud JG and Korst, Michael BJM and Samson, Tijs and Scholten, Ernst T and Schalekamp, Steven and Schaefer-Prokop, Cornelia M and Philipsen, Rick HHM and Meijers, Annet and others},
	journal={Radiology},
	volume={296},
	number={3},
	pages={E166--E172},
	year={2020},
	publisher={Radiological Society of North America}
}

@online{KER, title={Keras documentation: Image Data Pre-processing}, url={https://keras.io/api/preprocessing/image/}, journal={Keras}, author={Keras}, year=2021}

@online{KCB,  title={Keras documentation: Callbacks API}, url={https://keras.io/api/callbacks/}, journal={Keras}, author={Keras}, year={2021} }

@online{KES, title={Keras documentation: EarlyStopping}, url={https://keras.io/api/callbacks/early_stopping/}, journal={Keras}, author={Keras}, year={2021} }

@online{KLR, title={Keras documentation: ReduceLROnPlateau}, url={https://keras.io/api/callbacks/reduce_lr_on_plateau/}, journal={Keras}, author={Keras}, year={2021} }

@online{KMC, title={Keras documentation: ModelCheckpoint}, url={https://keras.io/api/callbacks/model_checkpoint/}, journal={Keras}, author={Keras}, year={2021} }

@online{IMG, title={ImageNet},author={ImageNet}, url={http://www.image-net.org/}, journal={Image-net.org}, year={2021} }

@online{VGG, url={https://neurohive.io/en/popular-networks/vgg16/}, journal={Neurohive.io}, author={Neurohive}, year={2018}, month={Nov} }

@online{DEN, title={Figure 2.6: A schematic illustration of the DenseNet-121 architecture [82].}, url={https://www.researchgate.net/figure/A-schematic-illustration-of-the-DenseNet-121-architecture-82_fig5_334170752}, journal={ResearchGate}, publisher={ResearchGate}, author={Noha Radwan}, year={2019}, month={Jun} }

@book{HLW+2016, title={Densely Connected Convolutional Networks}, url={https://arxiv.org/pdf/1608.06993v3.pdf}, institution={}, author={Huang, Gao and Liu, Zhuang and Weinberger, Kilian and Van Der Maaten, Laurens}, year={2016} }

@article{BAK2016, title={Microsoft Presents : Deep Residual Networks - Baki Er - Medium}, url={https://medium.com/@bakiiii/microsoft-presents-deep-residual-networks-d0ebd3fe5887}, journal={Medium}, publisher={Medium}, author={Baki Er}, year={2016}, month={Aug} }

@article{FEN2017, title={An Overview of ResNet and its Variants - Towards Data Science}, url={https://towardsdatascience.com/an-overview-of-resnet-and-its-variants-5281e2f56035}, journal={Medium}, publisher={Towards Data Science}, author={Feng, Vincent}, year={2017}, month={Jul} }

@book{SKZ2015, title={Published as a conference paper at ICLR 2015 VERY DEEP CONVOLUTIONAL NETWORKS FOR LARGE-SCALE IMAGE RECOGNITION}, url={https://arxiv.org/pdf/1409.1556.pdf}, institution={}, author={Simonyan, Karen and Zisserman, Andrew}, year={2015} }

@online{WJY2019, title={VGG Neural Networks: The Next Step After AlexNet - Towards Data Science}, url={https://towardsdatascience.com/vgg-neural-networks-the-next-step-after-alexnet-3f91fa9ffe2c}, journal={Medium}, publisher={Towards Data Science}, author={Wei, Jerry}, year={2019}, month={Jul} }

@online{KAP, title={Keras documentation: Keras Applications}, url={https://keras.io/api/applications/}, journal={Keras.io}, author={Keras}, year={2021} }

@online{TBT2019, title={An Ultimate Guide To Transfer Learning In NLP}, url={https://www.topbots.com/transfer-learning-in-nlp/}, journal={TOPBOTS}, author={Pratik Bhavsar}, year={2019}, month={Dec} }

@online{POL2019, title={Ensemble learning}, volume={4}, url={http://www.scholarpedia.org/article/Ensemble_learning#:~:text=Ensemble\%20learning\%20is\%20the\%20process,\%2C\%20function\%20approximation\%2C\%20etc.)}, DOI={10.4249/scholarpedia.2776}, number={1}, journal={Scholarpedia}, author={Polikar, Robi}, year={2009}, pages={2776} }

‌@online{LUT2017, title={Ensemble Methods in Machine Learning: What are They and Why Use Them?}, url={https://towardsdatascience.com/ensemble-methods-in-machine-learning-what-are-they-and-why-use-them-68ec3f9fef5f}, journal={Medium}, publisher={Towards Data Science}, author={Lutins, Evan}, year={2017}, month={Aug} }

‌@online{CON, title={Keras documentation: Concatenate layer}, url={https://keras.io/api/layers/merging_layers/concatenate/}, journal={Keras.io}, author={Keras}, year={2021} }

@online{SCR, url={https://scikit-learn.org/stable/modules/generated/sklearn.metrics.classification_report.html}, journal={Scikit-learn.org}, year={2020} }

@online{SCM, url={https://scikit-learn.org/stable/modules/generated/sklearn.metrics.confusion_matrix.html}, journal={Scikit-learn.org}, author={Scikit-learn}, year={2020} }


@misc{KCV, title={Keras documentation: Computer Vision}, url={https://keras.io/examples/vision/}, journal={Keras.io}, author={Keras}, year={2021} }

‌
@online{KGM, title={Keras documentation: Grad-CAM class activation visualization}, url={https://keras.io/examples/vision/grad_cam/}, journal={Keras.io}, author={Keras}, year={2020} }

‌@online{COR2020, url={https://coronacases.org/}, journal={Coronacases.org}, year={2020} }

‌@online{RAD2021, url={https://radiopaedia.org/?lang=us}, journal={Radiopaedia.org}, year={2021} }

‌@article{JCY+2020, title={COVID-19 CT Lung and Infection Segmentation Dataset}, url={https://zenodo.org/record/3757476}, DOI={10.5281/zenodo.3757476}, journal={Zenodo}, author={Jun, Ma and Ge Cheng and Wang Yixin and Xingle, An and Gao Jiantao and Ziqi, Yu and Zhang Minqing and Liu Xin and Deng Xueyuan and Cao Shucheng and et al.}, year={2020}, month={Apr} }

@article{ICT2019,
  title={iCTCF: an integrative resource of chest computed tomography images and clinical features of patients with COVID-19 pneumonia},
  author={Ning, Wanshan and Lei, Shijun and Yang, Jingjing and Cao, Yukun and Jiang, Peiran and Yang, Qianqian and Zhang, Jiao and Wang, Xiaobei and Chen, Fenghua and Geng, Zhi and others},
  year={2020}
}


@misc{UNT, title={UNet — Line by Line Explanation - Towards Data Science}, url={https://towardsdatascience.com/unet-line-by-line-explanation-9b191c76baf5}, journal={Medium}, publisher={Towards Data Science}, author={Zhang, Jeremy}, year={2019}, month={Oct} }

‌
@online{UNT+, title={a nested u-net architecture for medical image segmentation}, url={https://paperswithcode.com/paper/unet-a-nested-u-net-architecture-for-medical},
author = {Zongwei, Zhou and Mahfuzur, Rahman and Nima, Tajbakhsh and Jianming, Liang},
journal={Paperswithcode.com}, year={2018} }

‌@online{AUN, title={Biomedical Image Segmentation: Attention U-Net - Towards Data Science}, url={https://towardsdatascience.com/biomedical-image-segmentation-attention-u-net-29b6f0827405}, journal={Medium}, publisher={Towards Data Science}, author={Jingles (Hong Jing}, year={2019}, month={Dec} }

@article{HSA2018, title={UNet - Towards Data Science}, url={https://towardsdatascience.com/u-net-b229b32b4a71}, journal={Medium}, publisher={Towards Data Science}, author={Heet Sankesara}, year={2019}, month={Jan} }

@article{JHJ2019, title={Biomedical Image Segmentation: UNet++ - Towards Data Science}, url={https://towardsdatascience.com/biomedical-image-segmentation-unet-991d075a3a4b}, journal={Medium}, publisher={Towards Data Science}, author={Jingles (Hong Jing}, year={2019}, month={Dec} }

‌‌@online{VIN2020, title={A detailed explanation of the Attention U-Net - Towards Data Science}, url={https://towardsdatascience.com/a-detailed-explanation-of-the-attention-u-net-b371a5590831}, journal={Medium}, publisher={Towards Data Science}, author={Vinod, Robin}, year={2020}, month={May} }

‌@online{KUC, url={https://pypi.org/project/keras-unet-collection/}, journal={PyPI}, year={2021}, month={Mar}, author={PyPi} }

‌@online{agl2020, title={The best all-in-one responsive and scalable code structure for Flutter}, url={https://medium.com/@alisterluiz/the-best-all-in-one-responsive-and-scalable-code-structure-for-flutter-717a384f9eda}, journal={Medium}, publisher={Medium}, author={Alister Luiz}, year={2020}, month={Jul} }

‌@online{NUM, url={https://numpy.org/}, journal={Numpy.org}, year={2019}, author={Numpy}}

‌@online{IDS2020, url={https://www.idsociety.org/covid-19-real-time-learning-network/diagnostics/RT-pcr-testing/#:~:text=The\%20sensitivity\%20and\%20specificity\%20of,CI\%2095.3\%25\%E2\%80\%9399.7\%25).}, journal={Idsociety.org}, author={IDSociety}, year={2021}, month={Feb} }

‌@online{SUS2021, title={System Usability Scale (SUS) | Usability.gov}, url={https://www.usability.gov/how-to-and-tools/methods/system-usability-scale.html}, journal={Usability.gov}, author={Assistant Secretary for Public Affairs}, year={2021} }

‌@article{JBR1986,
  title={SUS-A quick and dirty usability scale. Digital Equipment Co Ltd},
  author={Brooke, John},
  journal={Reading, United Kingdom},
  year={1986}
}

@online{THO2015, title={How To Use The System Usability Scale (SUS) To Evaluate The Usability Of Your Website - Usability Geek}, url={https://usabilitygeek.com/how-to-use-the-system-usability-scale-sus-to-evaluate-the-usability-of-your-website/}, journal={Usability Geek}, author={Thomas, Nathan}, year={2015}, month={Jul} }

‌@misc{RGC2016, title={How can I create a single confusion matrix after K fold cross validation ?}, url={https://www.researchgate.net/post/How_can_I_create_a_single_confusion_matrix_after_K_fold_cross_validation}, journal={ResearchGate}, publisher={ResearchGate}, author={Massachusetts Institute of Technology Press (MIT Press}, year={2016}, month={Jun} }

@online{STA21, title={COVID-19 tests by country}, url={https://www.statista.com/statistics/1028731/covid19-tests-select-countries-worldwide/}, journal={Statista}, author={Elflein, John}, year={2021}, month={Mar}}
‌
@article{JFB2020,
  title={COVID-19 Artificial Intelligence Diagnosis using only Cough Recordings},
  author={Laguarta, Jordi and Hueto, Ferran and Subirana, Brian},
  journal={IEEE Open Journal of Engineering in Medicine and Biology},
  volume={1},
  pages={275--281},
  year={2020},
  publisher={IEEE}
}